{
  "https://www.pymc-labs.com/blog-posts/2022-10-26-AlvaLabs": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nIntroduction\nTimestamps\nBayesian Item Response Modeling in PyMC\n\nOctober 26, 2022\n\nBy Thomas Wiecki\n\nIntroduction#\n\nIn this panel discussion, we discuss IRT (Item Response Theory), GRM (Graded Response Model) and the advantages to using the Bayesian approach at Alva Labs.\n\nItem response theory, also known as the latent response theory, refers to a family of mathematical models that attempt to explain the relationship between latent traits (unobservable characteristic or attribute) and their manifestations (i.e. observed outcomes, responses or performance). Graded response model (or Ordered Categorical Responses Model) is a family of mathematical models for grading responses.\n\nTimestamps#\n\n00:00 Introduction to PyMC Labs\n\n02:48 Panelists introduction\n\n06:05 Outline of the talk by Morgan\n\n06:51 Alva Labs\n\n08:33 Alva Labs personality test\n\n12:14 Item Response Theory and its advantages\n\n16:12 Question: Won't people fake answers to the personality questionnaire if they know what the company is looking for?\n\n18:09 Question: What algorithm is used for combining data points?\n\n19:36 Graded Response Model\n\n20:50 Bayesian Inference\n\n23:19 ALva Labs workflow\n\n25:08 Is the person trait supposed to be a measure of performance and how is it quantified for training?On which data is the model trained?\n\n27:37 Emerging challenges over the years\n\n30:45 How PyMC helped Alva Labs improve their personality model\n\n32:33 Understanding the problem at Alva Labs\n\n34:34 Bayesian workflow\n\n35:32 Simulate the data generating process\n\n38:12 Develop the model\n\n40:25 Evaluate alternative parameterizations\n\n43:40 Test different inference engines\n\n46:05 Use the mode4 with real data\n\n47:52 The final deliverable\n\n49:48 Results of the new model compared to the original Alva Labs model\n\n51:12 Question: Could you comment on how much faster the sampler becomes and why do you care about memory?\n\n53:40 Question: How is the trained model validated and how do you know the person trait is useful and how is the usefulness measured?\n\n55:02 How do you often rerun the model to update the parameters\n\n56:02 Thank you!\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/privacy-policy": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\n\nPrivacy Policy\n\nAt PyMC Labs, we are committed to protecting your privacy and ensuring that your personal data is handled in accordance with the EU General Data Protection Regulation (GDPR).\n\nWhat personal data we collect\n\nWe may collect the following types of personal data:\n\nName, email address, and other contact information you provide when you sign up for our newsletter or contact us through our website\nInformation you provide when you apply for a job or research position with our group\nWebsite usage information collected through cookies and similar technologies\nHow we use your personal data\n\nWe will use your personal data for the following purposes:\n\nTo provide you with information about our research, events, and services, including our newsletter\nTo process your job or research application\nTo analyze website usage and improve our website\nLegal basis for processing your personal data\n\nWe will only process your personal data when we have a legal basis for doing so, which may include:\n\nYour consent to the processing\nProcessing necessary for the performance of a contract with you or to take steps at your request prior to entering into a contract\nProcessing necessary for the legitimate interests of our research group, such as improving our services and communicating with you about our research and events\nSharing your personal data\n\nWe may share your personal data with third parties in the following circumstances:\n\nWith service providers who assist us in providing our services, such as email newsletter providers and website analytics providers\nWith our research collaborators and partners, where necessary for the conduct of joint research projects\nWith government authorities or law enforcement agencies where required by law\nInternational transfers of personal data\n\nYour personal data may be transferred to and processed in countries outside the European Economic Area (EEA). Where we transfer your personal data outside the EEA, we will ensure that appropriate safeguards are in place to protect your personal data, such as by using standard contractual clauses approved by the European Commission.\n\nRetention of personal data\n\nWe will retain your personal data only for as long as necessary to fulfill the purposes for which it was collected, or as required by law.\n\nYour rights\n\nYou have the following rights with respect to your personal data:\n\nThe right to access your personal data and receive a copy of it\nThe right to erasure of your personal data\nThe right to restrict processing of your personal data\nThe right to object to processing of your personal data\nThe right to data portability, where applicable\nThe right to withdraw your consent at any time, where processing is based on consent\n\nTo exercise your rights, please contact us using the contact details below.\n\nContact us if you have any questions or concerns about our privacy policy or the processing of your personal data.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/workshops/applied-bayesian-modeling": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\n Live Cohort\n 4 Weeks\n Beginner - Intermediate\nApplied Bayesian Modeling Workshop\n\n— October cohort closed, join the waiting list for next cohort\n\nJoin the Waiting List\nWhat you'll learn\n How to build and interpret Bayesian models to solve real-world problems.\n How to run and diagnose MCMC workflows for reliable, interpretable results.\n Practical skills in using PyMC to model uncertainty and understand complex systems.\n Debugging, improving, and scaling models that apply to your own work.\nAbout this course\n\nThis hands-on, expert-led workshop from the developers of PyMC will take you from foundational Bayesian thinking to advanced modeling techniques. Designed for engineers, analysts, and scientists, this course emphasizes practical use over academic formality, helping you unlock the power of Bayesian methods in your work. At the end of this course, you'll understand how Bayesian models work and how to confidently apply them to your own data and challenges. Topics include:\n\nBayesian Thinking & Model Building\nHands-On MCMC & Inference Techniques\nAdvanced Topics: Hierarchical, Causal, & Time Series Models\n\nThe workshop is delivered through 2-hour live sessions twice a week over four weeks. All sessions are recorded and available for the duration of the workshop. Students will have access to a private GitHub repository with all code examples and a dedicated Discord server for communication with instructors and fellow learners.\n\nWho should join?\n\nThis course is ideal for software engineers, data analysts, and data scientists who want to move beyond black-box models and start building interpretable, flexible Bayesian models. No prior Bayesian experience is required, just curiosity and some Python.\n\nCourse Outline\nDate\tTime (ET)\tInstructor\tTopic\nMon, Oct 6\t11am - 1 pm\t\nAllen Downey\n\tIntro to Bayesian modeling and PyMC\nWed, Oct 8\t11am - 1 pm\t\nChris Fonnesbeck\n\tBuilding Models in PyMC\nMon, Oct 13\t11am - 1 pm\t\nVianey Leos Barajas\n\tMCMC\nWed, Oct 15\t11am - 1 pm\t\nVianey Leos Barajas\n\tPriors and Likelihood Choices\nMon, Oct 20\t11am - 1 pm\t\nVianey Leos Barajas\n\tBayesian Regression\nWed, Oct 22\t11am - 1 pm\t\nAllen Downey\n\tHierarchical Models\nMon, Oct 27\t11am - 1 pm\t\nChris Fonnesbeck\n\tCausal Inference Models\nWed, Oct 29\t11am - 1 pm\t\nAllen Downey\n\tTime Series Models\nEarn a certificate of completion\nDownload Certificate\nShareable on\nLinkedIn\nCertificates are globally recognized & they upgrade your programming profile.\nCertificates are generated after the completion of course.\nShare your certificate with prospective employers and your professional network on LinkedIn.\nInstructors\nChris Fonnesbeck\nChris is a Principal Quantitative Analyst at PyMC Labs and an Adjoint Associate Professor at the Vanderbilt University Medical Center, with 20 years of experience as a data scientist in academia, industry, and government. He is interested in computational statistics, machine learning, Bayesian methods, and applied decision analysis. He hails from Vancouver, Canada and received his Ph.D. from the University of Georgia.\nAllen Downey\nAllen Downey is a Principal Data Scientist at PyMC Labs and professor emeritus at Olin College. He is the author of several books about programming and data science, including Think Python, Think Bayes, and Probably Overthinking It.\nVianey Leos Barajas\nVianey Leos Barajas is an Assistant Professor, jointly appointed between the Department of Statistical Sciences and School of the Environment at the University of Toronto. She works in the areas of ecological statistics, time series modeling, and Bayesian statistics.\nProfessionals from Top Industries Learn from Us\nWhat students are saying\nWe regularly teach this workshop in various companies and got great feedback along the way\n\nI think the overall organization is great -- the lectures, course materials, the chat. Thank you for always answering questions even if they take the time away from the main material -- your answers really help!\n\nStill Having Doubts?\nDo I need prior experience with Bayesian stats or PyMC?\n\nNo prior experience with Bayesian methods is required. Familiarity with Python and basic statistics is helpful.\n\nWill MMMs be covered?\n\nThis workshop will not cover MMMs specifically, but will cover the foundational tools and techniques that are needed for Bayesian MMMs.\n\nWhat can I expect after I register?\n\nYou'll get a payment confirmation right away, and within two business days you'll get a welcome email with everything you need to get started, including how to access course materials.\n\nWhat is the time commitment?\n\nThere are 20 hours of live instruction: two 2-hour live sessions each week over four weeks, for eight sessions total. You'll have notebooks and access to instructors via Discord between sessions.\n\nWill sessions be recorded?\n\nYes, sessions will be recorded and available to participants for the duration of the course.\n\nWill I have access to the code?\n\nYes, a shared private repository will have all the code used in class.\n\nWhat if I can't attend a session live?\n\nNo worries! Recordings are available for the duration of the workshop and you'll still have access to instructors via Discord for help.\n\nWhat is the refund policy?\n\nIf the Participant cancels the subscription, a refund of the Fee will only be provided if the cancellation is made at least 7 days prior to the start of the Workshop.\n\nAdditionally, the Participant, as a natural person, has the right to withdraw from the subscription within 14 days of registration and receive a full refund, provided that the Workshop has not yet been delivered. See our full Terms and Conditions .\n\nApplied Bayesian Modeling Workshop\nBeginner - Intermediate\nOctober cohort closed, join the waiting list for next cohort\n4 Weeks\n8 Live Workshops\nAccess to Alumni Discord Q&As\nInstructors: Chris Fonnesbeck, Vianey Leos Barajas, Allen Downey\nJoin the Waiting List\nLooking for a team offer?\n\nContact us at info@pymc-labs.com to get a special group rate for multiple registrations from the same company.\n\nAlongside our regular consulting work, we regularly offer corporate workshops for teams looking to deepen their applied Bayesian modeling skills\n\nWe offer workshops at three levels: beginner, intermediate and advanced, and the content is tailed to your team's specific use cases, industry challenges, and technical backgrounds. Whether you're focused on experimentation, forecasting, causal analysis, or something else entirely, we'll tune the material to make it directly relevant to your goals.\n\nIf this looks like a good fit for your team, please email us at info@pymc-labs.com. We look forward to hearing from you.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/team": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nAbout\n\nWe are the creators of PyMC, the leading open-source software for statistical and Probabilistic AI modeling. Now, we've launched a consultancy to put our expertise to work for you.\n\nOur team of top Data Science and AI professionals has decades of experience developing impactful software solutions. We’ve successfully tackled complex challenges for hundreds of companies, demonstrating a proven track record at scale.\n\nComposed of PhDs, professors, mathematicians, AI specialists, and software engineers, our team stands out for its deep specialization in Probabilistic AI methods and Generative AI. At PyMC Labs, we craft custom models that integrate domain knowledge and precisely quantify uncertainty—delivering solutions tailored to your unique business challenges.\n\nOur Partners\nDr. Thomas Wiecki\n\nFounder of PyMC Labs\n\nBiography:\n\nAuthor of PyMC, the leading platform for statistical data science\n\nPhD in Computational Cognitive Neuroscience from Brown University\n\nFormer VP of Data Science and Head of Research at Quantopian Inc: Built a team of data scientists to build a hedge fund from a pool of 300k crowd researchers\n\nRecognized Public Speaker: Keynotes at the Open Data Science Conference (ODSC) & TACC as well as talks at Strata Hadoop & AI, Newsweek AI conference, and various PyData conferences around the world\n\nDr. Christian Luhmann\n\nChief Operating Officer\n\nBiography:\n\nPhD in Psychology from Vanderbilt University, BS in Computer Science from Northeastern University\n\nFormer Professor at Stony Brook University\n\nMore than 20 years of experience teaching and conducting research in data science, econometrics, statistics, and research methods\n\nLeads the PyMC Community Team, co-organizer of the PyMCon Web Series, and regularly hosts PyMC office hours and sprints for new project contributors\n\nDr. Luca Fiaschi\n\nPartner Gen AI Vertical\n\nBiography:\n\n15+ years of experience leading AI, infrastructure and analytics teams in hypergrowth tech companies. Successfully scaled data teams from 0 to 100 professionals\n\nFormer Chief Data & AI Officer at Mistplay, where he developed reinforcement learning models for personalization, recommendation engines, scalable data platforms, and advanced experimentation platforms enabling 200+ annual A/B tests and generating hundreds of millions of dollars in revenue\n\nFormer VP Data Science and HelloFresh where he oversaw $800M Marketing budget optimization and spearheaded Probabilistic AI Causal Modeling for pricing analytics and experimentation. Previous to that, management roles at Alibaba, Stitch Fix and Rocket Internet SE\n\nPhD in Computer Science at Heidelberg University, with expertise in AI and computer vision\n\nNiall Oulton\n\nVP Marketing Analytics\n\nBiography:\n\nMasters in Econometrics, University of Bristol (2014 - 2015)\n\n10 years industry and agency experience, developing a strong domain expertise in marketing. Successfully managed and coordinated high-value global clients in complex market environments, handling billion-dollar marketing budgets. Including the in-housing of holistic marketing measurement platforms driving million-dollar marketing scenarios\n\nCore PyMC-Marketing Developer: UI, time-varying parameterization & budget optimization\n\nJoe Wilkinson\n\nVP Marketing Analytics\n\nBiography:\n\nBSc in Economics, University of Sheffield\n\n15 years marketing analytics experience. Former Senior Partner at Gain Theory, leading MMM consultancy, where he led the internal innovation team and the development of the internal data science platform\n\nLed and contributed to marketing effectiveness programs for major brands in the CPG, Retail, Entertainment, Government and D2C verticals\n\nSome Of Our Clients\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/benchmark/LLMPriceIsRight": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nCan LLMs estimate prices?\n\nWe developed a novel LLM benchmark for evaluating the ability to estimate consumer product prices and reason strategically in an analogue of \"The Price is Right\" showcase game.\n\nWhat This Benchmark Tests\nPrice Knowledge\n\nCan models accurately estimate the cost of everyday consumer products?\n\nStrategy\n\nCan models bid strategically to avoid disqualification while maximizing accuracy?\n\nView Leaderboard\nRead Blog Post\nGitHub & Data\nSubmit Model\nWhy This Benchmark Matters For Business?\n\nWhile the benchmark may seem just like a game, it demonstrates the AI capabilities that are useful in several real business applications\n\nMarket Entry Research\n\nWhen expanding into new geographic regions or product categories, businesses need accurate price estimation capabilities.\n\nPrice Elasticity Research\n\nAI models that understand pricing can analyze how pricing changes affect demand signals and consumer behavior.\n\nEconomic Indicator Development\n\nConsumer price data serves as a leading indicator for broader economic trends and market conditions.\n\nRegulatory Compliance\n\nAccurate price estimation helps businesses maintain compliance with price discrimination and antitrust regulations.\n\nTop Performing Models\n🏆 Best Elo Rating\n#1\tqwen3-30b-a3b\t1239\n#2\tgpt-5\t1207\n#3\tgpt-4o\t1178\n#4\tgrok-4\t1170\n#5\to3\t1129\n📊 Lowest MAPE\n#1\to3\t13.78%\n#2\to1\t14.49%\n#3\tgpt-5\t17.06%\n#4\tgrok-4\t19.47%\n#5\tqwen3-30b-a3b\t22.84%\n⚖️ Lowest Overbid Rate\n#1\tqwen3-30b-a3b\t18.37%\n#2\tgpt-3.5-turbo\t20%\n#3\tgpt-4o\t28%\n#4\tgpt-4o-2024-08-06\t40%\n#5\tgpt-5-mini\t40.82%\nView Leaderboard\n\nLatest Update: September 25, 2025\n\nThe Showcase Challenge\n\nThis benchmark tests whether LLMs can understand real-world pricing and make strategic decisions under constraints, inspired by the Showcase game in \"The Price is Right\" TV show.\n\nHow the Game Works:\n• Two LLMs compete as contestants viewing the same showcase\n• Each model bids on the price of an everyday item (toothpaste, snacks, cleaners, etc.)\n• The closest bid without going over wins the round\n• If both models overbid, neither wins\n• Showcases typically total around $20 in value\nExample Products from The Price is Right\nKraft Cool Whip\n\n8oz dessert topping\n\n$2.29\n\nMezzetta Roasted Peppers\n\n16oz jar\n\n$3.99\n\nMinute Maid Orange Juice\n\n1 gal container\n\n$7.49\n\nData & Methodology\nData:\n\nWe use the set of grocery products that actually appeared on the Price Is Right show. The dataset consists of a short product description and its actual price in US dollars. All prices are the suggested retail price by the manufacturer on the US west coast. We use a fixed set of static prices in the first version of our benchmark, but we are planning to expand it and update in the future releases.\n\nChallenge Sequence:\n\nEvery model undergoes the same sequence of 50-100 showcases with consumer goods.\n\nEach round includes:\nSend the prompt with the rules of the game and output instructions\nProvide 10 reference price examples from similar products\nParse bids and rationales with strict formatting\nCompare to the actual retail price, calculate the absolute percentage error and determine an overbid\nTournament Structure:\n\nBased on the sequence of challenges and their outcomes we simulate a tournament in which each model competes with every other model and the Elo rating is computed to rank the models.\n\nEvaluation Metrics\nElo Rating\n\nChess-inspired rating system where models gain points for wins and lose points for losses\n\nMAPE (Mean Absolute Percentage Error)\n\nAverage percentage difference between bid and actual price (ignoring overbids)\n\nOverbid Rate\n\nPercentage of bids that exceeded the actual price (automatic disqualification)\n\nKey Findings\n• Elo ratings vary significantly between models, reflecting different skill levels in the game\n• Strategic behavior emerges in top models that balance accuracy with overbid avoidance\n• Price knowledge correlates with model size and training quality\n• Winning requires both accurate estimation and strategic risk management\n\nWhile the benchmark task description is very simple and easily understandable, it demonstrates a possibility to probe serious AI skills: using background knowledge and context to make constrained real-world decisions. By turning a game into a benchmark, we get a structured, repeatable way to measure models' real-world sensibility.\n\nCaveats\n• Game mechanics introduce constraints beyond basic price estimation tasks\n• Dataset limitations exist with static pricing that doesn't capture market dynamics or regional variations\n• Elo ratings can vary when tournaments use few Showcases due to random pairings\n\nWe are planning to address these caveats during further refinements of the benchmark.\n\nReady to Test Your Model?\n\nThink your LLM has what it takes to master \"The Price is Right\" showcase challenge? Submit your model and see how it performs against the current leaderboard champions.\n\nAdd Your Model\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nThe Probabilistic AI Consultancy\n\nAt PyMC Labs, we combine cutting-edge Probabilistic AI with world-class expertise to ignite transformative innovation for your business. Driven by our commitment to open-source excellence, we deliver scalable, reliable solutions designed to tackle your most challenging problems. We champion openness and collaboration, embedding transparency into every solution we create.\n\nContact Us\nOur team works with problems where traditional machine learning fails.\nWe specialize in innovative AI solutions that adapt to complex challenges.\nOur expertise lies in building AI models that go beyond conventional frameworks.\nWhat We Do\nModeling & Optimization\n\nUnlock the value of your data with bespoke Probabilistic AI models—designed, implemented, and optimized for precision, scalability, and clear business impact.\n\nAI Systems\n\nAccelerate innovation with custom AI agents and systems that integrate state-of-the-art Probabilistic AI and generative AI technologies for actionable insights and automation.\n\nWorkshops & Speaking\n\nEmpower your teams with hands-on training and expert-led workshops in Probabilistic AI modeling, AI, and advanced analytics, all tailored to real-world business challenges.\n\nStrategy & Technical Advising\n\nWhether it's advising executives to craft innovative, long-term strategies or guiding data science teams through the intricacies of technical implementation, access to our expertise ensures cohesive alignment and impactful execution across all organizational levels.\n\nRoadmap Acceleration\n\nSupercharge your in-house capabilities by embedding world-class PyMC Labs experts directly into your data science and AI teams for rapid delivery and knowledge transfer.\n\nWhat makes PyMC Labs distinctive?\n\nPyMC Labs, founded by the creators of PyMC, delivers unmatched expertise in Probabilistic AI, empowering organizations to tackle complex challenges beyond the reach of traditional methods. Our approach provides interpretable solutions that integrate domain knowledge, achieving greater accuracy while requiring significantly less data than conventional Machine Learning techniques.\n\nCONTACT US\nSome Of Our Clients\nWhat Our Clients Say\n\n\"At Colgate-Palmolive, we really value the relationship we've built with PyMC Labs. They continue to deliver truly unmatched quality work on the hardest and most cutting edge problems we encounter. Their blend of deep Bayesian expertise, GenAI, and domain knowledge makes them an essential partner for delivering innovative, practical, and impactful solutions.\"\n\nIraklis Pappas\n\nGlobal Head of AI\n\nColgate-Palmolive\n\n\"This was one of the main advantages of working with PyMC Labs.Those are people who have been building the tool that we're using for modeling and so my prior was that these people know how to actually put together a firm foundation of the model and it ended up being exactly the way I expected.So we got a very solid, very efficient, very quickly-running model so that we could easily go and expand it.\"\n\nTarmo Jüristo\n\nCEO\n\nSALK\n\n\"We wanted to be able to draw some big conclusions out of a big set of data.So, that's why we came to PyMC Labs for help.It was very successful collaboration.I've had many, many consultants working with in the past, and I think this is by far the most successful Collaboration that I've seen.\"\n\nTiti Alailima, MSE\n\nVP of Applied Data\n\nAkili\n\n\"I have some solid basis, but I'd say like, I'm sort of like random data scientist, not an expert in Probabilistic AI statistics and so, there was so much that i could do by myself.I was able to set up an initial model and get some interesting results and get buy-in internally to go further and that's where additional expertise.It was very helpful to get the model to the finish line and to production.\"\n\nManu Martinet, Phd\n\nLead Data Scientist\n\nIndigo\n\n\"At Ovative Group, PyMC-Marketing is our go-to for building custom MMMs. Its flexibility and customizability let us tailor robust models to each client’s needs. It’s a powerful tool that helps us deliver deeper insights and smarter media strategies.\"\n\nTim McWilliams\n\nSr. Manager Data Science\n\nOvative Group\n\n\"PyMC Labs has significantly enhanced our testing capabilities by leveraging the full power of Bayesian programming, maximizing the potential of the PyMC software. Their advisory role in delivering new feature requests and training our team has been invaluable, driving substantial improvements in our operations.\"\n\nNathan Kafi\n\nPrincipal Data Scientist\n\nHaleon\n\nWe make Our Work Available to the World\n\nWe champion transparency and open-source collaboration. By combining rigorous methods with open innovation, we ensure you benefit from cutting-edge advancements, trust-driven relationships, and future-proof solutions.\n\nThe premier open-source solution for Probabilistic AI Media Mix Modeling and marketing analytics.\n\nA powerful Python package enabling Probabilistic AI causal inference in quasi-experimental settings.\n\nA cutting-edge probabilistic programming framework in Python, purpose-built for Probabilistic AI.\n\nReady to Transform\nYour Data Strategy?\n\nUnlock the full potential hidden in your data. Partner with PyMC Labs and experience firsthand how probabilistic AI can drive smarter decisions, clearer insights, and measurable growth.\n\nLet’s talk about your next breakthrough!\nFeatured Articles\n\nPyMC Labs specializes in advanced Probabilistic AI modeling to provide data-driven insights and solutions.\n\nFrom Weeks to Minutes: Accelerate building your Bayesian Marketing Mix Model using Fivetran & PyMC-Marketing\n\nFivetran and PyMC-Marketing integrate to deliver a production-grade Bayesian MMM pipeline. Standardized dbt outputs flow directly into PyMC-Marketing loaders, enabling faster insights, defendable uncertainty estimates, and scalable budget optimization.\n\nIntroducing the BETA Release of Our MMM Agent - Powered by PyMC-Marketing\n\nWe're thrilled to open up BETA access to the latest version of our Marketing Mix Modeling (MMM) Agent - a fully AI-driven assistant built on top of PyMC-Marketing that turns what used to be a multi-month modeling effort into an interactive, informative, and insightful workflow in hours.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nFrom Weeks to Minutes: Accelerate building your Bayesian Marketing Mix Model using Fivetran & PyMC-Marketing\nSeptember 01, 2025\n\nFivetran and PyMC-Marketing integrate to deliver a production-grade Bayesian MMM pipeline. Standardized dbt outputs flow directly into PyMC-Marketing loaders, enabling faster insights, defendable uncertainty estimates, and scalable budget optimization.\n\nAI-based Customer Research: Faster & Cheaper Surveys with Synthetic Consumers\n\nMarketing Mix Modeling : A Complete guide\n\nThis blog explains how Marketing Mix Modeling separates base and incremental sales, adds lag and saturation effects, and uses Bayesian methods to handle uncertainty. It also shows how these ideas are applied in practice, from early PyMC models at HelloFresh to more recent work with PyMC-Marketing at companies like Bolt.\n\nLLMs and Price Reasoning: Toward an Industry Benchmark\n\nNovel LLM benchmark for evaluating the ability to estimate consumer product prices and reason strategically in an analogue of \"The Price is Right\" showcase game.\n\nPyMC-Marketing vs. Meridian: A Quantitative Comparison of Open Source MMM Libraries\n\nThis benchmark study directly compares PyMC-Marketing and Google’s Meridian on realistic synthetic datasets, from startup-scale to enterprise-level. Using aligned priors and identical configurations, it shows that PyMC-Marketing is consistently faster (2–20x), more accurate (lower error in channel contribution recovery, higher R², lower MAPE), and more scalable, thanks to its flexible sampling backends. Meridian remains leaner in storage size but suffers from slower performance, wider uncertainty, and convergence issues at scale. Overall, PyMC-Marketing emerges as the more robust, production-ready MMM library for data science teams.\n\nAnnouncing the Expert Access Program (EAP)\n\nTeams face critical deadlines where models must be rigorous, but hiring consultants or adding headcount only solves problems temporarily. This is why we created the Expert Access Program — to give teams ongoing access to our experts, ensuring their models keep delivering value long after the initial build.\n\nCan LLMs play The Price is Right?\n\nSynthetic consumers—LLMs simulating human survey participants—are becoming a powerful tool for marketing and behavioral research. They promise faster iteration, lower costs, and broader flexibility than traditional panels. But for them to be useful, they need not only to sound realistic, but also to demonstrate some level of real-world reasoning. A core question in this space: do LLMs “understand” prices?* That is, can they recognize how much everyday items cost, and make decisions based on that understanding?\n\nIntroducing the BETA Release of Our MMM Agent - Powered by PyMC-Marketing\n\nWe're thrilled to open up BETA access to the latest version of our Marketing Mix Modeling (MMM) Agent - a fully AI-driven assistant built on top of PyMC-Marketing that turns what used to be a multi-month modeling effort into an interactive, informative, and insightful workflow in hours.\n\nWrite Me a PyMC Model\n\nModelCraft is an AI agent that writes, checks, and refines PyMC models — turning natural language prompts into validated Bayesian code. Built during a PyMC Labs hackathon, it combines LLMs, LangGraph, and a secure compiler to eliminate hallucinated functions and broken models, making Bayesian modeling faster and easier for all.\n\nApplication of Bayesian Computation in Finance\n\nThis blog post explores the transformative potential of Bayesian computation and PyMC in the field of finance. It highlights how Bayesian methods can enhance financial analysis by quantifying uncertainty, overcoming restrictive assumptions of traditional econometric models, and managing complex model structures with non-normal distributions. The post emphasizes the advantages of Bayesian statistics, such as its ability to provide a comprehensive and interpretable framework for decision-making under risk, and showcases practical applications using PyMC.\n\nAI Innovation Lab: An agentic platform for transforming product development\n\nAt PyMC Labs, we’re tackling a core problem in the CPG industry: product innovation is too slow, fragmented, and disconnected from real consumer needs.\n\nCan Synthetic Consumers Answer Open-Ended Questions?\n\nEvaluating LLMs on generating open-ended text responses.Synthetic consumers are revolutionizing market research by making it faster, less expensive, and more flexible.\n\nProbabilistic Time Series Analysis: Opportunities and Applications\n\nProfound impact of Bayesian modelling on businesses decisions.\n\nSynthetic Consumers: The Promise, The Reality, and The Future\n\n📢 Announcing Our First White Paper: \"Synthetic Consumers: The Promise, The Reality, and The Future\"\n\nHow Realistic Are Synthetic Consumers?\n\nEvaluating LLMs on Political and Lifestyle Choices\n\nThe AI MMM Agent, An AI-Powered Shortcut to Bayesian Marketing Mix Insights\n\nAI revolutionizes marketing analytics by dramatically accelerating the traditional marketing mix modeling (MMM) process. This AI agent automates PyMC-Marketing workflows, delivering MMM results in hours instead of months.\n\nCausal sales analytics: Are my sales incremental or cannibalistic?\n\nThis post explores causal sales analytics, helping companies estimate whether sales from a new product are incremental or cannibalistic. The article discusses the complexities of such analysis and the need for bespoke causal models, ultimately enhancing decision-making for product portfolio management.\n\nUnobserved Confounders, ROAS and Lift Tests in Media Mix Models\n\nUnderstanding the role of lift tests in calibrating Media Mix Models.When working with Media Mix Models (MMMs), calibration with lift tests is not just a technical step —it's essential for making these models reliable and actionable.\n\nBayesian Baseball Monkeys\n\nUsing Bayesian methods to implement a MARCEL-style projection system for MLB ...\n\nHierarchical Customer Lifetime Value Models\n\nThis post explores the application of hierarchical Bayesian models to Customer Lifetime Value (CLV) prediction, improving accuracy for customer cohorts, and addressing seasonality and data sparsity.\n\nCustomer Lifetime Value in the non-contractual continuous case: The Bayesian Pareto NBD Model\n\nExploring the Bayesian Pareto NBD model for predicting customer lifetime value and purchase behavior\n\nCohort Revenue & Retention Analysis\n\nExplore how Bayesian methods can be applied to cohort-level customer lifetime value (CLV) models. This post focuses on combining retention and revenue components for improved forecasting and strategic insights in marketing.\n\nMastering Marketing Effectiveness: A Comprehensive Guide for Digital Marketers\n\nIn today's fast-paced digital marketing landscape, it's crucial to master measuring and understanding marketing strategies' effectiveness. This guide covers the importance of marketing effectiveness, explores various evaluation methods, and presents best practices for implementing an effective marketing measurement strategy.\n\nLatent Calendar: Modeling Weekly Behavior with Latent Components\n\nWe will delve into how Latent Dirichlet Allocation can be applied to discretized calendar events, allowing us to tap into the model's probabilistic origins and its connection to Bayesian principles, offering a wide array of potential applications and insights.\n\nDeveloping Hierarchical Models for Sports Analytics\n\nGrasp the intricacies of hierarchical models in the realm of sports analytics. This article presents a comprehensive analysis of these advanced techniques, highlighting their potential in transforming data-driven sports strategies.\n\nFrom Uncertainty to Insight: How Bayesian Data Science Can Transform Your Business\n\nCausal analysis with PyMC: Answering 'What If?' with the new do operator\n\nLearn how to use Bayesian causal analysis with PyMC and the new do operator to answer 'What If?' questions.\n\nBuilding an in-house marketing analytics solution\n\nGet a fresh perspective on constructing an in-house marketing analytics solution. This article offers unique insights into the process, highlighting the benefits and challenges of such an endeavor.\n\nBayesian Methods in Modern Marketing Analytics\n\nDiscover the innovative application of Bayesian methods in the realm of modern marketing analytics. This article offers a fresh perspective on how these advanced techniques are reshaping the landscape of data-driven marketing strategies.\n\nOut of model predictions with PyMC\n\nSimulating data with PyMC\n\nExplore how PyMC can be used for efficient and powerful data simulation.\n\nPyMC-Marketing: A Bayesian Approach to Marketing Data Science\n\nPyMC Labs is excited to announce the initial release of PyMC-Marketing\n\nLikelihood Approximations with Neural Networks in PyMC\n\nWe use an example from cognitive modeling to show how differentiable likelihoods learned from simulators can be used with PyMC.\n\nLikelihood Approximations for Cognitive Modeling with PyMC\n\nDive into cognitive modeling with PyMC. Learn about the impact of likelihood approximations.\n\nHow to use JAX ODEs and Neural Networks in PyMC\n\nLearn how to seamlessly integrate JAX-based ODE solvers and neural networks with PyMC for Bayesian modeling.\n\nHierarchical Bayesian Modeling of Survey Data with Post-stratification\n\nDive into the complexities of Hierarchical Bayesian Modeling applied to survey data with post-stratification. Understand the subtleties of multilevel regression and the potential of Gaussian Processes in this comprehensive analysis.\n\nCausalPy - causal inference for quasi-experiments\n\nUnveil the power of CausalPy, a new open-source Python package that brings Bayesian causal inference to quasi-experiments. Discover how it navigates the challenges of non-randomized treatment allocation, offering a fresh perspective on causal claims in the absence of experimental randomization.\n\nBayesian Marketing Mix Models: State of the Art and their Future\n\nLearn the cutting-edge of Bayesian Marketing Mix Models and glimpse into their promising future. Uncover how these models are revolutionizing business strategies and decision-making processes.\n\nSolving Real-World Business Problems with Bayesian Modeling\n\nA practical guide to solving business problems with Bayesian modeling\n\nBayesian Item Response Modeling in PyMC\n\nUncover the power of Bayesian Item Response Theory in PyMC. Learn how it revolutionizes data analysis and opens up new possibilities for personality modeling.\n\nModeling spatial data with Gaussian processes in PyMC\n\nWe build a Gaussian process model on a geospatial dataset with the goal of predicting expected concentrations of a radioactive gas in households depending on the county the houses belong to.\n\nBayesian inference at scale: Running A/B tests with millions of observations\n\nIndustry data scientists are increasingly making the shift over to using Bayesian methods. However, one often cited reason for avoiding this is because “Bayesian methods are slow.”\n\nBayesian Modeling in Biotech: Using PyMC to Analyze Agricultural Data\n\nUncover the power of Bayesian modeling in biotechnology. Learn how PyMC is used to analyze complex agricultural data, providing valuable insights for the industry.\n\nBayesian Media Mix Models: Modelling changes in marketing effectiveness over time\n\nwe outlined what Bayesian Media Mix Models (MMM's) are, how they worked, and what insights they can provide.\n\nNBA Foul Analysis with Item Response Theory using PyMC\n\nDelve into the use of Bayesian Item Response Theory and the Rasch model for analyzing NBA foul calls data. The model estimates individual player contributions to foul outcomes.\n\nWhat if? Causal inference through counterfactual reasoning in PyMC\n\nUnravel the mysteries of counterfactual reasoning in PyMC and Bayesian inference. This post illuminates how to predict the number of deaths before the onset of COVID-19 and how to forecast the number of deaths if COVID-19 never happened. A must-read for those interested in causal inference!\n\nPyMC, Aesara and AePPL: The New Kids on The Block\n\nDive into the world of PyMC, Aesara, and AePPL, the new powerhouses in probabilistic programming. Discover how they revolutionize Bayesian modeling and open up new possibilities for data analysis.\n\nBayesian Vector Autoregression in PyMC\n\nIt's time to let go of your custom Gibbs sampler\n\n1\n2\nStay connected with latest developments in Probabilistic AI Statistics\nand AI. You can unsubscribe at any time.\nSubscribe\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/terms-and-conditions/": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\n\nTerms & Conditions\n\nThese terms and conditions apply to the online workshop organized by PyMC Labs and govern your registration and participation.\n\nIntroduction\n\nThese terms and conditions (hereinafter referred to as the Terms) apply to the online workshop, which contains sessions (hereinafter referred to as the Workshop) organized by PyMC Labs (hereinafter referred to as the Company) and must be adhered to by the person registering and participating in the Workshop (hereinafter referred to as the Participant).\n\nBy registering for and participating in the Workshop, the Participant agrees to comply with and be bound by these Terms. By agreeing to the Terms set out below, the parties are deemed to have entered into an agreement on the following Terms.\n\nRegistration and Payment\n\nTo register for the Workshop, the Participant must complete the online registration form and provide accurate information.\n\nRegistration is on a first-come, first-served basis.\n\nFull payment of the participation fee (hereinafter referred to as the Fee) is required at the time of registration. Applicable value-added tax (VAT) and any other taxes or charges will be added to the Fee in accordance with the laws and regulations in force at the time of registration.\n\nThe Company may use third-party payment providers to collect payment. The processing is subject to the provider's terms, conditions, and privacy policies, and the Company is not liable for their security or performance.\n\nUpon receipt of the Participant's registration and payment of the Fee, the Participant will receive a confirmation email with details about the Workshop.\n\nWhen enrolling for a Workshop, the Participant represents and warrants that the Participant has the legal capacity and is of sufficient age to enter into a binding contract with the Company.\n\nWorkshop\n\nOnce the Company has received payment of the Fee, the Participant will be granted access to the Workshop.\n\nThe Workshop is conducted live online via Google Meet (or a similar solution) and the Participant is required to attend at the scheduled time. Recordings of the Workshop will be available for later viewing if provided by the Company.\n\nThe Participant is responsible for ensuring the Participant has the necessary technology and internet access to participate.\n\nParticipants are expected to conduct themselves professionally and respectfully during all Workshop activities. Disruptive behaviour may result in removal from the Workshop without a refund of the Fee.\n\nIntellectual Property\n\nThe Participant acknowledges and agrees that all intellectual property related to the Workshop, including but not limited to course materials (e.g., slides, examples, documentation) (hereinafter jointly as the Materials), as well as all associated copyrights, trademarks, trade secrets, patents, and other intellectual property rights (hereinafter jointly as the IP), are the sole and exclusive property of the Company or its licensors. The Participant does not acquire any ownership rights in the IP under these Terms.\n\nThe Participant agrees not to use the IP for any unlawful, infringing, or unauthorized purpose. Reproduction, adaptation, distribution, public display, transmission, resale, or commercial exploitation of the IP, whether electronically, physically, or otherwise, is strictly prohibited without the prior express written consent of the Company. This includes, but is not limited to, the registration of new trademarks, trade names, service marks, or domain names derived from the IP.\n\nUsage of the Workshop Materials\n\nSubject to the Participant's full compliance with these Terms, the Company grants the Participant a limited, non-exclusive, non-transferable, and revocable license to access and use the Workshop and associated Materials strictly for personal, non-commercial use and solely in connection with participation in the Workshop.\n\nThis license will automatically terminate upon (i) the Participant's completion of the Workshop, (ii) cessation of use of the Workshop, or (iii) termination of the agreement concluded between the parties. The Company reserves the right to revoke the license at any time if it suspects misuse of the Workshop or Materials, including but not limited to sharing login credentials, distributing content, or other breaches of these Terms.\n\nThe Workshop and any accompanying Materials are provided for use by the registered Participant only. Sharing access to the Workshop, Materials, or login credentials with any third party is strictly prohibited. If the Company reasonably suspects that such sharing has occurred, it reserves the right to immediately terminate the Participant's access to the Workshop and may, at its sole discretion, charge additional fees for unauthorized registrations or access.\n\nParticipation in the Workshop does not create any partnership, employment, agency, or other form of association with the Company or its affiliated brands. The Participant agrees not to make any public statements, representations, or implications suggesting such an association or endorsement by the Company following their participation in the Workshop.\n\nPublishing information online\n\nThe Participant is permitted to share general insights gained from the Workshop on online platforms or social media, provided that such content remains non-specific and does not disclose any proprietary material or detailed content from the Workshop.\n\nThe Participant is required to include appropriate attribution to the Company, such as a mention or hashtag, in any such publication. The Company reserves the right to request the removal of any such posts or any associated attributions at its discretion.\n\nPersonal Data\n\nThe Company will process the Participant's personal data based on legal grounds and in accordance with the Company's Privacy Policy, which can be found here: Privacy Policy.\n\nAdditionally, by signing up for the Workshop, the Participant agrees that the Company may contact the Participant via email about similar events or services that the Company believes may be of interest. If the Participant no longer wishes to receive such communications, the Participant can unsubscribe at any time using the link provided in our emails or by notifying the Company directly.\n\nCancellation and Refunds\n\nThe Participant may cancel registration at any time before the start of the Workshop. In order to cancel, the Participant must contact the Company using the contact details provided in section 11.\n\nIf the Participant is unable to attend the Workshop for which the Participant registered, the Participant's seat can be transferred to another person. The Participant must notify the Company as soon as possible to arrange the transfer.\n\nIf the Participant cancels the subscription, a refund of the Fee will only be provided if the cancellation is made at least 7 days prior to the start of the Workshop.\n\nAdditionally, the Participant, as a natural person, has the right to withdraw from the subscription within 14 days of registration and receive a full refund, provided that the Workshop has not yet been delivered.\n\nIn case the Company needs to cancel or reschedule a Workshop, the Participant will be notified as soon as possible and will have the option to transfer the Participant's registration to the next available Workshop or receive a full refund.\n\nIf the Workshop has already been delivered, no refund shall be granted under any circumstances.\n\nLiability\n\nThe Participant acknowledges that participation in the Workshop is voluntary and agrees to assume any risk associated with participation.\n\nThe Company is not responsible for any technical issues that the Participant may experience, including but not limited to issues with internet access, hardware, or software.\n\nThe Company does not offer any promises or guarantees with regard to its Workshop or Materials used in the Workshop. The Workshop and the Materials are provided for informational purposes only.\n\nThe Company is not liable for any result or non-result or any consequences which may come about due to the Participant's participation in the Workshop.\n\nThe Company is not liable for any damages that may occur as a result of participation, to the fullest extent permitted by law. This applies to any and all claims, including lost profits or revenues, consequential or punitive damages, negligence, strict liability, fraud, or torts of any kind.\n\nGoverning Law and Jurisdiction\n\nThese Terms shall be governed by and construed in accordance with the laws of Estonia.\n\nAny disputes arising out of or in connection with these Terms shall be subject to the exclusive jurisdiction of the courts in Estonia.\n\nMiscellaneous\n\nModification. The Company reserves the right to modify or amend these Terms at any time without prior notice to the Participant. Any such modifications shall take effect immediately upon being posted on the Company’s Website and shall supersede all previous versions, unless otherwise explicitly stated. The Participant agrees to review these Terms periodically and acknowledges that continued participation in the Workshop constitutes acceptance of any changes.\n\nAssignment. The Participant may not assign, transfer, novate, lease, or otherwise dispose of any rights or obligations under these Terms without the prior written consent of the Company. The Company may assign or transfer the agreement, in whole or in part, including any rights or obligations hereunder, without the Participant’s consent, upon providing notice to the Participant.\n\nSeverability. If any provision or part thereof of these Terms is found to be invalid, illegal, or unenforceable by a court or other competent authority, the remaining provisions shall remain in full force and effect. Any invalid or unenforceable provision shall be severed to the minimum extent necessary, and the remainder of these Terms shall be interpreted so as to best give effect to the original intent of the parties.\n\nNo Waiver. Failure by the Company to enforce any provision of these Terms shall not constitute a waiver of that provision or of any other provision in the future. A waiver shall only be effective if it is in writing and signed by the party granting it. No reliance may be placed on any verbal or implied waiver unless formally documented.\n\nNo Agency, Partnership or Joint Venture. No agency, partnership, or joint venture has been created between the parties as a result of this agreement.\n\nThird Party Services. The Workshop may involve third-party platforms or content and may be subject to their terms and conditions. The Participant agrees to review and comply with any applicable third-party terms. The Company is not liable for any faults, errors, or issues arising from third-party services, including Workshop delivery or enrolment problems. The Workshop may include third-party content or links to external websites. The Company makes no warranties regarding the quality, accuracy, or reliability of such content and is not responsible for it. The inclusion of links does not imply endorsement by the Company.\n\nContact Information. If the Participant has any questions about these Terms, please contact the Company at: info@pymc-labs.com.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/team-detail/joe-wilkinson": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nJoe Wilkinson\nVP Marketing Analytics\nBiography:\n\nBSc in Economics, University of Sheffield\n\n15 years marketing analytics experience. Former Senior Partner at Gain Theory, leading MMM consultancy, where he led the internal innovation team and the development of the internal data science platform\n\nLed and contributed to marketing effectiveness programs for major brands in the CPG, Retail, Entertainment, Government and D2C verticals\n\nSpecializations:\nMarketing Analytics\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/team-detail/niall-oulton": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nNiall Oulton\nVP Marketing Analytics\nBiography:\n\nMasters in Econometrics, University of Bristol (2014 - 2015)\n\n10 years industry and agency experience, developing a strong domain expertise in marketing. Successfully managed and coordinated high-value global clients in complex market environments, handling billion-dollar marketing budgets. Including the in-housing of holistic marketing measurement platforms driving million-dollar marketing scenarios\n\nCore PyMC-Marketing Developer: UI, time-varying parameterization & budget optimization\n\nSpecializations:\nMarketing Analytics\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/team-detail/christian-luhmann": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nDr. Christian Luhmann\nChief Operating Officer\nBiography:\n\nPhD in Psychology from Vanderbilt University, BS in Computer Science from Northeastern University\n\nFormer Professor at Stony Brook University\n\nMore than 20 years of experience teaching and conducting research in data science, econometrics, statistics, and research methods\n\nLeads the PyMC Community Team, co-organizer of the PyMCon Web Series, and regularly hosts PyMC office hours and sprints for new project contributors\n\nSpecializations:\nData Science\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/team-detail/thomas-wiecki": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nDr. Thomas Wiecki\nFounder of PyMC Labs\nBiography:\n\nAuthor of PyMC, the leading platform for statistical data science\n\nPhD in Computational Cognitive Neuroscience from Brown University\n\nFormer VP of Data Science and Head of Research at Quantopian Inc: Built a team of data scientists to build a hedge fund from a pool of 300k crowd researchers\n\nRecognized Public Speaker: Keynotes at the Open Data Science Conference (ODSC) & TACC as well as talks at Strata Hadoop & AI, Newsweek AI conference, and various PyData conferences around the world\n\nSpecializations:\nProbabilistic AI statistics\nteaching\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/team-detail/luca-fiaschi": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nDr. Luca Fiaschi\nPartner Gen AI Vertical\nBiography:\n\n15+ years of experience leading AI, infrastructure and analytics teams in hypergrowth tech companies. Successfully scaled data teams from 0 to 100 professionals\n\nFormer Chief Data & AI Officer at Mistplay, where he developed reinforcement learning models for personalization, recommendation engines, scalable data platforms, and advanced experimentation platforms enabling 200+ annual A/B tests and generating hundreds of millions of dollars in revenue\n\nFormer VP Data Science and HelloFresh where he oversaw $800M Marketing budget optimization and spearheaded Probabilistic AI Causal Modeling for pricing analytics and experimentation. Previous to that, management roles at Alibaba, Stitch Fix and Rocket Internet SE\n\nPhD in Computer Science at Heidelberg University, with expertise in AI and computer vision\n\nSpecializations:\nProbabilistic AI statistics\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/benchmark/LLMPriceIsRight/add-model": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nBack\nModel Submission\nModel Name *\nAPI Endpoint URL *\nAPI Key *\n\nYour API key will be encrypted and used only for benchmark testing.\n\nDescription (Optional)\nSubmit for Review\nAPI Requirements\n\nTechnical specifications\n\nEndpoint Format\n\nOpen-AI compatible endpoint (see OpenAI Docs)\n\nresponse = self.client.chat.completions.create( model=self.model, messages=messages, max_tokens=kwargs.pop(\"max_tokens\", DEFAULT_MAX_TOKENS), **kwargs )\nRequest Body\n{\n  \"role\": \"You are a contestant on The Price is Right...\",\n  \"content\": \"Ajax dishwashing liquid...\"\n}\nExpected Response\n{\n  \"bid\": 1234.56, \n  \"rationale\": \"Brief explanation of how you estimated the value.\"\n}\nReview Process\nInitial Validation: We'll test your endpoint with sample requests to ensure compatibility.\nBenchmark Testing: Your model will compete in 50 showcase rounds.\nResults & Publication: We'll add your model to the public leaderboard if it meets quality standards.\nImportant Notes\nYour model must respond with valid JSON in the specified format\nWe may run approximately 50-100 requests for the full benchmark\nAPI keys are encrypted and never shared publicly\nAPI keys are encrypted and never shared\nModels that don't follow format requirements may be excluded\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/price-benchmark": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nPrice-is-Right LLM Benchmark\nWhat This Benchmark Tests\nHow It Works\nThe Showcase Challenge\nData\nEvaluation Metrics\nModels\nCaveats\nWhy It Matters for Business\nLLMs and Price Reasoning: Toward an Industry Benchmark\n\nSeptember 17, 2025\n\nBy Maxim Laletin, Allen Downey\n\nLarge Language Models (LLMs) are rapidly becoming indispensable assistants across daily life, work, business, and research. As their power grows, so does interest in exploring new applications.\n\nCan LLMs play video games? Can they find vulnerabilities in web applications? Can they predict future events?\n\nTo answer such questions rigorously, we need well-defined tasks, data, and evaluation metrics—an LLM benchmark. Benchmarks measure specific skills, compare models consistently, and help developers choose the right LLM for a purpose.\n\nDemand for new benchmarks also arises from benchmark saturation: as LLMs train on more data, existing benchmarks often become too easy. If a model has seen the test data during training, it may simply memorize answers instead of generalizing. Thus, varied and dynamic benchmarks are crucial.\n\nIn a previous post, we tested whether LLMs can understand prices by simulating the TV game The Price is Right—specifically the “Showcase” segment, where contestants bid on prize totals without overbidding. With minimal instruction and examples, many LLMs performed impressively, sometimes surpassing human contestants.\n\nThis simple yet strategic challenge inspired us to create:\n\nPrice-is-Right LLM Benchmark#\nWhat This Benchmark Tests#\nPrice estimation: Can models accurately estimate consumer product costs?\nReasoning under constraints: Can models follow bidding rules to maximize accuracy while avoiding disqualification?\nHow It Works#\nThe Showcase Challenge#\n\nEach LLM guesses the total price of three products based on short descriptions, instructed not to overbid. Ten product-price examples are provided. The task—called a Showcase—is repeated 50–100 times with randomized product sets. Models don’t compete directly but are later compared in a simulated Tournament using pairwise matchups.\n\nData#\n\nWe use grocery items from the actual show, priced at west-coast retail levels. The database currently includes 820 items. Below we show some examples of the item descriptions and prices.\n\nDescription\tPrice ($)\nMentos chewing gum\t1.69\nSoft Scrub cleanser\t5.29\nZico coconut water\t2.49\nMerzetta roasted bell peppers (16 oz)\t3.99\nJ&D's croutons (4.25 oz)\t2.49\n\nThe pilot version uses static prices from a single source, with no geographic variation. Future updates will expand the dataset for broader coverage.\n\nEvaluation Metrics#\nMAPE (Mean Absolute Percentage Error): Average closeness of bids to actual prices (lower is better).\nOverbid Rate: Percentage of bids exceeding the true price; higher rates suggest weaker compliance with rules.\nElo Rating: Chess-inspired scoring, balancing accuracy and strategy.\n\nLeaderboard example:\n\nRank\tModel\tRating\tMAPE\tOverbid %\n1\tqwen3-30b-a3b\t1231.98\t22.74\t20.41\n2\tgpt-5\t1210.45\t16.79\t42.00\n3\tgpt-4o\t1208.76\t25.90\t26.00\n4\tclaude-sonnet-4-20250514\t1132.58\t26.34\t50.00\n5\to3\t1131.20\t13.67\t50.00\n6\tgemini-1.5-pro\t1117.05\t26.97\t44.00\n7\tgpt-5-mini\t1114.03\t24.42\t40.82\n8\tclaude-3-5-sonnet-20241022\t1076.95\t25.08\t52.00\n9\tglm-4p5\t1075.94\t23.57\t46.00\n10\to1\t1043.51\t14.60\t60.00\nModels#\n\nWe track leading LLMs from OpenAI, Google, Anthropic, and open-source providers like Fireworks. Models must accept text input and return JSON.\n\nThink your LLM can top the leaderboard?\nSubmit it for the next tournament via our benchmark site, or run it yourself using the GitHub framework.\n\nCaveats#\nThe task adds game-like restrictions beyond simple price prediction.\nData is static and limited, not reflecting dynamic pricing factors.\nElo ratings can vary when tournaments use few Showcases due to random pairings.\n\nWhile our benchmark provides meaningful insights into LLM performance, there’s room to grow. We see these limitations as opportunities for enhancement, and future versions will build on this strong foundation.\n\nWhy It Matters for Business#\n\nThough playful, the benchmark highlights skills essential in practice:\n\nMarket Entry Research: Estimate prices in new regions or categories.\nPrice Elasticity Research: Detect dynamics and forecast demand.\nEconomic Indicators: Use consumer prices as signals of broader trends.\nRegulatory Compliance: Ensure accurate estimates to meet fairness and antitrust standards.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/benchmark/LLMPriceIsRight/leaderboard": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nBack\nModel Performance Comparison\nElo Rating\nMAPE\nOverbid Rate\n\nChess-inspired rating system where models gain points for wins and lose points for losses. Higher ratings indicate better overall performance.\n\nLatest Update: September 25, 2025\n\nMAPE vs Overbid Rate\n\nEach point represents a model performance. Lower values on both axes indicate better performance.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/ai-mmm-agent-beta": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\n✅ What’s New (and Fast)\n📈 What’s New Since Alpha\n🔮 What’s Coming Next\n🧪 Join the BETA Today\nIntroducing the BETA Release of Our MMM Agent - Powered by PyMC-Marketing\n\nJuly 11, 2025\n\nBy PyMC Labs\n\nWe're thrilled to open up BETA access to the latest version of our Marketing Mix Modeling (MMM) Agent - a fully AI-driven assistant built on top of PyMC-Marketing that turns what used to be a multi-month modeling effort into an interactive, informative, and insightful workflow in hours.\n\nWhether you're a data scientist, media analyst, or CMO, this new BETA version is designed to get you to ROI-driven decisions faster - and with more confidence.\n\n✅ What’s New (and Fast)#\n\nOur MMM Agent is more than just another tool - it’s your AI co-pilot. Here’s what it can do:\n\nAutomated Data Wrangling\nStop wasting time on reconciling TikTok vs. GA4 inconsistencies. Upload your data and let the agent take care of the prep.\n\nSmart Bayesian Modeling\nAutomatically selects appropriate techniques for carryover, saturation, and trend detection, tailored to your data.\n\nInstant “What-If” Scenarios\nWant to double your Facebook budget? Cut TV spend in half? Simulate it instantly and see the impact on ROI.\n\nCausal Insights > Correlations\nThis isn’t another black-box MMM. Our agent incorporates experiments, interventions, and external data to give causally robust recommendations.\n\n📈 What’s New Since Alpha#\n\nWe’ve made major improvements since the Alpha release, based on your feedback:\n\nInline code and charts within the chat interface\nPersistent visualizations and tables stored in MLflow\nImproved modeling diagnostics and debugging tools\n🔮 What’s Coming Next#\n\nThe BETA is just the beginning. We're working on:\n\n🧾 One-click summary deck generation\n💸 An agent to optimize budgets given a fitted MMM\n🧪 Custom model priors and functions (e.g., adstock curves, saturation functions)\n🧠 Ability to define causal DAGs for deeper, structured modeling\n🧪 Join the BETA Today#\n\nWe're looking for forward-thinking marketing teams, analytics leads, and data scientists to try out our MMM Agent and shape its next phase.\n\nIf you've ever been frustrated by slow MMM cycles, black-box models, or unclear ROI attribution — this is your chance to try something better.\n\n👉 Apply for BETA access and get early insights into what’s working in your marketing mix.\n\nmmm-agent-beta@pymc-labs.com\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/accelerating-bayesian-mmm-fivetran-pymc-marketing": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nThe Integration That Changes Everything\nWhat We Shipped 🚀\nHow It Fits in Your Analytics Stack\nIngestion & Standardization (Fivetran + dbt)\nModel-Ready Shaping (PyMC-Marketing loaders)\nModeling & Decisioning (PyMC-Marketing)\nHierarchical Approaches When Needed\nWhat You Can Expect\nWho Should Use This?\nGet Started Today\nAcknowledgements\nFurther Reading\nFrom Weeks to Minutes: Accelerate building your Bayesian Marketing Mix Model using Fivetran & PyMC-Marketing\n\nSeptember 01, 2025\n\nBy PyMC Labs\n\nMarketing teams consistently report that data wrangling consumes the majority of their Marketing Mix Modeling efforts, leaving little time for the insights that actually drive decisions. This fundamental challenge affects everyone from teams building their first MMM to those optimizing existing ones. What modern growth teams really want are two things: speed to first insight and confidence in the data and methods behind the numbers\n\nThrough our strategic partnership with Fivetran, we're changing that equation entirely. Fivetran and dbt already standardize multi-platform advertising and commerce data, removing weeks of ad-hoc wrangling. PyMC-Marketing now turns those standardized tables into a production-grade Bayesian MMM complete with uncertainty quantification, adstock/saturation effects, and budget optimization without forcing teams to rebuild plumbing they already have.\n\nThe Integration That Changes Everything#\n\nIn concrete terms, Fivetran's ad_reporting dbt package harmonizes spend, impressions, clicks, and conversion metrics from major channels (Google, Meta, LinkedIn, TikTok, Amazon, etc.) into a unified schema (e.g., ad_reporting_ad_report). Crucially, it also supports unioning multiple connections, say, multiple markets or brands, so analysts can analyze blended performance while still tracing each record back to its origin via a source_relation field. That \"many sources, one shape\" design is exactly what a robust MMM pipeline needs.\n\nPyMC-Marketing then picks up where your warehouse leaves off. Based on that data, our Bayesian MMM API estimates channel effects with proper uncertainty, handles carryover (adstock) and saturation, and supports budget allocation, calibration to experiments, and time-varying dynamics. These capabilities are difficult to bolt onto a spreadsheet or rules-based approach.\n\nWhat We Shipped 🚀#\n\nAs part of our collaboration with Fivetran, PyMC-Marketing now includes data loaders that transform Fivetran/dbt outputs into MMM-ready inputs in a few lines (illustrated here with Shopify, but extensible to any e-commerce platform):\n\nprocess_fivetran_ad_reporting(...) turns long-format ad reporting tables into a wide, date-indexed design matrix (X) by platform and metric (e.g., impressions or spend).\n\nprocess_fivetran_shopify_unique_orders(...) converts Shopify orders into a clean target series (y) for modeling conversions/revenue.\n\nfrom pymc_marketing.data.fivetran import (\n    process_fivetran_ad_reporting,\n    process_fivetran_shopify_orders_unique_orders,\n)\n\ncampaign_df = pd.read_sql(\"SELECT * FROM <schema>.<any_fivetran_ad_report>\", con=conn)\norders_df = pd.read_sql(\"SELECT * FROM <schema>.<shopify_report>\", con=conn)\n\nx = process_fivetran_ad_reporting(\n    campaign_df, \n    value_columns=\"spend\",\n    rename_date_to=\"date\"\n)\n# Result: date | facebook_ads_spend | google_ads_spend | ...\n\n# Process conversion data (orders) as target variable\ny = process_fivetran_shopify_unique_orders(orders_df)\n# Result: date | orders\n\n# Use in MMM model\nmmm = MMM(...)\nmmm.fit(X=x, y=y[\"orders\"])\n\n\nThese helpers are built around the schemas emitted by Fivetran's dbt packages and come with example CSVs and tests. They're designed to be run directly on a simple SELECT * FROM . (and analogous Shopify tables) via pandas.read_sql, minimizing glue code between your warehouse and your MMM.\n\n\n\nYou can browse the \"MMM Fivetran Connectors\" example notebook in our docs to see the end-to-end flow from querying standardized tables to fitting an MMM and inspecting outputs. The notebook demonstrates switching between metrics (e.g., impressions vs. spend), handling missing dates, and producing the wide matrix expected by the model.\n\n\n\nHow It Fits in Your Analytics Stack#\nIngestion & Standardization (Fivetran + dbt)#\n\nFivetran connectors land raw platform data; and Fivetran’s quickstart data models (dbt based) transform it into consistent reporting layers such as ad_reporting_ad_report, alongside platform-specific rollups. The package explicitly supports unioning multiple accounts/regions/brands. The resulting models include a source_relation column for lineage. This is ideal for MMM use cases that span multiple markets or portfolios.\n\nModel-Ready Shaping (PyMC-Marketing loaders)#\n\nOur loaders aggregate by platform and date, pivot to wide format, fill gaps if requested, and rename columns to MMM-friendly conventions (e.g., facebook_ads_spend). A corresponding target loader processes Shopify orders to a daily series. With X and y prepared, you can fit a Bayesian MMM immediately.\n\nModeling & Decisioning (PyMC-Marketing)#\n\nThe MMM API provides carryover and shape (saturation) effects, full posterior uncertainty for ROAS and contributions, budget optimization and scenario analysis, plus optional time-varying effects and experiment calibration.\n\nHierarchical Approaches When Needed#\n\nFor teams managing multiple brands or regions, the framework scales to multidimensional approaches without sacrificing individual market insights.\n\nThe result is a clean, auditable pipeline: connect → standardize (dbt) → load (PyMC-Marketing) → infer (Bayesian MMM) → act.\n\n\nWhat You Can Expect#\nTime-to-insight is measured in days, not months. Teams already using Fivetran/dbt can skip bespoke cleaning and schema mapping. The standardized outputs slot directly into our loaders, so you can focus on model assumptions and decision-making instead of ETL.\nConfidence and governance. dbt's tested transformations, plus PyMC's probabilistic treatment of uncertainty, result in MMM outputs you can defend to finance and leadership. The source_relation field and dbt docs provide lineage and explainability across the stack.\nScalability across brands and regions. The union-across-connections feature makes it straightforward to analyze multi-market portfolios without sacrificing traceability, which is key for central teams supporting many business units.\nFlexibility to match your KPIs. Model on impressions, spend, or conversions; swap targets (e.g., orders or revenue) with a parameter change; and extend the model with controls or time-varying dynamics as your questions evolve.\nWho Should Use This?#\nIn-house data teams already landing ad + Shopify data via Fivetran and running the official dbt packages.\nAnalytics consultancies orchestrating dbt transformations for multiple clients and seeking a standardized way to deliver MMM outputs with uncertainty.\nFinance and growth leaders needing defendable ROAS, channel contributions, and budget recommendations aligned to data governance best practices.\nGet Started Today#\n\nStart here: Walk through the MMM Fivetran Connectors notebook to see the complete workflow and copy/paste starter code.\n\nGo deeper: Review what the new Fivetran data loaders do in the corresponding PyMC-Marketing pull request.\n\nAre you new to Fivetran's Ad Reporting? Check out the package README, especially the section on unioning multiple connections, to learn about supported platforms, models, and configurations.\n\nAcknowledgements#\n\nThis work builds on Fivetran's standardization efforts in the dbt ecosystem and PyMC-Marketing's growing MMM toolkit. We're excited to help teams go from raw connectors to Bayesian decisioning with minimal friction.\n\nFurther Reading#\nPyMC-Marketing overview and MMM guide\nMMM API docs and examples\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/2022-11-11-HelloFresh": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nIntroduction\nTimestamps\nResources\nBayesian Marketing Mix Models: State of the Art and their Future\n\nNovember 11, 2022\n\nBy Thomas Wiecki\n\nIntroduction#\n\nThis event is a discussion between Luca Fiaschi and Thomas Wiecki about Bayesian modeling in online marketing with a special focus on Marketing Mix Models and Customer Lifetime Value models.\n\nIn the panel discussion, Luca Fiaschi shares his thoughts on the current state and the future of Bayesian data science in online marketing, based on many years of real-world experience leading teams at HelloFresh and Stitchfix. A special focus will be placed on Marketing Mix Models (MMM) and the work PyMC Labs has done with Luca during his time at HelloFresh. In addition, Customer Lifetime Value models play an ever more important role in marketing but remain stuck in old modeling approaches. We will discuss a potential way forward.\n\nTimestamps#\n\n00:00 Introduction by Thomas\n\n01:39 Luca Fiaschi introduces himself\n\n02:37 Alex Andorra introduces himself\n\n03:57 Attribution methods have multiple use cases across businesses\n\n04:39 Analyze media models\n\n05:23 Find Optimal Budget Allocation(What-if scenarios and optimization)\n\n05:55 Forecasting\n\n06:27 Most common attribution methods\n\n11:02 Structure of a Media Mix Model\n\n12:04 Saturation and Adstock functions\n\n14:04 Advantages of Bayesian Media Mix Models\n\n17:14 Bayesian MMM can be calibrated to ensure consistency with incrementality measurements\n\n19:50 Challenges encountered when developing Hello Fresh’s model\n\n24:36 PyMC Labs work with Hello Fresh to build the model\n\n33:12 Comparing the Hello Fresh model with the other different frameworks\n\n36:05 Business insights that can be derived from the Hierarchical Gaussian Processes model\n\n40:10 Question: How can business people and Data science efforts be aligned in an organization?\n\n44:23 Question: Have you considered introducing relationships between regressor variables\n\n48:19 Question: To what level of confidence are you able to say that you are at a certain saturation level and can you use Bayesian methods to do that?\n\n54:28 Question: Have you ever settled for Gaussian Random walk parameters over latent Gaussian Processes(GPs) for the sake of simplification or explainability?\n\n59:19 Question: Media Mix Models suffer from multicollinearity, any advice?\n\n1:02:14 Thank you and closing remarks\n\nResources#\nImproving the Speed and Accuracy of Bayesian Media Mix Models\nModelling changes in marketing effectiveness over time\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/the-ai-mmm-agent": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nThe Traditional MMM Bottleneck: Why Teams Get Stuck\nAI Agent: From Data to Dollars\nUnder the Hood: AI’s Interaction with Causal Models\nWhy Marketers Love It: From Data to Action\nScaling Up: The Bayesian AI Lab for Cross-Sectional Insights\nReady to Unlock More ROI from Your Marketing Mix?\nThe AI MMM Agent, An AI-Powered Shortcut to Bayesian Marketing Mix Insights\n\nFebruary 24, 2025\n\nBy Luca Fiaschi\n\nAbstract:\n\nWhat if you could transform raw spend data into boardroom strategy in just one day? For CMOs, waiting months for marketing mix modeling (MMM) results is no longer an option. AI is revolutionizing the marketing analytics industry by dramatically accelerating the traditional modeling and insight workflow. In this post, we showcase an AI agent that automates PyMC-Marketing workflows—delivering MMM results in hours instead of months. Discover how it converts raw spend data into boardroom-ready strategy while balancing technical rigor with executive clarity.\n\nThe Traditional MMM Bottleneck: Why Teams Get Stuck#\n\nBuilding a Bayesian marketing mix model today often feels like assembling a plane mid-flight. The process is fraught with obstacles that slow progress at every turn:\n\nData Prep Chaos: Teams frequently spend weeks merging siloed data—from sales and ads to promotions—while wrestling with discrepancies. Questions like, “Are these TikTok spend numbers from Q2 accurate?” or “Why does the CRM data conflict with GA4?” are common. These gaps and inconsistencies require experienced analysts to identify issues and implement painstaking manual fixes.\nModel-Building Guesswork: Once the data is ready, the challenge shifts to model configuration. Data scientists grapple with decisions such as whether the adstock decay should be 30 or 45 days, or if saturation is best modeled using a Hill curve or an exponential function. This trial-and-error approach can involve testing over ten different configurations while battling issues like multicollinearity and the risk of overfitting.\nValidation Black Holes: Even with a model in place, validation can become a black hole. When the MCMC sampling won’t converge, experts are left questioning whether the problem lies with the priors or the underlying spend data. Debugging often consumes more time than building the model itself.\nStale Insights Syndrome: By the time a model is fully built and validated, the market may have already moved on. In fact, industry reports have highlighted that the lengthy process involved in building and updating MMM can severely limit their impact and adoption.\n\nEach of these phases requires time and expertise. In fact, you typically need a team of data scientists, analysts, and engineers to cover all these bases​– talent that is expensive and in short supply​.\n\nIn addition, this series of bottlenecks not only wastes valuable time and resources but also risks delivering outdated insights. This is where our AI MMM Agent steps in to transform the process.\n\nAI Agent: From Data to Dollars#\n\nThe traditional MMM workflow demands specialized expertise and considerable time—but what if AI could do it all? Enter the AI agent, your MMM copilot that compresses weeks of work into mere hours. Leveraging the open-source PyMC-Marketing library from PyMC Labs, this agent automates the end-to-end process of building and running a Bayesian marketing mix model.\n\nKey Capabilities of the AI Copilot\n\nGuided data exploration: The agent assists in accessing and cleaning your data, suggesting tailored visualizations and diagnostics. Combining the expertise of an experienced data scientist with deep knowledge of your specific data context, it uncovers key descriptive insights that might otherwise be missed.\n\nSmart model configuration: Based on your dataset and business context, the AI selects an optimal model structure. For example, if a long time series with underlying trends is detected, the agent enables a time-varying baseline. By harnessing PyMC-Marketing’s high-level API, it instantiates an MMM with the right components (carryover, saturation, seasonality) without the need for extensive manual coding.\n\nFast Bayesian inference: The agent fits the model to your data using PyMC’s efficient sampling methods. With optimizations like custom adstock calculations running in linear time and GPU sampling, the entire process is significantly faster than other implementations.\n\nAutomated insight delivery: The final output isn’t just a static presentation—it’s an interactive expert that translates complex posterior estimates into clear, actionable takeaways and visuals. For instance, it might output:\n\n“Facebook ads drove an estimated 20% of sales with a 4.5× ROI last quarter. Consider shifting the budget from print to Facebook, which could boost overall ROI by X%.” This immediacy and clarity let you bypass manual number-crunching and jump straight to strategy, while enabling live follow-up questions for further exploration. By streamlining data preparation, model building, and interpretation, the AI agent dramatically shortens the time-to-insight. What once took months can now happen in real time—saving costs, reducing labor, and enabling frequent, up-to-date MMM analyses and follow up questions answered in minutes.\n\nUnder the Hood: AI’s Interaction with Causal Models#\n\nThe AI agent isn’t a generic AutoML tool; it’s designed specifically for marketing mix modeling, with an emphasis on causal insights. Beyond speed, our AI agent also ensures the integrity of your insights by embedding causal reasoning into the model:\n\nCausal structure awareness: To answer “what-if” questions, an MMM must capture causal relationships (not just correlations). The AI agent ensures the model includes appropriate control variables and reflects a causal DAG of your marketing ecosystem​. For example, it will account for things like economic trends or competitor actions if those data are provided, so that channel effects are isolated. This causal design underpins reliable scenario planning (e.g., “If we cut email spend by 20%, we expect a 5% drop in sales”).\nExperiment calibration (lift tests): The agent can incorporate results from lift test experiments directly into the modeling process. PyMC-Marketing allows you to add lift test measurements to an MMM before fitting​. By calibrating the model with known incremental lift from experiments, the AI grounds the MMM in real-world cause-and-effect​. This approach helps correct for unobserved confounding and biases in the observational data​. In short, your MMM doesn’t exist in a vacuum – it aligns with any experimental evidence you have, leading to more trustworthy recommendations. For instance you could set prior in words such as “I believe the incremental CAC of facebook is between 200; is this data consistent with my belief?”\n\nBy acting as a sparring partner to intelligently configure these aspects, the AI agent delivers a robust Bayesian MMM that acts as a true causal decision tool—aligning statistical rigor with business reality to provide accurate, actionable insights.\n\nWhy Marketers Love It: From Data to Action#\n\nWhat do marketers and executives stand to gain from this AI-accelerated approach? Here are the headline benefits:\n\nFor Data Scientists:\n\nDramatically Reduce Grunt Work: By automating tedious tasks such as data validation, model configuration, and diagnostics, the AI agent cuts up to 80% of manual effort. This frees you up to concentrate on developing strategic insights rather than getting bogged down in the technical details.\nReal-Time “What If?” Analysis: The agent lets you test scenarios on the fly. Imagine asking, “What happens if we double Amazon Ads during Prime Day?” and receiving instant, actionable feedback. This agility means you can adapt strategies as market conditions change—live and in real time.\n\nFor Executives:\n\nRapid Model Updates for Agile Budgeting: No more waiting months for model insights. With our AI-powered approach, you can update budgets weekly instead of quarterly, ensuring that your decisions are always based on the most current data.\nClarity, Not Jargon: Forget the technical details. The AI agent translates complex outputs into straightforward, boardroom-ready recommendations like, “Stop overspending on saturated channels—here’s your optimal mix.” This clarity empowers you to make confident, data-driven decisions without getting lost in the details.\n\nIn essence, AI-powered MMM transforms what used to be a complex analytical exercise into an ongoing strategic asset. Marketers gain granular insights to fine-tune campaigns, and executives receive a high-level expert assistant – not a dashboard! – that highlights what’s driving ROI. The result is better-performing marketing and a unified strategy for future investments.\n\nScaling Up: The Bayesian AI Lab for Cross-Sectional Insights#\n\nAgentic workflows are revolutionizing industries—from marketing analytics to data science. At PyMC Labs, we’re at the forefront of this innovation, partnering with clients to develop tailored solutions that automate complex analytics workflows. Although the MMM agent can be used as an off-the-shelf solution, it can also be integrated with other agents to automate end-to-end business processes. For instance, the MMM agent can work alongside an inventory management agent that oversees order processing or a sales promotion agent that optimizes the mix of marketing, pricing, and inventory to maximize ROI—all while managing campaign execution.\n\nReady to Unlock More ROI from Your Marketing Mix?#\n\nAn AI-powered MMM shortcut might be the game-changer your team needs. Contact us today to access this cutting-edge solution and start making data-driven marketing decisions faster and smarter.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/pymc-marketing-a-bayesian-approach-to-marketing-data-science": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nWhat can PyMC-Marketing do?\nMedia Mix Models\nCustomer Lifetime Value\nWhy PyMC-Marketing?\nWhy Bayesian?\nWhy embrace open source?\n📞 Schedule a Consultation\nWant to help?\nFind out more\nPyMC-Marketing: A Bayesian Approach to Marketing Data Science\n\nApril 07, 2023\n\nBy Benjamin Vincent\n\nPyMC Labs is excited to announce the initial release of PyMC-Marketing. Unlock the power of marketing analytics with PyMC-Marketing – the open source solution for smarter decision-making. Media mix modelling and customer lifetime value modules allow businesses to make data-driven decisions about their marketing campaigns. Optimize your marketing strategy and unlock the full potential of your customer data.\n\nThe world of marketing is constantly changing. With the rise of digital channels, social media, mobile devices, and high-fidelity tracking, marketers had access to a wealth of insightful data. But with the changes in data privacy (GDPR, CCPA, iOS 14.5, and Google’s cookie policy), the way we collect, analyze, and use this data is changing. To stay ahead of the curve, marketers need powerful tools and techniques that can help them make sense of this data and make better decisions. That's where PyMC-Marketing comes in.\n\nWhat can PyMC-Marketing do?#\n\nPyMC-Marketing provides a user-friendly API for working with two key types of models: Media Mix Models (MMM) and Customer Lifetime Value (CLV). Although MMM and CLV are often treated separately, we believe that employing them side-by-side can provide a more complete picture on the short- and long-run return on investment of marketing activities.\n\nMedia Mix Models#\n\nSince the death of the cookie, it has become very hard, if not impossible to attribute any single sale (or user signup) to a particular advertising channel. You may be spending on various advertising channels like linear TV, different social medias, radio, print etc. But how can you evaluate the effectiveness of these channels in order to intelligently allocate future advertising budgets? This is the goal of Media Mix Models.\n\nWhen used correctly, MMMs can evaluate channel effectiveness, differentiate changes due to advertising spend, holidays, seasonal trends, or macro-economic factors. Models like this account for the diminishing returns of ad exposure over time, and channel saturation.\n\n📖 Find out more about Bayesian MMMs in our blog post Bayesian Media Mix Modeling for Marketing Optimization.\n\nPyMC-Marketing focuses on ease-of-use, so it has a simple API which allows you to specify your outcome (e.g. user signups or sales volume), historical advertising spend data, and control variables such as holidays or seasonality:\n\n➡️ Find out more about how to use MMMs to analyze your data in our Example Notebooks documentation.\n\nCustomer Lifetime Value#\n\nLet's say that you have embraced MMMs and are working towards maximizing new user signups. This is all well and good, but what you really want to do is to acquire more high value customers. But how do you know how valuable different customers are likely to be? You don't know what future business they will generate because it's in the future!\n\nThis is where Customer Lifetime Value (CLV) models come in. They are used to predict future purchases and to quantify the long-term value of a customer. PyMC-Marketing's Customer Lifetime Value module includes a range of models, to predict future churn rates, purchase frequency and monetary value of customers.\n\nAgain, we have a simple API for Customer Lifetime Value modeling. The example below uses a Beta Geometric (aka BG/NBD) model and we specify customer ID, date of each customer’s first and last purchases as well as number of repeat purchases from each customer.\n\n➡️ Find out more about how to use CLV models and the current features in our Example Notebooks documentation.\n\nWhy PyMC-Marketing?#\nOpen source: PyMC-Marketing is open-source, developed by a team of PyMC Labs researchers and a community of experts. PyMC Labs has deep expertise in building Bayesian models to provide business insights. Pairing that with input from a community with strong applied marketing expertise and experience makes for a winning combination.\nPython: PyMC-Marketing is written in Python, an excellent choice to build a data science stack on due to its popularity, existing use in industry, very large community of Python programmers, and suitability to deploying in production.\nReliability: PyMC-Marketing places an emphasis on modularity and rigour, ensured by a comprehensive suite of software tests.\nBayesian: PyMC-Marketing is fully Bayesian, built on top of PyMC, the popular Bayesian statistical modeling package in Python. PyMC itself is well-established, widely-used, road-tested, and has been a NumFOCUS Sponsored Project since 2016.\nWhy Bayesian?#\nData such as customer transactions and advertising expenditure can be low resolution (e.g. monthly), contain measurement errors, and have missing values. This kind of noisy data environment can be a challenge for traditional methods that rely solely upon data to draw their conclusions.\nBayesian approaches allow your team’s valuable domain expertise to be incorporated in the modelling process in the form of Bayesian prior beliefs. This can make a massive difference - especially when data are noisy and uncertain. Rather than your model contorting itself in strange ways to best fit complex and noisy data, Bayesian priors can keep your model right, leading to much more sensible insights that fit with your domain expertise.\nFrequentist approaches can require two or more years worth of historical data. Bayesian methods can work with very short-run data, meaning you don't have to wait a long time collecting data before getting insights. The certainty in your estimates will grow as your dataset increases.\nBayesian modelling excels at modelling hierarchical or nested data. This is particularly useful if you have just launched a new product, or operate in a new region, or are dealing with a new cohort or demographic of customers and don’t have many observations. Bayesian hierarchical models allow information learnt about some categories to intelligently inform you about novel categories.\nBayesian methods provide the way to make decisions under uncertainty. This enables you to generate and predict future scenarios - knowing how certain or not you are of those predictions. You can run optimization processes to decide what actions to take in the future, fully taking uncertainty into account. This helps manage your risk.\nWhy embrace open source?#\n\nIn recent years, there has been a significant shift across many data-intensive industries to build upon open source foundations. There are several advantages in building your marketing data science stack on an open source core:\n\nCost savings: Avoid paying for proprietary software licenses.\nAvoid vendor lock-in: Keep flexibility and control, with the ability to switch to other options or to customize your data science models on top of an open-source core.\nFlexibility and customization: Open source software is customizable and can be tailored to meet the specific needs of your business. This allows you to create a data science stack that is uniquely suited to your business and can give you a competitive advantage.\nIn-housing: Many companies are increasingly bringing their marketing analytics in-house to gain greater control and customization. Building a data science stack on an open core allows for greater customization and frees up your data science team to focus on tailoring the analytics to the company's requirements instead of spending time on foundational components that already exist elsewhere.\nInnovation: While still an early release, PyMC-Marketing represents our commitment to fostering innovation in the data science and marketing analytics communities. We believe that by embracing open source software, we can collectively advance the field with new tools and techniques. Building your data science stack on an open source core, such as PyMC-Marketing, can provide you with access to the latest developments and improvements as they arise. Join us in our mission to make data science more accessible and powerful for everyone.\nCommunity support: The Bayesian core of PyMC-Marketing (i.e. PyMC) is supported by a large and active community of developers, who contribute to its development and provide support and guidance to users. This can be invaluable for businesses that don't have the resources to hire their own team of data scientists.\nTalent acquisition and onboarding: Using open source software as a foundation for your data science stack can help attract top talent to your company. Developers and data scientists are often drawn to companies that use cutting-edge tools and contribute to the larger open source community.\n📞 Schedule a Consultation#\n\nUnlock your potential with a free 30-minute strategy session with our PyMC experts. Discover how open source solutions and pymc-marketing can elevate your media-mix models and customer lifetime value analyses. Boost your career and organization by making smarter, data-driven decisions. Don't wait—claim your complimentary session today and lead the way in marketing and data science innovation.\n\nWant to help?#\n\nWe are on a journey to make Bayesian Media Mix and Customer Lifetime Value models more accessible and user-friendly to the marketing community.\n\nPyMC-Marketing is still in its early stages of development, and we welcome feedback and contributions from the community. Visit our PyMC-Marketing GitHub repository and get involved!\n\nFind out more#\n\nStay tuned for more information here on the PyMC Labs blog, sign up to our newsletter, follow us on Twitter and LinkedIn, its members, and the PyMC-Marketing contributors.\n\nAnd check out the package here where you'll find more details, including installation instructions:\n\nGitHub repository: pymc-labs/pymc-marketing and star the GitHub repo!\nDocumentation: pymc-marketing.readthedocs.io\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/synthetic-consumers": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nKey Insights\nSynthetic Consumers: The Promise, The Reality, and The Future\n\nJune 03, 2025\n\nBy Nina Rismal, Luca Fiaschi\n\nSynthetic consumers – AI-generated personas designed to simulate human consumer behavior – are rapidly transforming market research by delivering faster, cost-effective, and highly scalable insights compared to traditional methods. By 2027, synthetic responses are expected to constitute over half of all market research data, highlighting the urgency for businesses to understand and adopt this technology strategically.\n\nThis white paper equips technical business leaders, consumer insights experts, and data scientists with clear and actionable knowledge about this technology.\n\nKey Insights#\n\nDefining Synthetic Consumers: We clearly define synthetic consumers and\ndistinguish them from related concepts like digital twins, synthetic respondents,\nand human simulacra.\n\nReal-World Use Cases: We highlight practical examples of synthetic consumers\napplications currently used by both major companies and startups for product\ntesting, innovation, data augmentation, and consumer insights.\n\nAccuracy and Validation: We analyze methods for evaluating synthetic consumer\naccuracy, summarizing recent research that increasingly shows confidence in the\nalignment between synthetic and human responses across multiple domains.\n\nPyMC Labs offers unique expertise by creating customized solutions that combine\nadvanced Generative AI with science-based benchmarking. Our rigorous and\ntransparent methods allow clients to confidently use synthetic consumer insights for\nclear business value.\n\nIn this paper, we aim to provide a balanced view on synthetic consumers,\nhighlighting their transformative potential while recognizing their current limitations.\n\n###📖 Read the Full Paper Here\n\n🔗 Download the white paper now\n\nJoin us in redefining market research. Reach out at info@pymc-labs.com or\nsubscribe to our newsletter: https://www.pymc-labs.com/newsletter/\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/cohort-revenue-retention": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nThe Data\nBaseline Retention Model\nBART Retention Model\nRevenue Component\nConclusion\nCohort Revenue & Retention Analysis\n\nMay 14, 2024\n\nBy Juan Orduz\n\nCustomer lifetime value (CLV) is a crucial metric to evaluate and steer long-term success for a product or service. We not only want to optimize the cost per-cost per acquisition (for example, with a media mix model with PyMC-Marketing), but also make sure this investment pays off through long-term revenue generated by users. Many CLV models exist in the literature (for contractual and non-contractual settings). Peter Fader and Bruce Hardie pioneered many of them, especially for models specified through a transparent probabilistic data-generating process driven by the user behavior mechanism. Currently, PyMC-Marketing 's CLV module provides the best Python production-ready implementations of these models. Many of these models allow user-level external time-invariant covariates (e.g. acquisition channel) to explain the variance of the data better.\n\nIn strategic business situations, we often need to look beyond user-level CLV and focus on a higher granularity: a cohort. These cohorts can be defined by registration or by first-time date. There are several reasons for working at the cohort level. For one, new privacy regulations such as GDPR might prohibit using sensitive individual-level data in models. In reality, individual-level predictions are not always necessary for strategic decision-making. On the other hand, from a technical perspective, while we can aggregate individual-level CLV at the cohort level, incorporating time-sensitive factors into these models is not straightforward and requires significant effort.\n\nThese observations led to the development of an alternative top-down modeling approach, where we model all cohorts simultaneously. This simple and flexible approach provides a robust baseline for CLV across the entire user pool. The key idea is to combine two model components in a single model: retention and revenue. We can use this model for inference (say, what drives retention and revenue changes) and forecasting.\n\nThis approach is based on Cohort Revenue & Retention Analysis: A Bayesian Approach.\n\nThe Data#\n\nTo illustrate the approach, we consider a synthetic dataset with the following features:\n\ncohort: month-level cohort tag\nn_users: number of users per cohort\nperiod: observation time\nn_active_users: number of active users (defined by, e.g., if they had made a purchase) for a given cohort at a given period.\nrevenue: revenue generated by a cohort at a given period.\nage: the number of weeks since the cohort started with respect to the latest observation period.\ncohort_age: number of months the cohort has existed.\n\nWe can define the observed retention for a given cohort and period combination as n_active_users / n_users. As this is a synthetic dataset, we have the actual retention values, which we use for model evaluation.\n\nStudying cohorts gives us much more information than working on aggregated data.\n\nLet's look into the retention values as a matrix:\n\nFor example, in the upper left corner, we see the value of 17%; this means that for the 2020-01 cohort, 17% percent of the users were active during 2020-2.\n\nFrom this plot, we see the following characteristics:\n\nThere is a yearly seasonality pattern in the observation period.\n\nThe retention values vary smoothly with both features cohort age (number of months the cohort has existed) and age (the number of weeks since the cohort started with respect to the latest observation period.). We do not see spikes or strong discontinuities.\n\nLet's visualize the retention as time series to better understand the seasonal pattern:\n\nRetention usually peaks twice yearly, with the biggest spike around February. It is also interesting that overall retention increases with the cohort's age.\n\nLet's look now into the revenue matrix:\n\nThe newer cohorts (especially after 2021-5) have a much higher revenue volume. We also see that the seasonal pattern is present but much milder. This revenue matrix becomes much more transparent when looking into the matrix of active users:\n\nAs expected, there is a strong correlation between revenue and the number of active users. We could also understand this relation as a combination of the cohort size and the retention values.\n\nTo start simple, let us first consider the retention model component.\n\nBaseline Retention Model#\n\nWe start with a period-based time split to get training and test sets. We use the former to fit the model and the latter to evaluate the output forecast.\n\nNext, let's think about retention as a metric. First, it is always between zero and one; this provides a hint for the likelihood function to choose. Second, it is a quotient. This fact is essential to understanding the uncertainty around it. For example, for cohort A, you have a retention of 20 % coming from a cohort of size 10. This retention value indicates that two users were active out of these 10. Now consider cohort B, which also has a 20% retention but with one million users. Both cohorts have retention, but intuitively, you are more confident about this value for the bigger cohort. This observation hints that we should not model retention directly but rather the number of active users. We can then see retention as a *latent variable *.\n\nAfter this digression about retention, a natural model structure is the following:\n\nA binomial likelihood to model the number of active users as a function of cohort size and retention.\nUse a link function to model the retention as a linear model in terms of the age, cohort age, and seasonality component (plus an interaction term).\n\nThis model is good enough for a baseline (see A Simple Cohort Retention Analysis in PyMC). The key observation is that all of the features (age, cohort age, and seasonality) are also known in the test set! Hence, we can use this model to generate forecasts for any forecasting window. In addition, we can easily add covariates to the regression component (for forecasting purposes, one has to have the covariates available in the future as well). Here, domain knowledge and feature engineering should work together to build a meaningful and actionable model.\n\nBART Retention Model#\n\nFor this synthetic data set, the model above works well. Nevertheless, disentangling the relationship between features in many real applications is not straightforward and often requires an iterative feature engineering process. An alternative is to use a model that can reveal these non-trivial feature interactions for us while keeping the model interpretable. Given the vast success in machine learning applications, a natural candidate is tree ensembles. Their Bayesian version is known as BART (Bayesian Additive Regression Trees), a non-parametric model consisting of a sum of  regression trees.\n\nOne of the reason BART is Bayesian is the use of priors over the regression trees. The priors are defined in such a way that they favor shallow trees with leaf values close to zero. A key idea is that a single BART-tree is not very good at fitting the data but when we sum many of these trees we get a good and flexible approximation.\n\nLuckily, a PyMC version is available PyMC BART, which we can use to replace the linear model component from the baseline model above. Concretely, the BART model enters the equation as\n\nHere,  denotes the design matrix of covariates, which includes age, cohort age, and seasonality features.\n\nThis is how the model looks in PyMC:\n\nwith pm.Model(coords={\"feature\": features}) as model:\n    # --- Data ---\n    model.add_coord(name=\"obs\", values=train_obs_idx, mutable=True)\n    x = pm.MutableData(name=\"x\", value=x_train, dims=(\"obs\", \"feature\"))\n    n_users = pm.MutableData(name=\"n_users\", value=train_n_users, dims=\"obs\")\n    n_active_users = pm.MutableData(\n        name=\"n_active_users\", value=train_n_active_users, dims=\"obs\"\n    )\n\n    # --- Parametrization ---\n    # The BART component models the image of the retention rate under\n    # the logit transform so that the range is not constrained to [0, 1].\n    mu = pmb.BART(\n        name=\"mu\",\n        X=x,\n        Y=train_retention_logit,\n        m=100,\n        response=\"mix\",\n        dims=\"obs\",\n    )\n    # We use the inverse logit transform to get the retention\n    # rate back into [0, 1].\n    p = pm.Deterministic(name=\"p\", var=pm.math.invlogit(mu), dims=\"obs\")\n    # We add a small epsilon to avoid numerical issues.\n    p = pt.switch(pt.eq(p, 0), eps, p)\n    p = pt.switch(pt.eq(p, 1), 1 - eps, p)\n\n    # --- Likelihood ---\n    pm.Binomial(name=\"likelihood\", n=n_users, p=p, observed=n_active_users, dims=\"obs\")\n\n\nFor more details, see the complete blog post Cohort Retention Analysis with BART.\n\nRemark: The response=\"mix\" option in the BART class allows us to combine two ways of generating prediction from the trees: (1) taking the mean and (2) using a linear model. Having a linear model component on the leaves will allow us to generate better out-of-sample predictions.\n\nLet us see the in-sample and out-of-sample prediction for some subset of cohorts:\n\nIn-sample\n\nOut-of-sample\n\nHere are some remarks about the results:\n\nAs expected from the modeling approach, the credible intervals are wider for the smaller (in this example, the younger) cohorts.\nThe out-of-sample predictions are good! The model can capture the trends and seasonal components.\nAs we use age and cohort age as features, we expect closer cohorts to behave similarly (this is something we saw in the exploratory data analysis part above). In particular, we can generate forecasts for very young cohorts with very little data!\n\nThe PyMC BART implementation provides excellent tools to interpret how the model generates predictions. One of them is partial dependence plots (PDP). These plots provide a way of understanding how a feature affects the predictions by averaging the marginal effect over the whole data set. Let us take a look at how these look for this example:\n\nHere, we can visualize how each feature affects the model's retention predictions. We see that cohort age has a higher effect on retention than age, and we also see a clear yearly seasonal pattern. Moreover, we can additionally use individual conditional expectation (ICE) plots to detect interactions across features.\n\nRevenue Component#\n\nThe BART model performs very well on the retention component. Still, our ultimate goal is to predict future cohort-level value measured in money (not just retention metrics). Hence, it is crucial to add the monetary value to the analysis. Fortunately, the Bayesian framework allows enough flexibility to do this in a single model! Let's think about the revenue variable:\n\nIt has to be non-negative.\n\nIn general, we expect bigger cohorts to generate more revenue.\n\nThe more active users, the more the revenue.\n\nTherefore, a natural metric to compare cohorts evenly is the average revenue per user.\n\nGiven these observations, it is reasonable to model the revenue component as:\n\nwhere  represents the average revenue per user. Observe that the mean of this distribution is .\n\nWe are now free to choose to model the parameter . For this specific example, we see that the strong seasonality in retention mainly drives the seasonal component in the revenue component. Thus, we do not include seasonal features from the  model. A linear model in the cohort and cohort age features (plus an interaction term) provides a good baseline for many applications. As  is always positive, we need to use a link function to use a liner model:\n\nThe retention and revenue models coupled by the variable  through the latent variables  (retention) and  (average revenue per user) is what we call the cohort-revenue-retention model.\n\nHere is a diagram of the model structure:\n\nThe out-of-sample predictions for the revenue component are pretty good as well:\n\nNote that the revenue predictions have a seasonal component, which comes from the  through the retention . In addition, similarly, as for the retention predictions, the credible intervals are wider for the smaller (in this case, younger) cohorts, and we can provide very good predictions for very young cohorts as, in some sense, we are pooling information across cohorts through the model.\n\nWe can also get insights into the average revenue per user and active user behavior as a function of the cohort features.\n\nConclusion#\n\nIn this blog post, we described how to leverage Bayesian methods to develop a solid baseline for a customer lifetime value at the cohort level by coupling retention and revenue components. This model is flexible, interpretable, and provides good forecasts (demonstrated in a synthetic dataset and from experience from real case applications). Moreover, it is a model on which we can build upon of. We can:\n\nSeamlessly add more features to the BART model. We need to ensure they are available in the future if we want to do forecasting.\n\nWe can split the data and consider cohort matrices per acquisition channel. Then, we could combine this with media mix model results to have a better estimate (it is not perfect because the user-level channel assignment had to be through an attribution model) of CAC / CLV (i.e. cost-per-acquisition / customer lifetime value).\n\nInclude more features in the revenue component. We can control them via a linear model or use more complex methods, such as Gaussian processes or even neural networks. See, for example, the blog post Cohort Revenue Retention Analysis with Flax and NumPyro where we use a neural network instead of a BART model.\n\nThe model parametrization is \"not that important\" aspect of the model. What is key is the coupling mechanism driving the CLV: retention and average revenue per user.\n\nAnother exciting application of this model is causal analysis. For example, assume we ran a global marketing campaign to increase the order rate (for example). Suppose we can not do an A/B test and have to rely on quasi-experimental methods like \"causal impact\". In that case, we might underestimate the campaign's effect if we aggregate all the cohorts together. An alternative is to use this cohort model to estimate the \"what-if-we-had-no-campaign\" counterfactual at the cohort level. It might be that newer cohorts were more sensitive to the campaign than older ones, and this could explain why we are not seeing a significant effect on the aggregation.\n\nThere are still many things to discover and explore about these cohort-level models. For example, a hierarchical model across markets on which we pool information across many of these retention and revenue matrices. This approach would allow us to tackle the cold-start problem when we do not have historical data. Stay tuned!\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/bayesian-inference-at-scale-running-ab-tests-with-millions-of-observations": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nA default (slow) PyMC implementation\nNo-indexing PyMC model\nWhy do these models get slow?\nWhat can we do?\nGrid approximation\nThe histogram approximation\nA super-fast PyMC implementation\nAny loss of inferential precision?\nLimitations\nWhat about 100 million observations?\nEnabling A/B tests at scale for our client\nBayesian inference at scale: Running A/B tests with millions of observations\n\nAugust 12, 2022\n\nBy Benjamin Vincent\n\nIndustry data scientists are increasingly making the shift over to using Bayesian methods. However, one often cited reason for avoiding this is because “Bayesian methods are slow.” There can be some truth to this, although this is often a non-issue unless working at serious scale. This is where we come in…\n\nWe previously wrote about how we achieved a 60x speedup on an A/B testing pipeline for HelloFresh- taking a process that took 5-6 hours to 5-6 minutes. In that particular case, the pipeline involved running analysis on many hundreds of A/B tests overnight.\n\nA different problem arises however when each individual A/B test has a very large number of observations. This blog post covers how we worked with a different client (a very large video streaming service) to get a proof-of-concept A/B test pipeline working at serious scale.\n\nA/B tests can be applied to test for differences in conversion rates (i.e. differences in proportions). But A/B tests are also often run on outcomes that are not proportions - for simplicity this blog post will use an example with normally distributed data that could represent metrics such as the number of clicks or time spent on site. In practice, the distribution of outcome variables can diverge from the normal distribution and thus be less suitable.\n\nA default (slow) PyMC implementation#\n\nFirst, we’ll build a simple PyMC A/B test model for continuous data and see how long it takes to run as a function of the number of observations. For this example, we won’t care about model details, like creating priors over effect sizes, or analysing the posterior distributions over model parameters. Mathematically, we can describe the model as:\n\nwhere  is a vector of group A/B memberships (with values 0 or 1), and  represents the observation number. We can code this up as a PyMC model like this:\n\nwith pm.Model() as slow_model:\n    mu = pm.Normal(\"mu\", mu=0, sigma=1, shape=2)\n    sigma = pm.HalfNormal(\"sigma\", 1, shape=2)\n    pm.Normal(\"outcome\", \n              mu=mu[df['group'].values], \n              sigma=sigma[df['group'].values],\n              observed=df['outcome'].values)\n\n\nAnd we can time how long the sampling takes when applied to datasets of various sizes.\n\nTotal sampling time as a function of total observations in the A/B test, shown on log-log scales. All timings in this post were conducted on a regular 2009 iMac with an 8-core Intel i9 CPU with 40Gb of RAM.\n\nWe can see that total compute times are not so bad when the number of observations are low. But things start to get quite slow when we have ~1 million observations. If we had 10 million, or 100 million observations, then this approach would not be practical.\n\nNo-indexing PyMC model#\n\nOne trick we could explore is to subtly change the model to remove the indexing which can often be a source of model slowness. In this model, we extract the A and B group observations into separate vectors and now define two separate likelihoods, one for each group.\n\nA_outcome = df['outcome'][df['group']==0].values\nB_outcome = df['outcome'][df['group']==1].values\nwith pm.Model() as no_indexing_model:\n    mu = pm.Normal(\"mu\", mu=0, sigma=1, shape=2)\n    sigma = pm.HalfNormal(\"sigma\", 1, shape=2)\n    pm.Normal(\"A\", mu=mu[0], sigma=sigma[0], observed=A_outcome)\n    pm.Normal(\"B\", mu=mu[1], sigma=sigma[1], observed=B_outcome)\n\n\nRunning our timing tests again shows that just by avoiding indexing, we can get over 3x speedups when we have about 1 million observations.\n\nWhile this is cool, we can do even better than this!\n\nWhy do these models get slow?#\n\nIn the models above, we are evaluating the likelihood of the data given some parameters , and we are doing this at each step in the MCMC chain. If we have 100 observations (i.e. ), then we are evaluating the likelihood  100 times each MCMC step. If there are 1,000,000 observations then we will evaluate the likelihood 1,000,000 times each MCMC step. (Note: This is an approximation and depends on the particular MCMC sampling algorithm being used.)\n\nThis implies sampling time will be some linear function of the number of observations and number of MCMC steps. So if we hold the number of MCMC samples constant then we might expect the sampling time to scale linearly with the number of observations. Although in practice, we can see that this is not quite the case.\n\nWe are of course also evaluating the prior  at each MCMC step, but the number of parameters is constant (and low), so would not be expected to contribute much to the evaluation time when there are many observations.\n\nAny single observation (point) belongs to one of the groups A or B and these are colour coded blue and red. For regular models, each observation (point) is evaluated. This is demonstrated schematically by the lines tracing up from the outcome value to the probability density given a set of parameters $(\\mu_A, \\sigma_A)$ as represented by the blue normal distribution, and $(\\mu_B, \\sigma_B)$ as represented by the red normal distribution. The overall likelihood is then given by the sum of these values. Here we have 200 total observations, which involves 200 likelihood evaluations at each step in the MCMC chain.\nWhat can we do?#\n\nWe must find a way to reduce the number of evaluations. There are at least two possibilities worth considering - we can either bin the parameter space (grid approximation) or we can bin the observations space.\n\nGrid approximation#\n\nGrid approximation would involve avoiding MCMC sampling altogether and directly evaluating the log posterior using grid approximation. However, this approach is only feasible with any degree of accuracy for only a few parameters. While we only have 4 in our case (2 ’s and 2 ’s), this is not a feasible solution.\n\nThe histogram approximation#\n\nRather than binning the parameter space, we can bin the observation space. If we have 1 million observations, perhaps we could approximate the true posterior by binning all the data up into 500 small bins for example and evaluating the likelihood only 500 times rather than 1 million. That would be 2,000 times less work!\n\nThe approximation will become more accurate the more bins we have. As the number of bins gets very high, then it would become equivalent to treating the observations as continuous. So there will be a trade-off between number of bins and accuracy (more bins = more computation = more accuracy).\n\nSo we could keep our MCMC sampling approach, asking for 1,000 samples (for example) and at each step of the MCMC chain we potentially have 2,000 times less to compute.\n\nWhat is even better about this is that we could scale up to 10,000,000 observations, discretise into 500 bins, and we still only have to evaluate the likelihood 500 times. So the computation time with this approach would scale with the number of bins, not the number of observations we have.\n\nWith the histogram approximation, we group the observations into bins (bottom plot). We can now approximate the likelihood by evaluating the likelihood of bin centre given the parameters $(\\mu_A, \\sigma_A)$ as represented by the blue normal distribution (top), and $(\\mu_B, \\sigma_B)$ as represented by the red normal distribution (top). This now involves only evaluating the likelihood at the bin centres, regardless of how many observations there are. Note: the likelihood at each bin centre is multiplied by the number of observations in that bin.\nA super-fast PyMC implementation#\n\nWe can implement this in PyMC code by evaluating the logp of a normal distribution with a given set of parameters just at these set of bin centers. We can add the log probability of the particular mean and std parameters given the data (bin centers and bin counts) to the PyMC model using pm.Potential:\n\npm.Potential(\"A\", pm.logp(pm.Normal.dist(mu[0], sigma[0]), A_bin_centres) * A_counts)\npm.Potential(\"B\", pm.logp(pm.Normal.dist(mu[1], sigma[1]), B_bin_centres) * B_counts)\n\n\nSo for each group, we are evaluating the log probability (with pm.logp) of the normal likelihood distribution with the relevant  and  parameters at the centre of the bins. We then multiply this by the number of observations within each bin. So this is just like we are finely discretising the observations and evaluating the likelihood for each observation. But because we know many of the (binned) observations are identical, we can simply evaluate one at each bin center and multiply by the number of observations in that bin. Note: A_bin_centres, B_bin_centres, A_counts, and B_counts are all vectors, the number of elements equal to the number of bins.\n\ndef bin_it(df, n_bins):\n    \"\"\"Bin the observations, returning the bin counts and bin centres for A and B data\"\"\"\n    edges = np.linspace(np.floor(df.outcome.min()), np.ceil(df.outcome.max()), n_bins)\n    \n    A_counts, A_bin_edges = np.histogram(df.query(\"group == 0\")[\"outcome\"], edges)\n    A_bin_widths = A_bin_edges[:-1] - A_bin_edges[1:]\n    A_bin_centres = A_bin_edges[:-1] - A_bin_widths/2\n    \n    B_counts, B_bin_edges = np.histogram(df.query(\"group == 1\")[\"outcome\"], edges)\n    B_bin_widths = B_bin_edges[:-1] - B_bin_edges[1:]\n    B_bin_centres = B_bin_edges[:-1] - B_bin_widths/2\n    return A_counts, A_bin_centres, B_counts, B_bin_centres\n\nA_counts, A_bin_centres, B_counts, B_bin_centres = bin_it(df, n_bins)\n\n\nThe final model ends up being:\n\nwith pm.Model() as fast_model:\n    mu = pm.Normal(\"mu\", mu=0, sigma=1, shape=2)    \n    sigma = pm.HalfNormal(\"sigma\", 1, shape=2)\n    pm.Potential(\"A\", pm.logp(pm.Normal.dist(mu[0], sigma[0]), A_bin_centres) * A_counts)\n    pm.Potential(\"B\", pm.logp(pm.Normal.dist(mu[1], sigma[1]), B_bin_centres) * B_counts)\n\n\nHow does this model compare, in terms of sampling time, to the other models?\n\nAs predicted, the sampling time for the fast model is constant (because it is a function of the number of bins) at around 30 seconds. This results in really meaningful speedups. Not only can we run A/B tests incredibly quickly, but it now becomes possible to run A/B tests on datasets at real scale, with 10’s or 100’s of millions of observations. This would simply not have been practical before.\n\nFor anyone wanting to try this out for themselves, we've added some new functionality to the pymc-experimental repo which adds a new histogram_approximation distribution. This reduces the number of manual steps a user has to implement. So go and check out this (merged) pull request for some concrete examples of how to use it.\n\nAny loss of inferential precision?#\n\nIt’s all very well producing speed-ups, but this is only useful if the precision of our inferences are not unduly decreased. To get a flavour of this, we ran a series of tests with simulated data, varying the true uplift from 0.1, 0.2, ..., 1.0. For each simulated dataset, we ran the regular AB test model as well as the fast model and plotted the posterior over the uplift in the scatter plot below. If the models come to different conclusions, then the posteriors for the slow and fast model will lie off the line of equality. But as we can see, over a range of true uplifts the inferences are highly similar, lying on the line of equality.\n\nNotably, we did this for 500,000 total observations, a sample size large enough to represent a meaningful difference in the speed of the models. The slow (no indexing model) took on average 75 seconds whereas the fast model took an average of 13 seconds. Of course, it is possible to probe the precision of the inferences even further (by changing the number of bins for example), but this gives a good flavour that we can achieve significant speed-ups without any meaningful loss of inferential precision.\n\nLimitations#\n\nThere are however some limitations to this approach as we’ve presented it. The implementation we have presented can extend to multiple groups (e.g. A/B/C/D tests). But this model would not extend to continuous predictor variables, so we don’t claim to have revolutionized all of Bayesian inference! But this is a neat approximation that we can use in this situation to make Bayesian inference very fast even with very large numbers of observations.\n\nIn order to focus on the core approach used to speed up A/B tests this post has explored a relatively simple, but still useful, A/B testing approach. It does, out of necessity, ignore a number of subtleties (such as non-normally distributed observations) which are important to attend to in real-world analysis situations.\n\nWhat about 100 million observations?#\n\nLet’s really see what we can do here… To be able to claim that we can really do Bayesian A/B tests at scale, we ran our model on 100 million observations. The MCMC sampling time was a mere 22 seconds on a modest iMac, and only about 30 seconds when taking model compilation time into account!\n\nI think we’ve convincingly shown that simple A/B, or A/B/C, or A/B/C/D tests can run with seriously large numbers of observations, on modest personal computing equipment, and be done before you even make it to the coffee machine.\n\nEnabling A/B tests at scale for our client#\n\nA/B tests are a staple for many organisations which make data-driven decisions. But each company and testing situation is different. Our client approached us with the particular ‘problem’ of having too much data. Because of the number of users and timescale of the tests, we are talking in the order of 1-10 million observations per test. As our timing tests have shown, this can initially pose a major challenge to the extent that Bayesian A/B tests are simply not feasible at major scale.\n\nThrough working with our client, we found a way forward, enabling A/B tests to be run at the kind of scale that our client operates at. This now means that analysis of A/B test results can get the advantage of Bayesian interpretations - such as being able to base decisions upon full posteriors (e.g. 95% credible intervals, the Region Of Practical Equivalence, or Bayes Factors) rather than point estimates alone, or p-values.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/marketing-mix-modeling-a-complete-guide": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nKey Takeaways:\nWhat is Marketing Mix Modelling\nWhat are The Core Components of Marketing Mix Modeling?\nThe Basic MMM Equation\nBeyond the Basic Equation: Why Reality Isn’t Linear\nAdstock\nSaturation: Modeling Diminishing Returns\nControl Variables: Factors Outside Marketing\nPutting It All Together, The Full MMM Model\nBase vs. Incremental Sales\nFrom Structure to Estimation: How Do We Find the Parameters?\nThe Frequentist Approach\nWhy Bayesian Methods?\nWhat Marketing Mix Modeling Can (and Can’t) Do\n…and this is where PyMC-Marketing steps in\nQuick setup: channels, controls, transforms\nIncorporate domain knowledge with priors easily\nModel fitting made easy\nPrior-predictive check: validate assumptions\nModel Diagnostics\nPosterior predictive: forecasts with uncertainty\nContribution decomposition\nScenario curves and “what ifs”\nROAS with uncertainty\nOut-of-sample forecasts\nWhat other open-source MMM tools exist?\nWhich open-source MMM library is the most popular today?\nWhich MMM library should you use? (Our Recommendation)\nWhere is MMM Used in the Real World?\nHelloFresh: Scaling Bayesian MMM from Prototype to Production\nFrom Prototype to Production Challenges\nThe Technical Transformation\nBusiness Translation is Key\nHow Bolt built smarter budgeting with PyMC-Marketing\nThe Future of Marketing Mix Models: From Measurement to Decision-Making\nMaking MMM Faster with AI\nExtending MMM Thinking Beyond Media\nClosing the Loop Between Product and Marketing\nFAQs on Marketing Mix Modeling (MMM)\nConclusion\nMarketing Mix Modeling : A Complete guide\n\nSeptember 22, 2025\n\nBy Sangam Swadi K\n\nEvery business wants to know where growth is really coming from. The challenge is untangling marketing activity from everything else that shapes sales. In this guide, we break down the basics, show why Bayesian methods make Marketing Mix Modeling (MMM) more reliable, and share how companies are using it in practice.\n\nKey Takeaways:#\n\nMMM lets you separate “base” sales from the lift created by marketing, so you can see what’s actually working.\n\nReal campaigns aren’t linear: adstock captures the lag of an ad, saturation shows diminishing returns, and controls explain outside bumps like seasonality.\n\nBayesian methods bring more honesty to the results, you see the uncertainty, and you can fold in prior knowledge instead of guessing.\n\nPyMC-Marketing makes all this easier with ready-made tools for modeling, ROAS, forecasting, and budget planning.\n\nIn head-to-head tests, PyMC-Marketing came out faster, more accurate, and more scalable than Google’s Meridian.\n\nCompanies like HelloFresh and Bolt have already put Bayesian MMM into production, using it for real budget decisions.\n\nThe next wave of MMM uses AI agents and synthetic consumer panels, turning it from a backward-looking report into a real-time decision tool.\n\nWhat is Marketing Mix Modelling#\n\nMarketing Mix Modeling (MMM) is a statistical method used to understand how different factors like advertising, promotions, pricing, and seasonality affect sales. MMM looks at past data and uses regression models to measure the impact of each channel.\n\nWhat's really made these models more reliable is the adoption of Bayesian methods. As Dr. Thomas Wiecki discussed in his PyData talk on Bayesian Marketing Science, this approach provides a much clearer picture of uncertainty and allows you to incorporate prior business knowledge into your models. We'll explore the technical aspects of this approach in detail later.\n\nAt its core, MMM works by decomposing your total sales like this:\n\nTotal Sales = Base Sales + TV Effect + Digital Ads Effect + Promotions Effect + Other Factors\n\nThis means you can put a number next to each channel and say things like:\n\nTV drove $4.2M in sales\nDigital drove $2.9M\nPromotions drove $1.5M\nThe rest came from seasonality or other unexplained factors\n\nIn practice, MMM pulls apart your sales to show the incremental impact of each channel.It tells you what’s really working, what isn’t working, and where your spend delivers the most value.\n\nFor a deeper understanding of MMM and how Bayesian methods are applied in modern marketing analytics, take a look at our webinar on Bayesian Methods in Modern Marketing Analytics. The session features Thomas Wiecki, Luca Fiaschi, and Alex Andorra, who share real-world lessons and challenges from applying these models in practice.\n\nWhat are The Core Components of Marketing Mix Modeling?#\nThe Basic MMM Equation#\n\nWhen working with MMMs, we usually start with historical data that tracks sales alongside different marketing and non-marketing factors over time. Here’s a simple example:\n\n\nWeek\tSales\tTV Spend\tDigital Spend\tPrice\tSeasonality\n1\t$9M\t$1M\t$0.5M\t$2.49\tLow Season\n2\t$10M\t$1.5M\t$0.6M\t$2.49\tHigh Season\n\n\n\nAt the core of Marketing Mix Modeling is a regression equation that links sales with marketing activities and other factors. The simplest version looks like this:\n\nHere's what each part means:\n\nSalest : Sales in week t\nTVt, Digitalt, Pricet, Seasonalityt : The independent variables, your inputs that can influence sales\n1, 2, 3, 4:: Coefficients that measure the effect of each input on sales\n0: Base sales, i.e., what you would expect to sell even without marketing\nt: Error term, the part of sales not explained by the model (random noise, unexpected events, etc.)\n\nSo, for example, if 1= 2.5, then every extra $1M spent on TV is associated with roughly $2.5M in additional sales, holding other factors constant.\n\nThis is the building block of MMM. In practice, we rarely stop here because marketing effects are not always linear or immediate. But this regression form gives us a starting point to split sales into base vs. incremental parts, compare channels, and plan budgets.\n\nTo see how regression and Bayesian modeling are applied in practice, take a look at our our talk at PyData on Solving Real-World Business Problems with Bayesian Modeling\n\nBeyond the Basic Equation: Why Reality Isn’t Linear#\n\nThe regression model we just saw is a good starting point because it links sales with different marketing inputs in a clean, linear way. But real marketing effects are rarely that simple.\n\nSpending on a channel doesn’t always translate to sales in a straight line. For example:\n\nIf you spend $0 on TV, you get $0 impact. Fair enough.\nIf you spend $10M, you see a lift in sales.\nBut if you keep pouring money, say $1B on TV, sales don’t keep rising forever. People get saturated, ads lose effectiveness.\n\nTiming also matters. A TV campaign may take weeks to build awareness, while digital ads can trigger responses almost instantly.\n\nThis is also where many MMMs can go wrong: if important factors are left out, the model can give biased estimates of how quickly returns diminish. In our post on Unobserved Confounders, ROAS and Lift Tests in Media Mix Models, we show how using lift tests as a “reality check” helps anchor these models and makes the saturation effect more trustworthy.\n\nTo handle this, MMM models don’t just stop at the basic equation, they add a few extra layers:\n\nAdstock: to capture lag and carryover effects\nSaturation: to model diminishing returns\nNon- linear regression: to better reflect how marketing works in practice\n\nWe’ll dive into adstock and saturation next, but the main point is that MMM needs to reflect the messy, delayed, and diminishing nature of real-world marketing.\n\nAdstock#\n\nSo far, our regression model assumes that the impact of advertising happens only in the week the money is spent. But in reality, that’s not how people behave.\n\nIf you run a TV ad on Sunday, not everyone buys on Monday. Some might buy that week, some a few weeks later, and some may forget altogether. In other words, the effect of an ad spreads over time and fades gradually.\n\nTo capture this, MMM uses a technique called adstock. Adstock creates a new version of your spend variable that accounts for this “carryover” effect. The formula looks like this:\n\nThat means the current effect is this week’s spend plus a fraction of last week, plus a smaller fraction of the week before, and so on.\n\nFor example, suppose you spend $1M on TV in Week 1 and nothing afterward, with λ = 0.8.\n\nWeek\tTV Spend\tAdstock (λ=0.8)\n1\t$1.0M\t$1.0M\n2\t$0\t$0.8M\n3\t$0\t$0.64M\n4\t$0\t$0.51M\nFig: Raw vs Adstocked spend (Adstock (λ=0.8) spreads a one-week spend over time, decaying gradually.)\nFig: Different channels decay at different speeds; λ controls how fast the effect fades. Eg: TV has a high λ (slower fade), while digital ads have a low λ (faster fade)\n\nSince the spend carries over and fades slowly we use the adstocked spend instead of raw spend for TV, Digital and other channels\n\nSaturation: Modeling Diminishing Returns#\n\nOnce we account for lag with adstock, there’s another challenge that is more spend doesn’t always mean proportionally more sales.\n\nEg:\n\nWeek1: Spent $10K and we gained 1000 customers\n\nWeek2: Spent $20K and we gained 1800 customers\n\nWeek3: Spent $30K and we gained 2200 customers\n\nYou can notice here that, as the budget increases the customers gained isn't proportional. This is a classic example of diminishing returns.\n\nIntuitively this makes sense because when you begin spending, the first part of your spend reaches the most responsive audience, and as you keep spending you end up reaching people who are hesitant, less interested, and harder to convince. So now, each dollar spent has less impact than initially.\n\nTo capture this MMM applies a saturation function to the adstocked spend before it enters regression.\n\nBelow are some saturation functions used:\n\nLogarithmic function\nWhat this is saying is, at low spend, the curve grows steeply (high returns), but at high spend, it flattens out (diminishing returns).\nHill function\nHere,  controls steepness (how fast saturation occurs).\n is the half-saturation point (the amount of spend where the channel gives half of its total impact).\nFig: Hill saturation curves with different α values (θ=5). All pass through the half-saturation point (x=5, f(x)=0.5). Small α gives gradual curves, large α gives sharp, switch-like responses.\nFig: Hill saturation curves with different α values (θ=5). All pass through the half-saturation point (x=5, f(x)=0.5). Small α gives gradual curves, large α gives sharp, switch-like responses.\nTanh function\n is the ceiling, or the maximum possible impact the channel can deliver.\n controls the initial efficiency: smaller values make the curve climb faster (more customers per dollar at the start), larger values make it flatter (less efficient).\nFig: Tanh saturation (varying b and c result in change in steepness)\nLogistic function\n controls efficiency. Larger values mean the curve rises faster (more efficient channel), while smaller values mean slower growth.\nThe function has a half-saturation point at approximately , which makes it easy to interpret in business terms: “how much spend gets me halfway to the channel’s total impact.”\nLike  and Hill, the curve is bounded: it rises quickly at first, then flattens smoothly as diminishing returns set in.\nFig: Logistic saturation function variants. Different λ values change the steepness of the curve.\n\nSo now with adstock and saturation, our flow looks like this\n\nIn equation form we have:\n\nwhere  is the saturation function\n\nControl Variables: Factors Outside Marketing#\n\nSo far we’ve looked at two big marketing behaviors, adstock and saturation. But if you think about it, marketing isn’t the only one that is moving sales up or down.\n\nSales can also shift because of non-marketing factors (nothing to do with ad spend) such as:\n\nA holiday like Christmas, creates spikes every year.\nWeather conditions, (eg: ice cream sales spike in summer heat)\nMore shelf space for your product\n\nThese are called control variables. They’re not part of your marketing budget, but they still affect sales. If you leave them out, ads will get credit for changes they didn’t cause.\n\nIn practice, you just add them alongside your media channels to account for normal sales shifts things like seasonality, holidays, or competitor moves and separate those from the real incremental sales.\n\nSo now, our flow looks like this\n\nPutting It All Together, The Full MMM Model#\n\nSo far, we’ve added layers, step by step, regression as the core, adstock to capture lag, saturation for diminishing returns, and control variables for non-marketing effects.\n\nNow let’s see how they fit into one full model.\n\nThe equation looks like this:\n\nWhere:\n\n = spend on channel  at time \n = carryover transformation as discussed\n = saturation function\n = effect of channel  on incremental sales\n = control variables like holidays, weather, more shelf space, etc.\n = effect of those controls\n = error term (unobserved factors and noise)\nFig: Fig: Illustration of the entire workflow\nBase vs. Incremental Sales#\n\nOnce this model is estimated, sales can be broken into two parts:\n\nBase sales: what you’d expect with no marketing spend, driven by controls like price, holiday and trend.\n\nIncremental sales, the extra sales generated by each marketing channel after adstock and saturation.\n\nFrom Structure to Estimation: How Do We Find the Parameters?#\n\nUntil now, we’ve familiarized ourselves with the structure of an MMM. We have understood why we need adstock, saturation and control variables. But understanding and building this structure is just half the story.\n\nThe real challenge now is to find the parameters (the betas (β) , lambdas (λ) etc…)\n\nThis step is called parameter estimation.\n\nFor example, suppose we run our model and want to know: “How much does an extra $1M in TV spend improve sales?” If β (TV) turns out to be 2.5, we’d say that every $1M invested in TV drives roughly $2.5M in incremental sales. That’s the kind of answer estimation gives us.\n\nThere are two main ways to estimate these parameters: Frequentist and Bayesian.\n\nThe Frequentist Approach#\n\nThe classic way is Ordinary Least Squares (OLS) regression. Here’s how it works in plain terms:\n\nYou start with your historical data (sales, ad spend, prices, etc.).\nThe model makes predictions for sales given a set of coefficients (the β’s).\nThe “error” is the difference between predicted sales and actual sales.\nOLS chooses the coefficients that make these errors as small as possible on average.\n\nThis gives you one best number for each parameter.\n\nFrequentist methods are fast but have some important drawbacks:\n\nPoint estimates only: You get one number for each coefficient (β(TV)= 2.5, say), without any sense of how uncertain that number is.\nOverconfidence: Real effects vary across markets, seasons, and contexts. A single number may oversimplify.\nGeneralization risks: Frequentist models can struggle when data is sparse or noisy.\nInability to include domain knowledge: Suppose you already know from past campaigns that digital ads usually decay faster than TV. In this approach you can’t directly encode that knowledge.\n\nBayesian methods are designed to solve these problems.\n\nWhy Bayesian Methods?#\n\nBayesian estimation replaces single “best guess” numbers with distributions. Instead of saying “TV adds exactly $2.5M,” Bayesian MMM might tell you:\n\nmost likely between $2.0M and $3.0M\nbut with some probability it could be lower or higher\n\nThis matters because it quantifies uncertainty rather than hiding it. At the same time, Bayesian methods give you all the strengths of Frequentist approach like clear parameter estimates and interpretability while also solving for the drawbacks. This is exactly what is needed for smarter decision making in MMM.\n\nThis shift matters because it quantifies uncertainty instead of hiding it, while still giving interpretable results. For a gentle intro to how Bayesian models do this, see MCMC Sampling for Dummies. And for a business view of why uncertainty is central, our blog From Uncertainty to Insight explains how Bayesian data science changes decision-making\n\nWhat Marketing Mix Modeling Can (and Can’t) Do#\n\nMMM is a reliable way to measure marketing impact, but it has its limits. It works best when you have a lot of historical data, which means it’s less useful for brand-new channels or sudden shifts in the market. Because MMM looks backward by design, it won’t give you instant answers on a new campaign. And since every model rests on assumptions, leaving out factors like competitor activity or shelf space can skew the results.\n\nMMM also shines at the big picture more than the details. It can show you how TV or digital perform overall, but it won’t tell you if one specific ad drove sales on a Tuesday. And the more advanced the model, the harder it can be to explain outside of the data team, which sometimes slows adoption.\n\nThat’s why MMM works best alongside other tools like lift tests, incrementality experiments, and attribution. Together, they give you both the broad view and the finer detail.\n\n…and this is where PyMC-Marketing steps in#\n\nEverything we just discussed can be a lot to hand-code. PyMC-Marketing gives you a plug-and-play Bayesian MMM: adstock and saturation transforms, priors, MCMC fitting, posterior/HDIs, contributions, ROAS, forecasts, even budget simulations in a few lines.\n\nQuick setup: channels, controls, transforms#\n\nPyMC-Marketing wraps a full Bayesian MMM behind a simple API. You specify channels, controls, and pick your adstock + saturation.\n\nfrom pymc_marketing.mmm import MMM, GeometricAdstock, LogisticSaturation\n\nmmm = MMM(\n    date_column=\"date_week\",\n    channel_columns=[\"tv\", \"digital\"],\n    control_columns=[\"price\", \"event\", \"t\"],\n    adstock=GeometricAdstock(l_max=8),\n    saturation=LogisticSaturation()\n)\n\nIncorporate domain knowledge with priors easily#\n\nYou can add domain knowledge directly to your model with priors supported by pymc-extras. You just pick the priors that are relevant to you and PyMC-Marketing handles the rest.\n\nfrom pymc_extras.prior import Prior\n\nmmm = MMM(..., model_config={\"adstock_alpha\": Prior(\"Beta\", alpha=1, beta=3)})\n\nModel fitting made easy#\n\nModel fitting is one line of code with mmm.fit( ), the package does all the heavy lifting so that you can focus interpreting results and less on debugging.\n\nmmm.fit(X, y, chains=4, target_accept=0.9)\n\nPrior-predictive check: validate assumptions#\n\nYou can test your priors before fitting the model by running a prior-predictive check. This lets you see if the priors you’ve chosen are reasonable or absurd.\n\nmmm.sample_prior_predictive(X, y, samples=2000)\nmmm.plot_prior_predictive(original_scale=True)\n\nModel Diagnostics#\n\nAfter fitting, PyMC-Marketing gives you diagnostics with ArviZ and a visual model graph. The graph helps you understand how your priors interacted with data and is a good way to confirm if your domain knowledge was included or not.\n\nimport arviz as az\n\naz.summary(mmm_fit_result, var_names=[\"adstock_alpha\", \"saturation_lam\"])\nmmm.graphviz()\n\n\n\nAn example of model graph:\n\n\nPosterior predictive: forecasts with uncertainty#\n\nWith PyMC-Marketing, forecasts don’t just give you a single number, they come with uncertainty bands. The result is easy-to-read “what if” scenarios where you see not just the most likely outcome, but also the range of plausible ones\n\nmmm.sample_posterior_predictive(X, extend_idata=True, combined=True)\nmmm.plot_posterior_predictive(original_scale=True)\n\nContribution decomposition#\n\nRemember we talked about figuring out the contribution of individual channels to sales? PyMC-Marketing shows how much each channel contributed to sales by separating baseline factors (trend, seasonality, controls) from channel lift, giving you a clear, business-aligned breakdown.\n\nmmm.plot_components_contributions(original_scale=True)\nfig = mmm.plot_grouped_contribution_breakdown_over_time(...)\n\n\nAn example of contribution breakdown over time:\n\nScenario curves and “what ifs”#\n\nThis feature is built right into PyMC-Marketing: you can quickly simulate what happens to contributions at different spend levels. The package generates curves that reflect adstock (carryover) and saturation (diminishing returns), guided by your priors.\n\nmmm.plot_channel_contribution_grid(start=0, stop=1.5, num=12)\n\n\nAn example of channel contributions at different spend levels:\n\nROAS with uncertainty#\n\nROAS (Return on Ad Spend) isn’t shown as just one number. Instead, you get a range of possible values based on both your data and your priors. This makes the results more realistic especially when spend or data are limited and gives you a clearer picture of how much sales you’re actually getting back for each dollar spent.\n\nchan = mmm.compute_channel_contribution_original_scale()\nspend = X[[\"tv\", \"digital\"]].sum().to_numpy()\nroas_samples = chan.sum(dim=\"date\") / spend[np.newaxis, np.newaxis, :]\n\nOut-of-sample forecasts#\n\nYou can project sales beyond your data. Priors make sure ad effects fade out realistically instead of dropping off suddenly at the forecast horizon.\n\nmmm.sample_posterior_predictive(X_oos, include_last_observations=True)\n\n\nThere are plenty more features too. The library also lets you add seasonality and trends, inspect parameters like adstock or saturation, and see channel shares with uncertainty. You can bring in lift test results to guide the model, use a time-varying baseline when markets shift, and save or reload models to keep things consistent over time.\n\nIn short, PyMC-Marketing gives you the full Bayesian MMM workflow from contributions and ROAS to forecasts and what-if scenarios while keeping priors and domain knowledge at the center.\n\nWhat other open-source MMM tools exist?#\n\nWhile PyMC-Marketing has gained a lot of traction, it's not the only open-source library available for Marketing Mix Modeling. Over the last few years, several major players including Google, Meta, and Uber have released their own approaches.\n\nEach framework takes a slightly different path, some lean on more traditional regression techniques, while others emphasize Bayesian methods and advanced time-series forecasting.\n\nThe choice of tool often comes down to your team's technical expertise, the ecosystem you're already invested in, and how much flexibility you need for customization.\n\nHere's a side-by-side comparison of the most popular options.\n\n(Last updated: 2025-08-07)\n*Meridian has been released as successor of Lightweight-MMM, which has been deprecated by Google\nFeature\tPyMC-Marketing\tRobyn\tOrbit KTR\tMeridian*\nLanguage\tPython\tR\tPython\tPython\nApproach\tBayesian\tTraditional ML\tBayesian\tBayesian\nFoundation\tPyMC\t–\tSTAN/Pyro\tTensorFlow Probability\nCompany\tPyMC Labs\tMeta\tUber\tGoogle\nOpen source\t✅\t✅\t✅\t✅\nModel Building\t🛠 Build\t🛠 Build\t🛠 Build\t🛠 Build\nOut-of-Sample Forecast\t✅\t❌\t✅\t❌\nBudget Optimizer\t✅\t✅\t❌\t✅\nTime-Varying Intercept\t✅\t❌\t✅\t✅\nTime-Varying Coeffs\t✅\t❌\t✅\t❌\nCustom Priors\t✅\t❌\t❌\t✅\nCustom Model Terms\t✅\t❌\t❌\t❌\nLift-Test Calibration\t✅\t✅\t❌\t✅\nGeographic Modeling\t✅\t❌\t❌\t✅\nUnit-Tested\t✅\t❌\t✅\t✅\nMLFlow Integration\t✅\t❌\t❌\t✅\nGPU Sampling Accel.\t✅\t–\t❌\t✅\nConsulting Support\tProvided by Authors\tThird-party agency\tThird-party agency\tThird-party agency\n\nIf you are interested in a head-to-head benchmark between PyMC-Marketing and Google’s Meridian, check out our deep-dive blog PyMC-Marketing vs. Meridian: A Quantitative Comparison of Open Source MMM Libraries. The study shows PyMC-Marketing to be faster (2–20x), more accurate, and more scalable, thanks to its flexible sampler options.\n\nWhich open-source MMM library is the most popular today?#\n\nMost open-source MMM libraries are built on Bayesian foundations, but they differ in how flexible they are, the tech stack they run on, and how they’re implemented.\n\nPyMC-Marketing has emerged as the most popular option by far, leading PyPI downloads (see chart below). It offers the widest feature set and the greatest flexibility, making it well suited for teams that need a customizable, state-of-the-art solution. The trade-off is that it’s also the most sophisticated, which can mean a steeper learning curve.\n\nOther libraries fill different needs. Google Meridian provides a more opinionated API and integrates tightly with the Google ecosystem, which is useful for advertisers already working heavily in Google Ads. Meta Robyn, meanwhile, takes a more traditional regression approach and is a strong fit for teams using R that want a faster, simpler setup.\n\nYour choice ultimately depends on:\n\nYour team’s technical expertise\nYour core advertising channels\nWhether you prefer an independent open-source solution or one backed by an ad network.\nWhich MMM library should you use? (Our Recommendation)#\n\nThe best tool depends on your team’s skills, your ad ecosystem, and how much flexibility you need. Here’s how we see it:\n\nPyMC-Marketing\n\nIdeal if you need maximum flexibility for complex, unique business requirements.\nSupports advanced Bayesian modeling (including Gaussian Processes) and full customization.\nProduction-ready with integration into data science workflows (MLflow, Python ecosystem).\nYou prefer independence from major ad publishers and networks\nOptional consulting support from the authors.\n\nGoogle Meridian\n\nYou want a simplified API (but less flexible) to build models across geographies.\nDirect integration with the Google advertising ecosystem is important\nYou want strong integration with other Google products such as Colab\n\nRobyn (Meta)\n\nIf your team works in R instead of Python.\nYou prefer a simpler but less rigorous approach than Bayesian Models (Ridge regression)\nYou want direct integration with Meta/Facebook advertising data.\nWhere is MMM Used in the Real World?#\n\nOver the years, we've helped companies across different industries implement Bayesian MMM at scale. Here's how these projects actually played out in practice.\n\nHelloFresh: Scaling Bayesian MMM from Prototype to Production#\n\nWhen HelloFresh first began experimenting with Bayesian Marketing Mix Models, it was less a polished system and more a late-night experiment. The team had come across Google’s MMM paper and translated the original Stan implementation into PyMC. After cross-checking the model with company data, the results quickly built credibility.\n\nOne early recommendation was to increase TV spend. The marketing team was skeptical, as TV felt expensive compared to digital channels. But they took the risk, and the following quarter turned out to be one of their best.\n\nFrom Prototype to Production Challenges#\n\nThe original model ran for hours. They needed time-varying effects to capture how COVID impacted different channels. Some digital channels became super expensive, others became cheap as advertisers either pulled back or concentrated everything into specific media.\n\nAround this time, HelloFresh's work had caught attention at PyMCon 2020, where their MMM presentation was one of the most popular talks. That's how the collaboration with PyMC Labs started\n\nThe Technical Transformation#\n\nTogether, HelloFresh and PyMC Labs tackled the core challenges. Through re-parameterization and optimization, runtimes were cut down from hours to just minutes. Gaussian Processes were introduced so channel effects could vary over time, and a hierarchical structure allowed related channels like Facebook and Instagram to share strength.\n\nMost importantly, the insights were translated into usable tools: visual reports and a web-based simulator where marketers could play out \"what if\" budget scenarios and see the predicted impact on conversions.\n\nBusiness Translation is Key#\n\nThe collaboration wasn’t just technical. The models were translated into insights and tools that marketing and finance teams could actually use. This combination of technical innovation and business translation is what made the project successful.\n\nHelloFresh now runs Bayesian MMM in production, with marketers and finance using it directly in planning cycles. The work also fed improvements back into the open-source PyMC ecosystem, so the innovations benefit the broader community beyond HelloFresh.\n\nHow Bolt built smarter budgeting with PyMC-Marketing#\n\nBolt faced a familiar challenge: attribution models were no longer enough. With privacy changes and fragmented channels, they needed a way to understand the true incremental value of their marketing investments across both digital and offline media. That's where PyMC-Marketing became a core part of their in-house analytics toolkit.\n\nUsing the library's Bayesian MMM framework, Bolt's team built models that accounted for adstock (carry-over effects) and saturation (diminishing returns), giving them a realistic view of how spend in each channel translated to outcomes. They didn't stop at modeling though. Every result was validated against experiments and quasi-experiments such as Google's CausalImpact, so the MMM stayed grounded in reality.\n\nWith calibrated models in place, Bolt could do more than just measure. They contrasted response curves across channels, identified plateau points where additional spend no longer drove growth, and used these insights to design automated budget allocation strategies. In practice, this meant their system could adapt to different business goals - sometimes prioritizing market share growth at any cost, other times optimizing for profitability or efficiency.\n\nThe impact was twofold. First, marketing decisions became faster and more data-driven, with resource allocation tailored to each campaign's objectives. Second, the collaboration strengthened the open-source ecosystem: Bolt contributed back by opening a pull request to add their budget allocation approach to PyMC-Marketing, ensuring other teams could benefit from the same advances.\n\nAs Bolt summarized in their write-up:\n\nPyMC has earned its place among Bolt's treasured toolkits, thanks to the malleability it offers in crafting models perfectly suited to our needs. Along the same vein, PyMC-Marketing offers a compelling edge in devising flexible yet standard MMMs, serving both as a springboard and a muse for our analyses.\n\nThe Future of Marketing Mix Models: From Measurement to Decision-Making#\n\nMarketing Mix Modeling has always carried a reputation for being slow and a bit academic, something that happened in the back office long after campaigns were over.\n\nThat’s changing quickly. The next wave of MMM isn’t about dashboards that summarize the past; it’s about tools that shape decisions in real time.\n\nMaking MMM Faster with AI#\n\nAnyone who’s built an MMM knows the bottlenecks: endless data prep, debates about adstock curves, and nights chasing down model diagnostics. The modeling itself isn’t the biggest challenge, it is everything around it.\n\nThat’s exactly what the AI MMM Agent is built to solve. Acting as a co-pilot, it automates the critical but repetitive steps: cleaning data, configuring the right Bayesian model, running diagnostics, and translating outputs into business-ready insights. What used to take months can now happen in a day.\n\nThe result isn’t just speed (though speed matters in fast-moving markets). It’s accessible. MMM shifts from being a once or twice a year project into an ongoing decision-support tool that marketers and finance teams can use in real time.\n\nExtending MMM Thinking Beyond Media#\n\nThe second shift we’re seeing is MMM-style thinking applied outside of media planning altogether. With our AI Innovation Lab, we take the same approach but aim it upstream at product development.\n\nHere, synthetic consumer panels test ideas like packaging, pricing, or messaging before launch. It’s the same logic,but instead of asking “does TV drive sales?” the question becomes “does this new design actually win over buyers?”\n\nWe’ve validated synthetic panels against real consumer responses, showing up to 90% accuracy in categories like oral care. Combined with AI agents that simulate expert review for feasibility and compliance, the Lab gives companies a way to pressure-test products and campaigns weeks before they hit shelves.\n\nClosing the Loop Between Product and Marketing#\n\nThe real step change comes when these tools connect. MMM tells you which channels and creatives work; the Innovation Lab tells you which product concepts resonate. Put them together, and you get a closed-loop system where product and marketing no longer operate in silos.\n\nThis means teams can align on questions that cut across functions: What should we launch? How should we price it? And how should we market it? Instead of chasing separate answers, they can share evidence, iterate faster, and act with confidence.\n\nBreaking down the wall between consumer research and marketing planning doesn’t just make processes faster; it makes decisions smarter. It gives brands a way to build products and campaigns that reinforce each other, turning MMM from a rearview tool into part of the innovation engine itself.\n\nFAQs on Marketing Mix Modeling (MMM)#\n\nQ: How much historical data is needed for MMM to be effective?\nWe recommend at least one to two years of consistent sales and spend data. This helps the model distinguish true marketing impact from recurring patterns such as seasonality or external events.\n\nQ: Can MMM cover both online and offline channels?\nYes. MMM can integrate digital, TV, radio, print, and in-store activities into one framework, providing a single view of how all channels contribute to sales.\n\nQ: How frequently should MMM models be updated?\nMany organizations refresh their models quarterly, while others update more often in fast-changing markets. The right cadence depends on your industry, data availability, and planning cycles.\n\nQ: Does MMM replace experiments and attribution models?\nNo. MMM provides a broad, long-term view of channel effectiveness, while experiments and attribution add granular, short-term evidence. Used together, they provide a more complete understanding of marketing performance.\n\nQ: Do you offer consulting or expert support for MMM?\nYes. At PyMC Labs, we work directly with teams to design, implement, and scale Bayesian MMMs. This includes everything from setting up models with PyMC-Marketing to customizing them for unique business needs. If you’re interested in collaborating or want to explore how Bayesian MMM could work for your organization, feel free to contact us for more details.\n\nConclusion#\n\nMarketing Mix Modeling is becoming an essential way to see what really drives sales when tracking every click isn’t possible. Using Bayesian methods with open-source tools makes it easier to measure the impact of each channel and to be upfront about the uncertainty in the results.\n\nAt PyMC Labs, we’ve helped teams move from early tests to MMM systems that guide real budget decisions. That shift shows MMM is no longer just about reporting on the past. It’s about helping companies plan ahead with evidence they can trust.\n\nAs marketing gets more complex and decisions need to be made faster, MMM is turning into a practical tool for action. With the right data and setup, it can connect marketing and finance and give businesses the clarity they need to spend smarter.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/simulating-data-with-pymc": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nTaking draws from simple distributions\nAll PyMC distributions are vectorized\nMeta-Distributions\nMultiple variables\nWhat if I don't know what I want to sample?\nTechnical advice\nConcluding remarks\nSimulating data with PyMC\n\nApril 20, 2023\n\nBy Ricardo Vieira And Tomás Capretto\n\nImage from Wikipedia\n\nPyMC provids a great API for defining statistical models. When paired with its sampling algorithms, it becomes the ideal tool for conducting reliable and robust Bayesian inference.\n\nStill, Bayesian inference is far from its only use case. PyMC models specify a highly structured data-generating process that can be very useful on its own. Applications include simulation for optimization routines, risk analysis, and research design, among many others.\n\nPyMC comes with many user-friendly builtin distributions and meta-distributions which are cumbersome to write from scratch with NumPy or SciPy routines. These include Mixtures, Timeseries, Multivariate parametrizations, Censored and Truncated distributions, and pretty much anything you would would ever need when doing data simulation.\n\nIn this blog post we will give you a flavor for some of these, and show how we use them as part of a data modelling workflow!\n\nTaking draws from simple distributions#\nimport pymc as pm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nSciPy has a lot of distributions, but they are often difficult to work with, due to their focus on loc-scale parametrizations.\n\nPyMC tends to pick more intuitive parametrizations (and often offers multiple options).\n\nFor instance, in PyMC you can define a Gamma distribution using the shape/rate parametrization (which we call alpha and beta), and then take draws with the draw function.\n\nx = pm.Gamma.dist(alpha=2, beta=1)\nx_draws = pm.draw(x, draws=1000, random_seed=1)\nsns.histplot(x_draws);\n\n\nOr, perhaps more intuitively, using the mean/standard deviation parametrization (called mu and sigma).\n\nx = pm.Gamma.dist(mu=2, sigma=1)\nx_draws = pm.draw(x, draws=1000, random_seed=2)\nsns.histplot(x_draws);\n\n\nPyMC takes care of converting between equivalent parametrizations for the user.\n\nAll PyMC distributions are vectorized#\n\nNot all SciPy distributions allow NumPy-like broadcasting for their parameters. PyMC distributions always do!\n\nimport scipy.stats as st\n\ntry:\n    st.dirichlet([[1, 5, 100], [100, 5, 1]]).rvs()\nexcept ValueError as err:\n    print(err)\n\nParameter vector 'a' must be one dimensional, but a.shape = (2, 3).\n\n\nAh well...\n\nx = pm.Dirichlet.dist([[1, 5, 100], [100, 5, 1]])\npm.draw(x, random_seed=3)\n\narray([[0.00173804, 0.0396383 , 0.95862366],\n       [0.94999365, 0.04176138, 0.00824496]])\n\nMeta-Distributions#\n\nNeither NumPy nor SciPy offer a pre-built truncated LogNormal distribution (last time I checked). They do offer a Truncated Normal, and you could exponentiate those to obtain Truncated LogNormal draws. But what if you wanted to sample some other truncated distribution?\n\nPyMC can truncate any (pure univariate) distribution you throw at it, via the Truncated class.\n\nx = pm.Truncated.dist(pm.Lognormal.dist(0, 1), upper=3)\nx_draws = pm.draw(x, draws=10_000, random_seed=4)\nsns.histplot(x_draws);\n\n\nOr you can sample from Mixtures, using Mixture.\n\nHere we sample from a Mixture of two Normals, with weights [0.3, 0.7], meaning that 30% of the draws will come from the first component and 70% from the second (on average).\n\nx = pm.Mixture.dist(\n    w=[0.3, 0.7], \n    comp_dists=[\n        pm.Normal.dist(-1, 1), \n        pm.Normal.dist(1, 0.5),\n    ],\n)\nx_draws = pm.draw(x, draws=10_000, random_seed=5)\nsns.histplot(x_draws);\n\n\nOr Random walks... with mixture initial distributions? Sure.\n\ninit_dist = pm.Mixture.dist(\n    w=[0.3, 0.7], \n    comp_dists=[\n        # Why? Because we can!\n        pm.Beta.dist(1, 1), \n        pm.Normal.dist(100, 0.5),\n    ]\n)\n\nx = pm.RandomWalk.dist(\n    init_dist=init_dist,\n    innovation_dist=pm.StudentT.dist(nu=4, mu=0, sigma=1),\n    steps=1000,\n)\n\nx_draws = pm.draw(x, draws=5, random_seed=6)\nfor x_draw in x_draws:\n    plt.plot(x_draw)\nplt.xlabel(\"t\");\n\n\nMultiple variables#\n\nYou can also draw multiple non-independent variables easily.\n\nIn this example we first sample a categorical index variable, which is then used to select an entry from a vector of 3 Normals with means [-100, 0, 100], respectively.\n\nWe can retrieve both the index and selected Normal draws via draw.\n\nidx = pm.Categorical.dist(p=[.1, .3, .6])\nx = pm.Normal.dist(mu=[-100, 0, 100], sigma=1)[idx]\nidx_draws, x_draws = pm.draw([idx, x], draws=5, random_seed=7)\nidx_draws, x_draws\n\n(array([2, 0, 1, 1, 0]),\n array([  99.56070737,  -97.86364272,   -1.91664666,   -0.51318227,\n        -101.0769092 ]))\n\n\nHere we first sample a Poisson variable. This value then determines how many draws to take from a Gamma variable that is finally summed.\n\nn_events = pm.Poisson.dist(5)\nemissions = pm.Gamma.dist(mu=10, sigma=2, shape=n_events)\npm.draw([n_events, emissions.sum()], draws=3, random_seed=8)\n\n[array([ 4, 10,  5]), array([36.44587888, 99.99521681, 57.83917302])]\n\nWhat if I don't know what I want to sample?#\n\nSometimes you have a rough idea of what the data looks like, but you don't know how to specify it.\n\nThis could happen in a parameter recovery study, where you want to simulate covariates that are somewhat \"realistic\". Maybe their marginals match an observed dataset or they show some level of covariance. But you may not know exactly what parameters to use.\n\nThe good news is that PyMC can also do inference (that's its main goal after all)!\n\nLet's say we have the following marginal data, and we want to simulate something that behaves roughly like it.\n\ndf = sns.load_dataset(\"diamonds\")\n\nfg = sns.displot(data=df, x=\"price\", col=\"cut\", facet_kws=dict(sharey=False), height=3.5, aspect=0.85);\nfor ax in fg.axes.ravel():\n    ax.tick_params(axis=\"both\", labelsize=11)\n\n\nLooks like a positive distribution, maybe multimodal. Perhaps a LogNormal mixture?\n\nWhat about the parameters for the components, what should be the mixture weigths? Let's make PyMC infer them!\n\nimport pandas as pd\nimport numpy as np\n\ncut_idxs, cut_labels = pd.factorize(df[\"cut\"])\ncut_idxs, cut_labels\n\n(array([0, 1, 2, ..., 3, 1, 0]),\n CategoricalIndex(['Ideal', 'Premium', 'Good', 'Very Good', 'Fair'], categories=['Ideal', 'Premium', 'Very Good', 'Good', 'Fair'], ordered=False, dtype='category'))\n\n\nLet's assume the price for each of the 5 cuts can be modeled by a mixture of 3 LogNormal distributions. That will be 5 * 3 = 15 means and 15 standard deviations. We will also need 15 mixture weights. Sounds like many parameters, but we have much more data.\n\nf\"rows={df.shape[0]}\"\n\n'rows=53940'\n\n\nLet's create a PyMC model, with vague priors and fit the most likely parameter combination that could have generated the data.\n\ncoords = {\n    \"cut\": cut_labels.codes,\n    \"components\": (0, 1, 2),\n    \"obs\": range(len(df)),\n}\nwith pm.Model(coords=coords) as m:\n    # Priors for the weights, means and standard deviations\n    mix_weights = pm.Dirichlet(\"mix_weights\", np.ones((5, 3)), dims=(\"cut\", \"components\"))\n    mix_means = pm.Normal(\"mix_means\", mu=[7, 8, 9], sigma=3, dims=(\"cut\", \"components\"))\n    mix_stds = pm.HalfNormal(\"mix_stds\", sigma=2, dims=(\"cut\", \"components\"))\n\n    # Distribution of the data\n    # We use numpy advanced indexing to broadcast the 5 mixtures parameters \n    # and weights into the long form shape of the data\n    price = pm.Mixture(\n        \"price\",\n        w=mix_weights[cut_idxs],\n        # You can pass a single distribution to Mixture,\n        # in which case the last dimensions correspond to the mixture components.\n        comp_dists=pm.LogNormal.dist(mu=mix_means[cut_idxs], sigma=mix_stds[cut_idxs]),\n        observed=df[\"price\"],\n        dims=\"obs\",\n    )\n\n\nThat's a bit complicated... but not too bad once you write a couple of PyMC models. PyMC comes with a whole set of utilities to help you define complex statistical models.\n\nIn the example above we used coords, to specify the shape of our parameters with human-readable labels.\n\nWe can also request a graphical representation of our model:\n\npm.model_to_graphviz(m)\n\n\nLooks about right. And here are the most likely parameters according to find_MAP:\n\nwith m:\n    fit = pm.find_MAP(include_transformed=False)\nfit\n\n 100.00% [236/236 00:06<00:00 logp = -4.9015e+05, ||grad|| = 7.9382]\n{'mix_means': array([[6.66074039, 7.75610166, 9.05369536],\n        [6.72625732, 8.39101105, 9.55073404],\n        [6.43443448, 8.29902563, 7.11347194],\n        [6.47404918, 8.18681784, 9.52971883],\n        [6.91160121, 8.16400639, 9.50047371]]),\n 'mix_weights': array([[0.35500674, 0.43518759, 0.20980567],\n        [0.32025714, 0.59956718, 0.08017567],\n        [0.21574635, 0.73892949, 0.04532416],\n        [0.26608238, 0.68359   , 0.05032761],\n        [0.13718139, 0.78682301, 0.0759956 ]]),\n 'mix_stds': array([[0.29090793, 0.58885309, 0.42603866],\n        [0.32267874, 0.61074076, 0.17280056],\n        [0.26029962, 0.68052945, 0.09724909],\n        [0.28147658, 0.69736908, 0.18833131],\n        [0.35619133, 0.52964347, 0.19005135]])}\n\n\nNow that we have a set of parameter values, we can take draws from the distribution of interest. Hopefully it will resemble our data.\n\ncut_labels.codes\n\narray([0, 1, 3, 2, 4], dtype=int8)\n\ncut_idxs = cut_labels.codes\nprice = pm.Mixture.dist(\n    w=fit[\"mix_weights\"][cut_idxs],\n    comp_dists=pm.LogNormal.dist(\n        mu=fit[\"mix_means\"][cut_idxs], \n        sigma=fit[\"mix_stds\"][cut_idxs]\n    ),\n)\n# Each draw returns one price for each cut category\npm.draw(price, random_seed=9)\n\narray([ 681.86652441, 1559.814205  , 3460.20197464, 3291.09521261,\n       2400.54237695])\n\ndraws = pm.draw(price, draws=10_000, random_seed=10)\n\n+ Show Code\nfig, ax = plt.subplots(2, 5, figsize=(16, 6), sharex=\"col\")\nfor col in range(5):\n    sns.histplot(data=df.query(f\"cut=='{cut_labels[col]}'\"), x=\"price\", binwidth=500, element=\"step\", ax=ax[0, col])\n    sns.histplot(data=draws[:, col], binwidth=500, element=\"step\", ax=ax[1, col], color=\"C1\")\n\n    if col == 0:\n        ax[0, 0].set_ylabel(\"Observed data\")\n        ax[1, 0].set_ylabel(\"Simulated data\");\n    else:\n        ax[0, col].set_ylabel(\"\")\n        ax[1, col].set_ylabel(\"\") \n    ax[0, col].set_title(f\"cut={cut_labels[col]}\")\n    ax[1, col].set_xlabel(\"price\")\n\nfor axi in ax.ravel():\n    axi.tick_params(axis=\"both\", labelsize=11);\n\n\nThe marginal simulated histograms do resemble those from the original dataset.\n\nAnother way of checking their similarity is to look at the empirical CDF. The two lines should look alike for distributions that are similar.\n\n+ Show Code\nr = sns.displot(data=df, x=\"price\", col=\"cut\", kind=\"ecdf\", log_scale=True, height=3.5, aspect=0.85)\nfor i in range(5):\n    sns.ecdfplot(draws[:, i], ax=r.axes[0, i], color=\"C1\", lw=2)\n\n\nLooks close enough!\n\nIf it didn't, we could go back to the Model and try something else. Maybe more components, or different distribution families...\n\nBut careful, if you do this enough times, you may end up as a data modelling practitioner!\n\nTechnical advice#\n\nIn the examples above we used the handy draw function. Under the hood, this function creates a compiled function that takes draws from your specified variables, seeds it, and then calls it multiple times in a Python loop.\n\nIf you are writing performance-critical code, you should avoid calling draw in a loop, as it will recompile the same function every time it is called. Instead you can compile the underlying random function directly with compile_pymc and reuse it whenever needed.\n\nThe internals of draw are pretty straightforward.\n\nfrom pymc.pytensorf import compile_pymc\n\nx = pm.Normal.dist()\nfn = compile_pymc(inputs=[], outputs=x, random_seed=11)\nfn(), fn()\n\n(array(-0.10568235), array(-0.6541993))\n\n\nSecondly, if you need to take many draws from the same distribution, it's better to define it with the final shape and call the function only once. In the examples above we never did this!\n\nThis way, the random number generation can be vectorized.\n\nx = pm.Normal.dist(shape=(2,))\nfn = compile_pymc(inputs=[], outputs=x, random_seed=11)\nfn()\n\narray([-0.10568235, -0.6541993 ])\n\n\nTo better understand PyMC shapes, check out this page.\n\nConcluding remarks#\n\nIn this blog post we showed how PyMC can be used as a powerful replacement for NumPy and SciPy when writing structured random generation routines.\n\nWe also hinted at how the same code can be reused for both simulation and inference in the last example. If you go a bit further, you can start doing predictions based on your estimated statistical model. This is, in a nutshell, what model based data science is all about!\n\nIf you are a data scientist, doing data simulation, we hope you give PyMC a try!\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/spatial-gaussian-process-01": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nBeyond naive hierarchical models\nOur dataset\nBut first, let's look at some maps\nGaussian processes on a sphere\nCustom PyMC covariance kernel on a sphere\nGaussian process geospatial model\nWrap up\nAcknowledgements\nReferences\nModeling spatial data with Gaussian processes in PyMC\n\nAugust 17, 2022\n\nBy Thomas Wiecki\n\nBeyond naive hierarchical models#\n\nSo many times I've found myself having to work with geospatial data. It's everywhere: from advertisement, to product inventories, to electoral polls. While I was learning Bayesian modeling, I used to think about geographical information as some categorical value that grouped some observations together. The nice thing about this approach is that observations coming from the exact same geographical area will share a common feature, and this feature will be used (somehow) to explain the similarities between both observations. The bad thing about this approach is that observations from neighboring geographical areas are assumed to have absolutely nothing in common. Sounds weird, right? Usually we would picture some kind of continuous latent geographical feature that makes observations taken from nearby places be similar to each other. Surely there must be a better way of modeling geospatial data!\n\nCompletely independently from me modeling geospatial data, I learned how to work with Gaussian processes (GPs). GPs provide a very nice and flexible way of setting a prior that essentially says: \"nearby observations should be similar to each other, and as the observations go further away, they become uncorrelated\". This really clicked with what I wanted to do with geospatial data! The only problem was that there weren't many sources targeting general audiences that explained how to use GPs on geospatial data. That's why I wanted to put together a small example that showcases how you can use GPs with geospatial data using PyMC.\n\nOur dataset#\n\nWe will be revisiting a classic: the radon dataset, from Gelman and Hill 2006 (if you haven't read the PyMC case study yet, you really should go ahead and do that now, it's an excellent resource!). Just to give you a quick refresher, Gelman and Hill studied a dataset of radon measurements that were performed in 919 households from 85 counties of the state of Minnesota. The case study cited above, focuses on how to use the county information to group households together, and then try to estimate a state-wide expected radon level, and the expected radon for each observed county. This is exactly the same as my old grouping approach to geospatial data!\n\nThe grouping approach comes with the crucial drawback I mentioned before: measurements from neighboring counties are uncorrelated from each other. This makes no sense! Why should the radon concentration in Earth's crust follow some county boundary line?! It makes much more sense to imagine that radon concentration varies continuously on the surface of the Earth, and then simply use the household locations to estimate it. This would actually give us the information of the radon we expect to measure in neighboring states or countries! Let's see how we can do this using PyMC!\n\nBut first, let's look at some maps#\n\nWe can't start our modeling without first looking at our data on a map. So we'll do a small detour on how I used cartopy to plot the radon dataset.\n\nimport arviz as az\nimport cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nimport cartopy.io.shapereader as shpreader\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nfrom aesara import tensor as at\nfrom matplotlib import pyplot as plt\n\n\nCartopy can be used to get shape files from some public sources, like Natural Earth. One of these are the shape files for counties in the United States. The county shapes have a lot of meta information. One important field is called FIPS, that stands for Federal Information Processing System. At the time the radon measurements were performed, the counties were identified using their FIPS codes, and we will use these to align our observations to the corresponding shape files.\n\n# Load the radon dataset and add the US FIPS prefix\ndf = pd.read_csv(pm.get_data(\"radon.csv\"), index_col=0, dtype={\"fips\": int})\ndf[\"fips\"] = \"US\" + df[\"fips\"].astype(\"string\")\ncounty_idx, counties = df.county.factorize(sort=True)\nunique_fips = df.fips.unique()\n\n# Get the state of Minnesota shapefile\nreader = shpreader.Reader(\n    shpreader.natural_earth(\n        resolution=\"10m\", category=\"cultural\", name=\"admin_1_states_provinces\"\n    )\n)\nminnesota = [\n    s\n    for s in reader.records()\n    if s.attributes[\"admin\"] == \"United States of America\"\n    and s.attributes[\"name\"] == \"Minnesota\"\n][0]\n\n# Get Minnesota counties and neighboring counties shape files\nreader = shpreader.Reader(\n    shpreader.natural_earth(\n        resolution=\"10m\", category=\"cultural\", name=\"admin_2_counties\"\n    )\n)\nminnesota_counties = [\n    county\n    for county in reader.records()\n    if county.geometry.intersects(minnesota.geometry)\n    and county.geometry.difference(minnesota.geometry).area / county.geometry.area\n    < 0.01\n]\nminnesota_neighbor_counties = [\n    county\n    for county in reader.records()\n    if county.geometry.intersects(minnesota.geometry)\n    and county.geometry.difference(minnesota.geometry).area / county.geometry.area > 0.5\n]\ncounties_with_measurements = [\n    c for c in minnesota_counties if c.attributes[\"FIPS\"] in unique_fips\n]\ncounties_without_measurements = [\n    c for c in minnesota_counties if c.attributes[\"FIPS\"] not in unique_fips\n]\nlen(counties_with_measurements), len(counties_without_measurements), len(\n    minnesota_neighbor_counties\n)\n\n(85, 2, 37)\n\n\nNow that we have loaded the dataset and extracted all the necessary shape files, we notice something interesting. The dataset has measurements for 85 out of 87 counties from Minnesota. We will use our model to predict the expected level of radon that we should see in the remaining 2 counties and also on the 37 neighboring counties from the neighboring states. Let's have a look at where these counties are on a map:\n\nThe green counties are the ones where we have at least one household measurement. The red ones are where we don't have measurements. The blue dots are points that are within the county. Since the radon dataset doesn't have the precise coordinates of each household (I imagine for privacy reasons), we will use the coordinates of the blue dots to impute the county households locations.\n\nNow let's look at the average radon measurement for each county. To view this, we first need to use a dictionary that maps from the county measured in the radon dataframe to the shapefile record for said county.\n\n# Get a mapping from county names to latitude/longitude\n# and another mapping from county names to shapefile records for plotting\ncounty_fips = {counties[idx]: df.fips.iloc[i] for i, idx in enumerate(county_idx)}\nfips_to_records = {\n    record.attributes[\"FIPS\"]: record for record in counties_with_measurements\n}\ncounty_to_records = {c: fips_to_records[county_fips[c]] for c in counties}\ncounty_lonlat = {\n    c: np.array(\n        [\n            county_to_records[c].attributes[\"longitude\"],\n            county_to_records[c].attributes[\"latitude\"],\n        ]\n    )\n    for c in counties\n}\ncond_counties = [\n    c.attributes[\"NAME\"].upper() for c in counties_without_measurements\n] + [\n    f\"{c.attributes['NAME']} - {c.attributes['REGION']}\".upper()\n    for c in minnesota_neighbor_counties\n]\ncounty_to_records.update(\n    {\n        name: record\n        for name, record in zip(\n            cond_counties, counties_without_measurements + minnesota_neighbor_counties\n        )\n    }\n)\ncond_county_lonlat = {\n    c: np.array(\n        [\n            county_to_records[c].attributes[\"longitude\"],\n            county_to_records[c].attributes[\"latitude\"],\n        ]\n    )\n    for c in cond_counties\n}\n\nfig = plt.figure(figsize=(12, 7))\nprojection = ccrs.PlateCarree()\nax = plt.axes(projection=projection)\nax.add_feature(\n    cfeature.ShapelyFeature([minnesota.geometry], projection),\n    edgecolor=\"k\",\n    facecolor=\"w\",\n)\nexpected_radon = df.groupby(\"county\")[\"log_radon\"].mean()\nvmin = expected_radon.min()\nvmax = expected_radon.max()\ncolor_getter = lambda x: plt.get_cmap(\"viridis\")(np.interp(x, [vmin, vmax], [0, 1]))\nfor county in counties:\n    county_record = county_to_records[county]\n    val = expected_radon[county]\n    ax.add_feature(\n        cfeature.ShapelyFeature([county_record.geometry], projection),\n        edgecolor=\"gray\",\n        facecolor=color_getter(val.data),\n    )\n\ncbar = fig.colorbar(plt.matplotlib.cm.ScalarMappable(norm=None, cmap=\"viridis\"))\ncbar.set_ticks(np.linspace(0, 1, 6))\ncbar.set_ticklabels(\n    [f\"{round(np.interp(x, [0, 1], [vmin, vmax]), 2)}\" for x in cbar.get_ticks()]\n)\ncbar.set_label(\"Observed mean Log Radon\")\nax.add_feature(cfeature.LAKES, alpha=0.5)\nax.add_feature(cfeature.RIVERS)\nax.set_xlim([-99, -87])\nax.set_ylim([42, 50])\nax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False);\n\n\nWe can see a small pattern forming here. It looks like radon is denser to the south west, and sparser in the north east. We're almost ready to take this data to the next level!\n\nGaussian processes on a sphere#\n\nEven though the above plot seems flat, Earth is round (kind of):\n\nAnd this makes it a bit harder for us to define Gaussian processes. Thankfully, there is a very nice review paper by Jeong, Jun and Genton that talks exactly about how one could write down covariance functions for Gaussian process that occur on the surface of a sphere. We will basically follow one of their simplest approaches: use a Matérn kernel that relies on the chordal distance between points.\n\nWait, the what distance?!\n\nThe chordal distance simply is the distance between two points on the surface of the Earth if you could travel between them using a straight line that went through the planet. On the other hand, the Geodesic distance is the shortest distance between two points if one was forced to travel on the surface of the globe.\n\nCustom PyMC covariance kernel on a sphere#\n\nPyMC comes with many covariance kernels, but they all assume that the distance between two points is the Euclidean distance. Since we will be working with longitude and latitude pairs, we will need to write a custom Covariance subclass that operates using the metric we actually want - the chordal distance between points on a sphere.\n\nTo do this, we basically need to inherit from pymc.gp.cov.Stationary and overload the full method. This method computes the covariance between a set of points. To do this, we will copy the implementation from pymc.gp.cov.Matern32 but change the distance function used. Instead of using the assumed euclidean_dist, we will write a custom method, the chordal_dist. To compute the chordal distance, we only need to convert the longitude/latitude coordinates into their 3-D position counterparts. The chordal distance then simply is the Euclidean distance between the 3-D position coordinates.\n\nclass Matern32Chordal(pm.gp.cov.Stationary):\n    def __init__(self, input_dims, ls, r=6378.137, active_dims=None):\n        if input_dims != 2:\n            raise ValueError(\"Chordal distance is only defined on 2 dimensions\")\n        super().__init__(input_dims, ls=ls, active_dims=active_dims)\n        self.r = r\n\n    def lonlat2xyz(self, lonlat):\n        lonlat = np.deg2rad(lonlat)\n        return self.r * at.stack(\n            [\n                at.cos(lonlat[..., 0]) * at.cos(lonlat[..., 1]),\n                at.sin(lonlat[..., 0]) * at.cos(lonlat[..., 1]),\n                at.sin(lonlat[..., 1]),\n            ],\n            axis=-1,\n        )\n\n    def chordal_dist(self, X, Xs=None):\n        if Xs is None:\n            Xs = X\n        X, Xs = at.broadcast_arrays(\n            self.lonlat2xyz(X[..., :, None, :]), self.lonlat2xyz(Xs[..., None, :, :])\n        )\n        return at.sqrt(at.sum(((X - Xs) / self.ls) ** 2, axis=-1) + 1e-12)\n\n    def full(self, X, Xs=None):\n        X, Xs = self._slice(X, Xs)\n        r = self.chordal_dist(X, Xs)\n        return (1.0 + np.sqrt(3.0) * r) * at.exp(-np.sqrt(3.0) * r)\n\nGaussian process geospatial model#\n\nIt's been a long road up until now, but we are finally ready to write our PyMC model of the geospatial dataset! Let's go step by step, and start by simply creating the model instance and populating it with some data.\n\nX = np.array(list(county_lonlat.values()))\nwith pm.Model(\n    coords={\n        \"county\": counties,\n        \"household\": df.index,\n        \"feature\": [\"longitude\", \"latitude\"],\n    }\n) as m:\n    _county_idx = pm.ConstantData(\"county_idx\", county_idx, dims=\"household\")\n    _X = pm.ConstantData(\"county_X\", X, dims=(\"county\", \"feature\"))\n\n\nNow, we will basically assume that the observed radon comes from a state level mean plus some deviation due to the county. The county deviation will be represented by a GP. Let's first create the state level random variables.\n\nwith m:\n    state_mu = pm.Normal(\"state_mu\", 0, 3)\n    state_scale = pm.HalfNormal(\"state_scale\", 0.7)\n\n\nWe create the GP prior for the county deviations. We will try to infer the length scale and a priori assume that it's around 200 km with a standard deviation of about 50 km.\n\nwith m:\n    ls = pm.Gamma(\"ls\", mu=200, sigma=50)\n    latent = pm.gp.Latent(cov_func=Matern32Chordal(2, ls),)\n    county_eps = latent.prior(\"county_eps\", _X, dims=\"county\", jitter=1e-7)\n\n\nWe piece everything together and compute the expected radon measurement of each household:\n\nwith m:\n    county_mu = pm.Deterministic(\n        \"county_mu\", state_mu + state_scale * county_eps, dims=\"county\"\n    )\n    household_mu = county_mu[_county_idx]\n\n\nNow, the final piece of the puzzle: the observational distribution. We will model the log radon values\n\nwith m:\n    measurement_error = pm.HalfNormal(\"measurement_error\", 0.5)\n    observed = pm.Normal(\n        \"log_radon\",\n        mu=household_mu,\n        sigma=measurement_error,\n        observed=df.log_radon.values,\n        dims=\"household\",\n    )\npm.model_to_graphviz(m)\n\n\nIf you are familiar with the radon dataset you will have noted that we haven't included any information about whether the radon was recorded at the basement or not. We could exploit this extra information to make the model better. We decided to go with the simplest geospatial model posible to begin with, but if you want more, just ping us on Twitter @pymc_labs, and we'll write up a follow up blog post on how you could add this extra piece of information to the model.\n\nNow let's sample from the model and see what the model learns about the measured counties:\n\nwith m:\n    idata = pm.sample(\n        init=\"jitter+adapt_diag_grad\", random_seed=42, chains=4, target_accept=0.9\n    )\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag_grad...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [state_mu, state_scale, ls, county_eps_rotated_, measurement_error]\n\n 100.00% [8000/8000 05:31<00:00 Sampling 4 chains, 0 divergences]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 332 seconds.\n\naxs = az.plot_trace(idata, var_names=[\"~county_mu\", \"~county_eps\"], filter_vars=\"like\")\naxs[0,0].figure.tight_layout();\n\n\nFinally, the most important piece, extrapolating what we learned to unobserved counties! To do this, we will have to create conditional GP's on the unobserved coordinates, and then create the conditional observed log radon random variables:\n\ncond_X = np.array(list(cond_county_lonlat.values()))\nwith m:\n    m.add_coords({\"cond_county\": cond_counties})\n    _cond_X = pm.ConstantData(\"cond_X\", cond_X, dims=[\"cond_county\", \"feature\"])\n    cond_county_eps = latent.conditional(\n        \"cond_county_eps\", _cond_X, jitter=1e-3, dims=\"cond_county\"\n    )\n    cond_mu = pm.Deterministic(\n        \"cond_county_mu\", state_mu + state_scale * cond_county_eps, dims=\"cond_county\"\n    )\n    observed = pm.Normal(\n        \"cond_log_radon\", mu=cond_mu, sigma=measurement_error, dims=[\"cond_county\"],\n    )\npm.model_to_graphviz(m)\n\n\nAfterwards, predicting on these new counties is as easy as sampling from the posterior predictive distribution.\n\nwith m:\n    pp = pm.sample_posterior_predictive(\n        idata, var_names=[\"cond_county_mu\", \"cond_log_radon\"], random_seed=42\n    )\n\n 100.00% [4000/4000 00:07<00:00]\nfig = plt.figure(figsize=(12, 7))\nprojection = ccrs.PlateCarree()\nax = plt.axes(projection=projection)\nexpected_radon = idata.posterior.county_mu.mean([\"chain\", \"draw\"])\npp_expected_radon = pp.posterior_predictive.cond_county_mu.mean([\"chain\", \"draw\"])\nvmin = min([expected_radon.min().data, pp_expected_radon.min().data])\nvmax = max([expected_radon.max().data, pp_expected_radon.max().data])\ncolor_getter = lambda x: plt.get_cmap(\"viridis\")(np.interp(x, [vmin, vmax], [0, 1]))\nfor county in counties:\n    county_record = county_to_records[county]\n    val = expected_radon.sel(county=county)\n    ax.add_feature(\n        cfeature.ShapelyFeature([county_record.geometry], projection),\n        edgecolor=\"gray\",\n        facecolor=color_getter(val.data),\n    )\nfor county in cond_counties:\n    county_record = county_to_records[county]\n    if county_record in counties_without_measurements:\n        edgecolor = \"red\"\n    else:\n        edgecolor = \"gray\"\n    val = pp_expected_radon.sel(cond_county=county)\n    ax.add_feature(\n        cfeature.ShapelyFeature([county_record.geometry], projection),\n        edgecolor=edgecolor,\n        facecolor=color_getter(val.data),\n    )\n\ncbar = fig.colorbar(plt.matplotlib.cm.ScalarMappable(norm=None, cmap=\"viridis\"))\ncbar.set_ticks(np.linspace(0, 1, 6))\ncbar.set_ticklabels(\n    [f\"{round(np.interp(x, [0, 1], [vmin, vmax]), 2)}\" for x in cbar.get_ticks()]\n)\ncbar.set_label(\"Expected Log Radon\")\nax.add_feature(cfeature.LAKES, alpha=0.5)\nax.add_feature(cfeature.RIVERS)\nax.set_xlim([-99, -87])\nax.set_ylim([42, 50])\nax.add_feature(\n    cfeature.ShapelyFeature([minnesota.geometry], projection),\n    edgecolor=\"k\",\n    facecolor=(1, 1, 1, 0),\n)\nax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False);\n\n\nBeautiful! Our model learned the north-east south-west pattern of radon from the observations! I also takes advantage of the discovered pattern to make predictions of the expected radon in the neighboring counties, and in the two counties without measurements in Minnesota.\n\nAnd, of course, we can also visualize the uncertainty we have on our predicted expected log radon measurements by plotting the standard deviation of the posterior predictive distribution:\n\nThe model is quite certain in its prediction for the Minnesota counties, and the standard deviation of expected radon is small. But as the model is forced to make predictions outside of Minnesota, the standard deviation increases, meaning that the model is less certain about its mean predicted value. We can also see that the model is fairly certain about the predicted value for the two Minnesota counties without measurements, which makes sense, given that we have a bunch of recordings all around them.\n\nWrap up#\n\nThat was quite a ride! We managed to build a Gaussian process prior for the radon dataset, which allowed us to learn a geospatial pattern of expected radon concentration, and to predict measurement concentrations in neighboring, unmeasured counties. To compare this against the grouping approach, which is sometimes called a hierarchical model, we are able to see a more informed extrapolation from observed counties to unobserved counties. The grouping approach, at its core, assumes that a priori all groups are exchangeable. In other words, a naive hierarchical model would only be able to predict that counties to the east and west of Minnesota would have exactly the same expected radon concentration.\n\nAs we mentioned earlier in this post, this model doesn't exploit part of the information present in the dataset. It can be improved to leverage whether a recording was taken in a basement or not, and it could also exploit the measured Uranium similarly to how it's shown in the group level predictors example using the naive hierarchical model. If you're interested in reading a future blog post that shows this, feel free to reach out on twitter.\n\nAcknowledgements#\n\nCover photo by Greg Rosenke on Unsplash\n\nReferences#\nGelman, A., & Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models (Analytical Methods for Social Research). Cambridge: Cambridge University Press. doi:10.1017/CBO9780511790942\nPyMC radon dataset Case study\nJaehong Jeong. Mikyoung Jun. Marc G. Genton. \"Spherical Process Models for Global Spatial Statistics.\" Statist. Sci. 32 (4) 501 - 513, November 2017. https://doi.org/10.1214/17-STS620\nRadon dataset from Gelman & Hill (2006). We only consider the recordings performed in Minnesota (MN) that go from row 5081 to 5999. A file with the relevant subset is available in the pymc-examples repository\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/probabilistic-forecasting": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nClassical Time Series Forecasting Models\nProbabilistic Forecasting Models\nHierarchical Models\nCensored Likelihoods\nAvailability-Constrained TSB Models for Intermittent Series\nCalibration and Custom Likelihoods\nHierarchical Price Elasticity Models\nState Space Models in PyMC\nSummary\nProbabilistic Time Series Analysis: Opportunities and Applications\n\nJune 03, 2025\n\nBy Juan Orduz\n\nOver the past few years, we've witnessed the profound impact of Bayesian modeling on businesses' decisions. In this post, we aim to delve into a specific area where Bayesian methods have demonstrated their transformative potential: probabilistic forecasting models.\n\nForecasting is a critical component of business planning across industries—from retail inventory management to financial market analysis, from energy demand prediction to marketing budget allocation. Traditionally, businesses have relied on point forecasts that provide a single estimate of future values. While these approaches can work reasonably well under stable conditions, they often fail to capture the inherent uncertainty in real-world systems and can lead to suboptimal decisions when confronted with volatile or complex environments.\n\nProbabilistic forecasting addresses these limitations by generating complete probability distributions over possible future outcomes rather than single-point predictions. This paradigm shift provides decision-makers with a more comprehensive view of potential scenarios, enabling robust planning that accounts for risk and uncertainty. With recent advances in computational methods and Bayesian statistics, these sophisticated approaches have become increasingly accessible to practitioners.\n\nIn this post, we'll explore how probabilistic forecasting models can provide a competitive advantage through their ability to incorporate domain expertise, handle data limitations, and model complex relationships. We'll demonstrate these capabilities through several case studies that showcase practical applications across different business contexts.\n\nClassical Time Series Forecasting Models#\n\nIn many business domains, such as logistics, retail, and marketing, we are interested in predicting the future values of one or more time series. Typical examples are KPIs like sales, conversions, orders, and retention. When businesses encounter forecasting challenges, they typically have two main approaches to consider:\n\nUse statistical methods to infer the trend, seasonality, and remainder components. Once we understand these components, we can use them to predict the future. Classical examples that have been widely applied are exponential smoothing and autoregressive models. These models are great baselines and relatively easy to fit. Use external regressors and lagged copies of the time series to fit a machine-learning model, such as linear regression or a gradient-boosted tree model.\n\nDepending on the data and concrete applications, either of these methods can perform very well. In fact, with modern open-source packages like the ones developed by Nixtla, it is straightforward to test and experiment with these models (see statsforecast for statistical models and mlforecast for machine learning forecasting models)).\n\nOf course, forecasting is a well-studied domain (see, for example, Forecasting: Principles and Practice for an introduction), and there are many more approaches and combinations of these methods. For instance, the article M4 Forecasting Competition: Introducing a New Hybrid ES-RNN Model describes a hybrid method between exponential smoothing and recurrent neural networks that outperformed many classical time series models in the M4 competition. Trying to summarize all the models and possibilities in a blog post is nearly impossible. Instead, we want to describe some concrete cases where Bayesian forecasting models can be a competitive advantage for business applications.\n\nProbabilistic Forecasting Models#\n\nGenerally speaking, when discussing probabilistic forecasting models, we often refer to a forecasting model learning parameters of a distribution instead of just the point forecast. For example, we can predict the mean and the standard deviation when using a normal likelihood. This strategy provides great flexibility, as we can better control the uncertainty and model the mean and variance jointly. A well-known example of these models is the DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks developed by Amazon Research.\n\nIn addition, we can go fully Bayesian by setting priors on the model parameters. The blog post Notes on Exponential Smoothing with NumPyro provides an explicit description of how to implement an exponential smoothing model as a fully Bayesian probabilistic forecasting model.\n\nOkay, and so what? What benefits can we get from these types of models? We are glad you asked 😉! Next, we present various case studies where probabilistic forecasting models can significantly improve performance.\n\nHierarchical Models#\n\nOne typical scenario where Bayesian models shine is when we have a hierarchical structure (e.g., category levels, region groupings). In this case, sharing information across related groups can improve model performance by regularizing parameter estimation (the shrinkage effect). For a detailed description of this approach, please see the complete write-up Hierarchical Modeling by Michael Betancourt. In the context of time series forecasting, we can better estimate trend and seasonality components by leveraging the information-sharing nature of hierarchical models.\n\nLet's look at a concrete example to illustrate this point. Consider the quarterly tourism In Australia dataset, where we have tourism volumes between 1998 and 2016 per state, region, and purpose. For illustration, let us consider forecasting these tourism volumes for each state, region, and purpose combination (308 time series). We can take a look at a sample from the state of \"Victoria\" in the following plot:\n\nWhile subtle, we see a positive trend component in most of these sample series.\n\nA natural approach would be to consider each series independently and use a time series model, such as exponential smoothing. This approach is a great baseline, but we can do better! In this example, there is a shared trend component across regions, which might not be evident in the historical data of some smaller areas. Using hierarchical exponential smoothing, we can improve the model performance, particularly for these smaller areas, as illustrated in the following plot:\n\nWe clearly see that in the test period, a mild trend component is inferred from the hierarchical structure (orange and green lines), which would likely not have been captured by a purely univariate time series model (pink line).\n\nThe effort to adapt an exponential smoothing model to a hierarchical version is not as complicated as it might sound. Here you can find all the code and details to reproduce this result (and potentially adapt it for your application 😉).\n\nRemark [Forecasting Baselines]: In this simple case, the hierarchical model performs slightly better than independent AutoETS models fitted to each individual series. This result reflects the general situation where strong baseline models are often hard to beat. So please establish baseline models and robust evaluation processes before implementing more complex models 💡!\n\nCensored Likelihoods#\n\nIn demand forecasting, we often face the challenge of censored data, where observations are only partially known. Censoring occurs when values above (or below) a certain threshold are unobserved or replaced with the threshold value itself. This is particularly relevant in retail, where sales data only captures observed demand when products are in stock. The true, unconstrained demand remains unobserved during stockouts or when inventory capacity limits are reached.\n\nThe simulation study Demand Forecasting with Censored Likelihood demonstrates how Bayesian models with censored likelihoods can provide more accurate demand forecasts compared to traditional time series methods such as ARIMA. By simulating true demand as an AR(2) process with seasonal components and then generating censored sales data reflecting stockouts and capacity constraints, the study shows how traditional forecasting methods systematically underestimate future demand because they treat the observed sales as the complete signal.\n\nLet's examine the simulated data:\n\nThe top plot shows the true underlying demand (black) and the sales that would have occurred without availability constraints (blue).\nThe bottom plot shows the expected sales without constraints (blue) versus the observed sales (orange) when availability constraints impose a maximum sales value (note: this upper bound often varies over time).\n\nUltimately, we want to forecast the unconstrained demand (black curve) while only observing the constrained sales (orange curve). The following plot compares the forecasts from a simple AR(2) seasonal model versus the censored likelihood model:\n\nThe results highlight a significant advantage of censored likelihood models: they accurately model the underlying demand distribution even during periods with stockouts or capacity constraints, effectively \"reconstructing\" what the true demand would have been. This is clearly visible as the predictions (pink) from the censored model are much closer to the true demand target (black) than those from the simple AR(2) model (green).\n\nThis modeling approach leads to forecasts that better capture both the true magnitude and the uncertainty of future demand. It provides critical information for inventory planning, capacity decisions, and revenue optimization that would be missed by conventional forecasting techniques ignoring the censoring mechanism.\n\nAvailability-Constrained TSB Models for Intermittent Series#\n\nIn retail, intermittent time series are the norm rather than the exception. While top-selling products might show regular daily sales patterns, the vast majority of items in a typical retail catalog exhibit sporadic demand - with many days showing zero sales followed by occasional purchases. This pattern is particularly common for niche products, seasonal items, or products with long replacement cycles. Traditional forecasting methods, often designed for continuous demand streams, struggle with these patterns.\n\nFor products with sporadic demand patterns (intermittent time series), the Teunter-Syntetos-Babai (TSB) model is a popular forecasting approach. However, standard TSB models cannot distinguish between true zero demand (no customer interest) and zeros caused by product unavailability (out of stock). The case study Hacking the TSB Model for Intermediate Time Series to Accommodate for Availability Constraints demonstrates how to extend the TSB model to account for availability constraints in intermittent demand forecasting.\n\nThe study, motivated by Ivan Svetunkov's \"Why zeroes happen\" blog post, simulates intermittent time series using a Poisson process combined with a binary availability mask. By incorporating this availability information directly into the model, it prevents the estimated probability of non-zero demand from dropping excessively when zeros are observed due to unavailability rather than a lack of demand. The modified TSB model better preserves the true demand signal when forecasting, as it can distinguish between zero sales caused by unavailability versus actual zero demand. Let's look at a specific time series prediction:\n\nDespite the fact that the last two data points of the training data are zeros, the model does not simply predict zeros for the next time steps, which is a common behavior for standard intermittent time series models. The reason is that the model is aware that these last two zeros are due to the lack of availability and not actual zero demand. Hence, when we set the availability to one for the future forecast period, the forecast is appropriately non-zero. This is exactly what we want 🙌!\n\nThis approach demonstrates the flexibility of probabilistic models to incorporate business constraints directly into the forecasting process. The results show significant improvement in forecast accuracy, particularly for products with limited availability histories, providing more reliable demand estimates for inventory planning and allocation. The study also highlights how these models can be efficiently implemented and scaled to handle thousands of time series simultaneously, making them practical for real-world business applications.\n\nCalibration and Custom Likelihoods#\n\nProbabilistic forecasting models can be further enhanced through parameter calibration using domain knowledge or experimental data. The case study Electricity Demand Forecast with Prior Calibration demonstrates how to incorporate external information to improve a dynamic time-series model for electricity demand forecasting. The core idea is to model electricity demand as a function of temperature:\n\nWhen examining the ratio of electricity demand to temperature, we observe that this ratio is not constant but depends on the temperature itself. Hence, we can use a Gaussian process to model this relationship. The specific model uses a Hilbert Space Gaussian Process (HSGP) to capture the potentially complex, time-varying relationship between temperature and electricity demand.\n\nA key innovation here is the use of a prior calibration process to constrain the estimated temperature effect on demand, particularly for extreme temperatures where historical data might be limited. By incorporating domain expertise on how electricity demand responds to very high temperatures (e.g., above 32°C due to air conditioning load), the model produces more reliable forecasts in these edge cases. The calibration works by adding custom likelihoods that effectively \"inform\" the model about expected parameter values in certain regimes. In this concrete example, we use a Gaussian process to model the temperature-electricity relationship while imposing a constraint on this ratio for high temperatures:\n\nThis estimated effect plot aligns with the observations made in the exploratory data analysis section of Forecasting: Principles and Practice (2nd ed.) by Hyndman and Athanasopoulos:\n\nIt is clear that high demand occurs when temperatures are high due to the effect of air-conditioning. But there is also a heating effect, where demand increases for very low temperatures.\n\nIndeed, our model captures this: at the extremes of the common temperature range, the temperature effect on demand increases. Heating and cooling demand typically increases outside the approximate range of 15°C - 25°C.\n\nThis calibration approach mirrors the methodology used in PyMC Marketing's Media Mix Model with Lift Test Calibration, where experimental results (lift tests) are used to calibrate marketing effectiveness parameters. In both cases, we enhance the forecasting model by incorporating additional likelihood terms that reflect external knowledge—whether that's experimental lift test results, known physical constraints, or other domain expertise. This calibration technique is especially valuable when historical data doesn't fully represent all possible future scenarios or when we suspect the presence of unobserved confounders (see Unobserved Confounders, ROAS and Lift Tests in Media Mix Models).\n\nIt is important to emphasize that these ideas are not new. The simulation example above is inspired by a similar technique from the Pyro tutorial Train a DLM with coefficients priors at various time points. Similarly, extensions of this approach exist for larger time-series architectures like Causal DeepAR developed by Zalando Research.\n\nRemark [Scaling Probabilistic Forecasting Models]: Depending on the size of your dataset, you might consider running full Markov Chain Monte Carlo (MCMC) or opting for faster approximate inference methods like Stochastic Variational Inference (SVI), as described in NumPyro's tutorial Hierarchical Forecasting.\n\nHierarchical Price Elasticity Models#\n\nProbabilistic modeling isn't limited to time series applications; it can also provide powerful solutions for price sensitivity analysis. The case study Hierarchical Pricing Elasticity Models demonstrates how Bayesian hierarchical models can significantly improve price elasticity estimates in retail settings, especially where data may be sparse at the individual product level.\n\nUsing a retail dataset with over 5,000 SKUs across nearly 200 categories, the study compares three approaches to modeling the constant elasticity demand function :\n\nA simple model where each SKU has independent elasticity estimates.\nA fixed-effects model accounting for date effects.\nA hierarchical model that incorporates the product category structure.\n\nThe hierarchical approach shows distinct advantages by leveraging the multilevel structure inherent in retail data. When individual SKUs have limited price variation or sparse sales data, the hierarchical model \"borrows strength\" from similar products within the same category, producing more stable and economically sensible elasticity estimates.\n\nA key insight from this study is how the hierarchical model regularizes extreme elasticity values often caused by data sparsity or outliers. Where simple, independent models might estimate implausible positive price elasticities for some products due to noise, the hierarchical structure pulls these estimates toward more reasonable category-level means through partial pooling. This shrinkage effect is especially valuable for retailers with large product catalogs where many items inevitably have limited historical price and sales data.\n\nThe approach also enables elasticity estimation at different levels of the product hierarchy (e.g., individual SKU, category, department), allowing analysts to understand price sensitivity at multiple relevant decision-making levels. The implementation using stochastic variational inference in NumPyro demonstrates that these sophisticated models can scale efficiently to large retail datasets with thousands of products, making them practical for real-world pricing applications.\n\nState Space Models in PyMC#\n\nFor time series forecasting, state space models (SSMs) provide a powerful and flexible framework capable of handling complex dynamics, including time-varying trends, multiple seasonality patterns, and the influence of external regressors. The PyMC ecosystem recently expanded with the introduction of PyMC-Extras, which includes a specialized module for structural time series modeling based on state space representations (primarily developed by Jesse Grabowski).\n\nThe Structural Time Series Modeling notebook demonstrates how to leverage these components to build sophisticated forecasting models within PyMC. The module provides building blocks that can be combined to create custom models tailored to specific needs, including:\n\nLocal Linear Trend: Captures time-varying level and slope components.\nSeasonal components: Models periodic patterns with flexible frequencies (e.g., daily, weekly, yearly).\nRegression components: Incorporates the static effects of covariates.\nDynamic regression coefficients: Allows for time-varying relationships between predictors and the target variable.\nCycle components: Models quasi-cyclical behaviors with stochastic periodicity and damping factors.\n\nWhat makes this implementation particularly powerful is its composability – you can easily mix and match these components to construct models that reflect the specific characteristics and expected dynamics of your data.\n\nThe implementation relies on an efficient Kalman filter algorithm for state estimation and likelihood calculation. This allows for seamless handling of missing data, robust anomaly detection, and integrated forecasting all within a unified Bayesian framework. This approach also facilitates the incorporation of domain knowledge through informative priors on the variance parameters, which control the flexibility and rate of change for each model component over time.\n\nA key advantage of the state space approach is its interpretability. It decomposes the time series into distinct, understandable components, making it easier to communicate insights to stakeholders. For example, you can extract and visualize the estimated trend, seasonal effects, and regression impacts separately, providing clear explanations of the factors driving your forecasts.\n\nFor practitioners looking to move beyond traditional or \"black-box\" forecasting methods, PyMC-Extras' state space module offers a fully Bayesian approach that explicitly models uncertainty while maintaining high interpretability. This is especially valuable in business contexts where understanding the why behind predictions is often as important as the predictions themselves.\n\nIf you want to learn more about this module, check out our webinar:\n\nSummary#\n\nIn this blog post, we've explored several ways probabilistic forecasting models can provide significant advantages for business applications compared to traditional time series methods. We've seen how hierarchical models can leverage shared information across related time series to improve forecasts, especially when dealing with sparse data. Censored likelihoods help overcome limitations imposed by historical data, enabling us to model the true underlying demand even when observations are truncated by stockouts or capacity constraints. For intermittent demand patterns, availability-constrained TSB models distinguish between true zero demand and stockout-induced zeros, leading to more accurate forecasts for sporadically selling products.\n\nWe've also demonstrated how domain knowledge can be incorporated through model calibration using custom likelihoods. This technique proves particularly valuable when historical data contains unobserved confounders or doesn't fully capture all relevant dynamics, allowing us to understand not only the forecast but also the drivers behind it. Finally, we've highlighted the powerful state space modeling capabilities now available in PyMC-Extras, which provide flexible, interpretable components for building sophisticated Bayesian time series forecasting models.\n\nAcross all these applications, the common thread is clear: probabilistic forecasting doesn't just quantify uncertainty more effectively—it enables us to directly incorporate business constraints, domain knowledge, and potentially causal relationships into our models. This leads to forecasts that are not only potentially more accurate but also more actionable and interpretable for decision- makers. As computational tools and methods continue to evolve, these sophisticated approaches are becoming increasingly accessible to practitioners, making probabilistic forecasting a valuable addition to any modern data scientist's toolkit.\n\nAdditionally, we've shown how these probabilistic approaches extend beyond traditional time series forecasting into areas like price elasticity modeling. Here, hierarchical Bayesian methods can regularize elasticity estimates for large product catalogs by leveraging the natural hierarchy in retail data. This ability to produce reliable estimates at different levels of aggregation (from individual SKUs to product categories) while effectively handling data sparsity demonstrates the versatility of probabilistic modeling for critical business decision-making beyond pure forecasting tasks.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/out-of-model-predictions-with-pymc": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nA simple use of posterior predictive sampling\nMaking predictions on different models\nSimulating new groups in hierarchical models\nForecasting time series\nSampling latent variables\nPredicting uncensored variables\nRecovering mixture indexes\nConcluding remarks\nOut of model predictions with PyMC\n\nJune 12, 2023\n\nBy Ricardo Vieira And Tomás Capretto\n\nPyMC has three core functions that map to the traditional Bayesian workflow:\n\nsample_prior_predictive (docs)\nsample (docs)\nsample_posterior_predictive (docs)\n\nPrior predictive sampling helps understanding the relationship between the parameter priors and the outcome variable, before any data is observed.\n\nSampling is used to infer the posterior distribution of parameters in a model, conditioned on observed data.\n\nFinally, posterior predictive sampling can be used to predict new outcomes, conditioned on the posterior parameters.\n\nWhat may not be immediately obvious is that predictions need not be done on the same model where parameters were inferred.\n\nFor example, if you learn (make inferences) about the volatility of a process in one context (or model), and you expect it to be similar in another, you can use what you learned to make better predictions in that second context (or predictive model). As we will see, the posterior predictive sampling function is more than happy to support this type of knowledge transfer.\n\nIn this blog post, we will walk through five different applications of the sample_posterior_predictive function:\n\nMaking predictions on the same model\nMaking predictions on different models\nSimulating new groups in hierarchical models\nForecasting time series\nSampling latent variables\nA simple use of posterior predictive sampling#\n+ Show Code\nimport arviz as az\nimport pymc as pm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nseed = sum(map(ord, \"Posterior Predictive\"))\nrng = np.random.default_rng(seed)\nsns.set_style(\"darkgrid\")\nsns.set(font_scale=1.3)\n\nprint(f\"Using arviz version: {az.__version__}\")\nprint(f\"Using pymc version: {pm.__version__}\")\n\nUsing arviz version: 0.13.0\nUsing pymc version: 5.3.1\n\n\nThere are two common uses of posterior predictive sampling, which we illustrate here:\n\nPerforming posterior predictive checks\nObtaining out-of-sample predictions\nwith pm.Model() as m:\n    # y ~ 2 * x\n    x = pm.MutableData(\"x\", [-2, -1, 0, 1, 2])\n    y_obs = [-4, -1.7, -0.1, 1.8, 4.1]\n    \n    beta = pm.Normal(\"beta\")\n    y = pm.Normal(\"y\", mu=beta * x, sigma=0.1, shape=x.shape, observed=y_obs)\n\n    idata = pm.sample(random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [beta]\n\n 100.00% [8000/8000 00:01<00:00 Sampling 4 chains, 0 divergences]\nwith m:\n    pp = pm.sample_posterior_predictive(idata, random_seed=rng)\n\nSampling: [y]\n\n 100.00% [4000/4000 00:00<00:00]\naz.plot_ppc(pp);\n\n\nwith m:\n    # Make predictions conditioned on new Xs\n    pm.set_data({\"x\": [-1, 3, 5]})\n    pp = pm.sample_posterior_predictive(idata, predictions=True, random_seed=rng)\n\nSampling: [y]\n\n 100.00% [4000/4000 00:00<00:00]\naz.plot_posterior(pp, group=\"predictions\");\n\n\nIn this blog post we are mostly interested in out-of-sample predictions, but we will see some cases of in-sample predictions as well.\n\nIn (generalized) linear models like this one we can obtain out-of-sample predictions by conditioning on new predictor values. When we do this, we are implicitly assuming that the same statistical relationship between predictor and outcome still holds.\n\nThere is nothing special about linear models, other than how easy it is to make predictions. In PyMC we don't even need to write a new model as we can simply swap the predictors with set_data (docs).\n\nHowever, there are many cases where such a \"trick\" does not suffice. We may need to write separate models for parameter inference and predictions, respectively. You may actually have multiple models for different types of predictions. This blog post shows how this can be done easily in PyMC.\n\nBefore we move on, let's see how we could have written a separate predictive model even for this simple linear model:\n\nwith pm.Model() as pred_m:\n    # Only x changes\n    x = np.array([-1, 0, 1])\n\n    beta = pm.Normal(\"beta\")\n    y_pred = pm.Normal(\"y_pred\", mu=beta * x, sigma=0.1, shape=x.shape)\n    \n    pp = pm.sample_posterior_predictive(\n        idata, \n        var_names=[\"y_pred\"], \n        predictions=True, \n        random_seed=rng,\n    )\n\nSampling: [y_pred]\n\n 100.00% [4000/4000 00:00<00:00]\naz.plot_posterior(pp, group=\"predictions\");\n\n\nNotice that we reused the idata object we got from sampling the first model. The posterior samples for the beta variable contained there were used when making predictions in this new model. We will explain in a moment how this works under the hood, but the important point is that sample_posterior_predictive does not care whether the current model is the one that generated the posterior draws we fed into it.\n\nYou may also have noticed that we had to pass var_names=[\"y_pred\"]. By default, sample_posterior_predictive only samples observed variables, but in our predictive model we didn't have observations (otherwise they wouldn't be predictions). We defined y_pred as an unobserved random variable (the kwarg observed was not specified). To get posterior predictive samples from these variables, we just need to include them in var_names.\n\nNow let's see how we can apply this strategy in more complex cases.\n\nMaking predictions on different models#\n\nIf we believe that our inferred variables are still valid in a new context, we can use posterior predictive sampling to make predictions conditioned on those variables.\n\nIn this example we imagine we have a process where the latent mean is the same as in the linear model above, but the observational noise follows a Student's T-distribution instead of a normal.\n\nIn our model, it means we assume mu = beta * x still holds. All the knowledge we have about beta is conveniently stored as posterior draws in our InferenceData, which we will reuse in the new model.\n\nidata.posterior.beta\n\nxarray.DataArray'beta'chain: 4draw: 1000\narray([[1.99738968, 1.99738968, 2.03544969, ..., 2.03003163, 1.91305585,\n1.91551041],\n[1.97578801, 1.99005488, 1.95586019, ..., 1.96489625, 1.96489625,\n1.9623966 ],\n[1.9866023 , 2.04438365, 1.90143213, ..., 2.0112917 , 2.01303214,\n1.99563726],\n[1.96956778, 1.96956778, 1.96431009, ..., 1.97842336, 1.96940486,\n1.99869095]])\nCoordinates:\nchain\n(chain)\nint64\n0 1 2 3\ndraw\n(draw)\nint64\n0 1 2 3 4 5 ... 995 996 997 998 999\nAttributes: (0)\n\nYou may want to pause and read about InferenceData before we go on.\n\nwith pm.Model() as pred_t_m:\n    # Using the same x as in the last example\n    x = np.array([-1, 0, 1])\n\n    beta = pm.Normal(\"beta\")\n\n    # Only the likelihood distribution changes\n    y_t = pm.StudentT(\"y_pred_t\", nu=4, mu=beta * x, sigma=0.1)\n    \n    pp_t = pm.sample_posterior_predictive(\n        idata, \n        var_names=[\"y_pred_t\"], \n        predictions=True, \n        random_seed=rng,\n    )\n\nSampling: [y_pred_t]\n\n 100.00% [4000/4000 00:00<00:00]\naz.plot_posterior(pp, group=\"predictions\");\naz.plot_posterior(pp_t, group=\"predictions\", color=\"C1\");\n\n\nIn fact it doesn't even matter that the \"transferred variables\" are given the same prior as in the original model. After all, the posterior distribution rarely follows the same form as the prior (there is even a funny name, conjugate prior, for the few cases where this happens).\n\nIn our case, all the knowledge we have about the posterior distribution of the parameters is encoded in the form of samples in our InferenceData posterior group. sample_posterior_predictive simply checks if a model variable has the same name as one in that group. If it finds a match, it assumes those draws are valid for the variable in the current model.\n\nTo illustrate this, we will give beta a Flat prior in our new predictive model. Note that one can't take random draws from this distribution in the first place:\n\ntry:\n    pm.draw(pm.Flat.dist())\nexcept Exception as exc:\n    print(f\"{exc.__class__.__name__}: {exc.args[0].splitlines()[0]}\")\n\nNotImplementedError: Cannot sample from flat variable\n\n\nIf sample_posterior_predictive was trying to take random draws from this variable, we would see this error. But because we have a variable with the same name in the posterior group, the function will use those draws instead, assuming implicitly that they form a valid posterior.\n\nwith pm.Model() as pred_bern_m:\n    x = np.linspace(-1, 1, 25))\n\n    beta = pm.Flat(\"beta\")\n\n    # We again change the functional form of the model\n    # Instead of a linear Gaussian we Have a logistic Bernoulli model\n    p = pm.Deterministic(\"p\", pm.math.sigmoid(beta * x))\n    y = pm.Bernoulli(\"y\", p=p)\n\n    pp = pm.sample_posterior_predictive(\n        idata, \n        var_names=[\"p\", \"y\"], \n        predictions=True, \n        random_seed=rng,\n    )\n\nSampling: [y]\n\n 100.00% [4000/4000 00:00<00:00]\n\nIn this example we forced our linear predictors through a sigmoid transformation, in order to take Bernoulli draws.\n\ndef jitter(x, rng):\n    return rng.normal(x, 0.02)\n\nx = pp.predictions_constant_data[\"x\"]\nfor i in range(25):\n    p = pp.predictions[\"p\"].sel(chain=0, draw=i)\n    y = pp.predictions[\"y\"].sel(chain=0, draw=i)\n    \n    plt.plot(x, p, color=\"C0\", alpha=.1)\n    plt.scatter(jitter(x, rng), jitter(y, rng), s=10, color=\"k\", alpha=.1)\n\nplt.plot([], [], color=\"C0\", label=\"p\")\nplt.scatter([], [], color=\"k\", label=\"y + jitter\")\nplt.legend(loc=(1.03, 0.75));\n\n\nWe will briefly describe the internal mechanism used by sample_posterior_predictive to combine posterior draws into predictive samples at the end. For now let's just see it in action in some other places.\n\nSimulating new groups in hierarchical models#\n\nHierarchical models are a powerful class of Bayesian models that allow the back-and-forth flow of information across statistically related groups. One predictive question that arises naturally in such settings, is what to expect from yet unseen groups.\n\nThink about all the cases where this applies. You may want to predict the lifetime of the next acquired customer, or predict the sales of a new product that has not yet been launched. In both cases, we assume there is some similarity between old and new customers or products.\n\nWe will grab the eight schools model to show how posterior predictive sampling can be used to simulate new groups from a hierarchical model. We will investigate what a 9th and 10th school might look like.\n\ny = np.array([28, 8, -3, 7, -1, 1, 18, 12])\nsigma = np.array([15, 10, 16, 11, 9, 11, 10, 18])\nJ = 8\n\nwith pm.Model() as eight_schools:\n    eta = pm.Normal(\"eta\", 0, 1, shape=J)\n\n    # Hierarchical mean and SD\n    mu = pm.Normal(\"mu\", 0, sigma=10)\n    tau = pm.HalfNormal(\"tau\", 10)\n\n    # Non-centered parameterization of random effect\n    theta = pm.Deterministic(\"theta\", mu + tau * eta)\n\n    pm.Normal(\"y\", theta, sigma=sigma, observed=y)\n\n    idata = pm.sample(2000, target_accept=0.9, random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [eta, mu, tau]\n\n 100.00% [12000/12000 00:09<00:00 Sampling 4 chains, 1 divergences]\n\nAfter sampling (and recklessly ignoring divergences) we write a predictive model that can be used to predict the out-of-sample schools 9 and 10.\n\nWhile we don't have to, we will write the model in such a way that we can also get posterior predictive draws from the initial 8 schools. This will allow us to discuss some subtleties when defining predictive models.\n\nFirst, note we can't simply define an eta distribution with shape=J+2, because the posterior predictive sampling function would assume we have the whole posterior for this distribution and try to use the 8 values to take 10 draws (which would crash immediately).\n\nWe actually don't know what the eta should be for the two unobserved schools, so we want to sample it from the prior. The solution is to create two vectors of variables separately, eta and eta_new and concatenate them. The sample_posterior_predictive function will reuse the InferenceData draws for eta and take new draws for eta_new.\n\nA predictive model can have unobserved variables that were not present in the original model. When we request samples for variables that depend on unobserved variables that can't be found in the InferenceData, we will get draws from their prior. This is how we will get eta_new draws needed to generate predictions for our variable of interest, y.\n\nLet's also assume we don't know exactly what the sigma is for the new schools, but that we can come up with a unique prior for each. We will add that as yet another unobserved variable to the predictive model. We will name it sigma_new.\n\nwith pm.Model() as ten_schools:\n    # Priors for schools 9 and 10\n    # We assume that the mean of school 10 is expected to be one std above the mean\n    # and have a relatively low measurement error\n    eta_new = pm.Normal(\"eta_new\", mu=[0, 1.0], sigma=1)\n    sigma_new = pm.Uniform(\"sigma_new\", lower=[10, 5], upper=[20, 7])\n\n    # These are unchanged\n    eta = pm.Normal(\"eta\", 0, 1, shape=J)\n    mu = pm.Normal(\"mu\", 0, sigma=10)\n    tau = pm.HalfNormal(\"tau\", 10)\n\n    # We concatenate the variables from the old and new groups\n    theta = pm.Deterministic(\"theta\", mu + tau * pm.math.concatenate([eta, eta_new]))\n    pm.Normal(\"y\", theta, sigma=pm.math.concatenate([sigma, sigma_new]))\n\n    pp = pm.sample_posterior_predictive(idata, var_names=[\"y\"], random_seed=rng)\n\nSampling: [eta_new, sigma_new, y]\n\n 100.00% [8000/8000 00:01<00:00]\naz.summary(pp, group=\"posterior_predictive\")\n\n\tmean\tsd\thdi_3%\thdi_97%\tmcse_mean\tmcse_sd\tess_bulk\tess_tail\tr_hat\ny[0]\t8.815\t16.110\t-20.375\t40.508\t0.178\t0.130\t8184.0\t7836.0\t1.0\ny[1]\t7.160\t11.415\t-14.631\t28.470\t0.126\t0.091\t8245.0\t7910.0\t1.0\ny[2]\t5.465\t17.145\t-27.803\t36.261\t0.189\t0.137\t8256.0\t7846.0\t1.0\ny[3]\t6.548\t12.328\t-15.879\t30.123\t0.146\t0.103\t7128.0\t6930.0\t1.0\ny[4]\t4.875\t10.765\t-15.694\t24.765\t0.124\t0.088\t7592.0\t7693.0\t1.0\ny[5]\t5.653\t12.332\t-17.918\t28.482\t0.135\t0.101\t8285.0\t7559.0\t1.0\ny[6]\t8.716\t11.520\t-13.165\t30.285\t0.127\t0.091\t8289.0\t7738.0\t1.0\ny[7]\t7.101\t18.875\t-28.648\t42.553\t0.217\t0.153\t7614.0\t7686.0\t1.0\ny[8]\t6.354\t16.879\t-25.829\t38.007\t0.192\t0.136\t7690.0\t7457.0\t1.0\ny[9]\t11.220\t10.158\t-6.581\t31.223\t0.115\t0.083\t7885.0\t7480.0\t1.0\n+ Show Code\npps = az.extract(pp, group=\"posterior_predictive\")\n\n_, ax = plt.subplots(5, 2, figsize=(8, 14), sharex=True, sharey=True)\nfor i, axi in enumerate(ax.ravel()):\n    sns.kdeplot(pps[\"y\"][i], fill=True, ax=axi, color=\"C0\" if i < 8 else \"C1\")\n    axi.axvline(0, ls=\"--\", c=\"k\")\n    axi.set_title(f\"School[{i}]\")\nplt.tight_layout()\n\n\nThe predictions for new schools are informed by the group-level variables mu and tau, which were estimated via sampling of the original subset of 8 schools.\n\nAs there is no further structure that distinguishes the new schools, the difference in predictions arises only from the eta_new and sigma_new priors we assigned to them.\n\nOther models could yield different predictions from independent variables, while keeping the priors equal. Other models yet may have no information that distinguishes new groups, in which case their posterior predictive draws would all be identical (up to random noise).\n\nLet's now look into the future...\n\nForecasting time series#\n\nIf we have a time series model, it's relatively easy to perform a forecast by creating a predictive model with a new time series that starts where the observations \"left off\".\n\nFor this example we will simulate draws from a Gaussian random walk (docs). If you are unfamiliar with the use of dist and draw, you may want to read our previous article on simulating data with PyMC.\n\nmu_true = -0.05\nsigma_true = 0.5\n\ny = pm.GaussianRandomWalk.dist(\n    init_dist=pm.Normal.dist(), \n    mu=mu_true, \n    sigma=sigma_true,\n    steps=99,\n)\ny_obs = pm.draw(y, random_seed=rng)\n\nplt.title(f\"mu={mu_true:.2f}, sigma={sigma_true:.2f}\")\nplt.plot(y_obs, color=\"k\");\n\n\nwith pm.Model() as m:\n    mu = pm.Normal(\"mu\")\n    sigma = pm.Normal(\"sigma\")\n    y = pm.GaussianRandomWalk(\n        \"y\", \n        init_dist=pm.Normal.dist(), \n        mu=mu, \n        sigma=sigma,\n        observed=y_obs\n    )\n\n    idata = pm.sample(random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\n\n 100.00% [8000/8000 00:02<00:00 Sampling 4 chains, 0 divergences]\n\nTo force a new time series to start where the observations \"left off\", we define init_dist as a DiracDelta on the last observed y. This will force every predictive series to start at that exact value.\n\nNote again that the prior distributions don't matter, only the variable names and shapes. We use Flat for sigma as in an earlier example. We use Normal for mu because (spoiler alert) we will actually sample from it in the next example.\n\nwith pm.Model() as forecast_m:\n    mu = pm.Normal(\"mu\")\n\n    # Flat sigma for illustration purposes\n    sigma = pm.Flat(\"sigma\")\n\n    # init_dist now starts on last observed value of y\n    pm.GaussianRandomWalk(\n        \"y_forecast\",\n        init_dist=pm.DiracDelta.dist(y_obs[-1]),\n        mu=mu,\n        sigma=sigma,\n        steps=99,\n    )\n    \n    pp = pm.sample_posterior_predictive(\n        idata, \n        var_names=[\"y_forecast\"], \n        predictions=True, \n        random_seed=rng,\n    )\n\nSampling: [y_forecast]\n\n 100.00% [4000/4000 00:00<00:00]\n+ Show Code\nsteps = np.arange(100, 200)\nax = az.plot_hdi(x=steps, y=pp.predictions[\"y_forecast\"])\n# Plot first five forecasts\nfor i in range(5):\n    y = pp.predictions[\"y_forecast\"].isel(chain=0, draw=i)\n    ax.plot(steps, y, color=\"k\")\nax.plot(np.arange(100), y_obs, color=\"k\", alpha=0.7)\nax.axvline(100, ls=\"--\", color=\"k\")\nax.set_xticks([50, 150])\nax.set_xticklabels([\"observed\", \"forecast\"]);\n\n\nWe mentioned that the validity of posterior predictive samples hangs on the premise that what we learned about variables in one context transfers to novel ones. But reality is seldom so simple.\n\nFor instance, we may believe some parameters generalize but not others. Or they may generalize but we are still unsure how much. When we create a predictive model we can easily codify this knowledge.\n\nWe will repeat the forecast example but assume only the volatility (sigma), but not the mean drift (mu) holds into the future. There are a few ways we can achieve this:\n\nDrop the mu draws from the posterior group. As we mentioned in the schools example, any unobserved variable that is required for generating draws from the predictive variables will be sampled from the prior if not present.\nUse a different name for the drift in the forecast time series. This is basically the same as option 1, since the new variable won't be present in the posterior group and will have to be resampled from the prior.\nInclude mu in var_names. This will force sample_posterior_predictive to ignore the mu posterior draws and resample it from the prior. Any variables between mu and our variables of interest would also be resampled from the prior as the posterior draws are no longer relevant, since they depended directly on mu.\n\nAny option is equally valid. We will pick the last one as we can reuse the model we already defined.\n\nwith forecast_m:\n    pp_resampling_mu = pm.sample_posterior_predictive(\n        idata, \n        var_names=[\"mu\", \"y_forecast\"], \n        predictions=True, \n        random_seed=rng,\n    )\n\nSampling: [mu, y_forecast]\n\n 100.00% [4000/4000 00:00<00:00]\n+ Show Code\nsteps = np.arange(100, 200)\nax = az.plot_hdi(x=steps, y=pp_resampling_mu.predictions[\"y_forecast\"])\n# Plot first five forecasts\nfor i in range(5):\n    y = pp_resampling_mu.predictions[\"y_forecast\"].isel(chain=0, draw=i)\n    ax.plot(steps, y, color=\"k\")\nax.plot(np.arange(100), y_obs, color=\"k\", alpha=0.7)\nax.axvline(100, ls=\"--\", color=\"k\")\nax.set_xticks([50, 150])\nax.set_xticklabels([\"observed\", \"forecast\"]);\n\n\nForecasting is now incredibly wide. We can achieve a middle ground, by reusing mu but adding new uncertainty downstream of it.\n\nwith pm.Model() as forecast_noisy_m:\n    # Again using Flat priors. This has a nice debug value,\n    # because it confirms the values must come from the trace and not the prior\n    mu = pm.Flat(\"mu\")\n    sigma = pm.Flat(\"sigma\")\n\n    # We add a new normal noise term around the inferred mu\n    mu_noisy = pm.Normal(\"mu_noisy\", mu, sigma=0.1)\n\n    pm.GaussianRandomWalk(\n        \"y_forecast\",\n        init_dist=pm.DiracDelta.dist(y_obs[-1]),\n        mu=mu_noisy,\n        sigma=sigma,\n        steps=99,\n    )\n\n    pp_noisy_mu = pm.sample_posterior_predictive(\n        idata, \n        var_names=[\"y_forecast\"], \n        predictions=True, \n        random_seed=rng,\n    )\n\nSampling: [mu_noisy, y_forecast]\n\n 100.00% [4000/4000 00:00<00:00]\nsteps = np.arange(100, 200)\nax = az.plot_hdi(x=steps, y=pp_noisy_mu.predictions[\"y_forecast\"])\n# Plot first five forecasts\nfor i in range(5):\n    y = pp_noisy_mu.predictions[\"y_forecast\"].isel(chain=0, draw=i)\n    ax.plot(steps, y, color=\"k\")\nax.plot(np.arange(100), y_obs, color=\"k\", alpha=0.7)\nax.axvline(100, ls=\"--\", color=\"k\")\nax.set_xticks([50, 150])\nax.set_xticklabels([\"observed\", \"forecast\"]);\n\n\nSampling latent variables#\n\nThe examples up to here focused on predicting model outcomes. In some cases we may be more interested in predicting latent variables.\n\nIn the next two examples we show that sample_posterior_predictive can be easily used for this purpose as well.\n\nPredicting uncensored variables#\n\nWe will start with a rather simple application. After doing inference on censored data we wonder what future observations may look like ignoring any censoring process (docs).\n\nThis could be used to make predictions about the expected lifetime of a patient that was still alive when the latest data was collected, or even a completely new patient.\n\nx_censored_obs = [4.3, 5.0, 5.0, 3.2, 0.7, 5.0]\n\nwith pm.Model() as censored_m:\n    mu = pm.Normal(\"mu\")\n    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n    \n    x = pm.Normal.dist(mu, sigma)\n    x_censored = pm.Censored(\n        \"x_censored\", \n        dist=x, \n        lower=None, \n        upper=5.0, \n        observed=x_censored_obs,\n    )\n\n    idata = pm.sample(random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma]\n\n 100.00% [8000/8000 00:02<00:00 Sampling 4 chains, 1 divergences]\n\nAll we have to do is to recreate the original model without censoring the variable of interest.\n\nJust for illustration purposes, we will actually make predictions from a still-censored and an uncensored process. This way we can compare the two side by side.\n\nwith pm.Model() as uncensored_m:\n    mu = pm.Normal(\"mu\")\n    sigma = pm.HalfNormal(\"sigma\")\n\n    x = pm.Normal.dist(mu, sigma)\n    x_censored = pm.Censored(\"x_censored\", dist=x, lower=None, upper=5.0)\n\n    # This uncensored variable is new\n    x_uncensored = pm.Normal(\"x_uncensored\", mu, sigma)\n\n    pp = pm.sample_posterior_predictive(\n        idata,\n        var_names=[\"x_censored\", \"x_uncensored\"],\n        predictions=True,\n        random_seed=rng,\n    )\n\nSampling: [x_censored, x_uncensored]\n\n 100.00% [4000/4000 00:00<00:00]\naz.plot_posterior(pp, group=\"predictions\");\n\n\nLet's mix things up for our final example...\n\nRecovering mixture indexes#\n\nIt's common advice to marginalize discrete parameters, so that inference can be done exclusively with gradient-based samplers like NUTS.\n\nHowever, we often do care about the latent discrete variables. For example we may be interested in classifying which discrete source generated an observed event.\n\nOnce we have inferred the continuous parameters in our model, it's generally possible to recover marginalized variables by doing a bit of algebra. As you may have guessed, we will again rely on sample_posterior_predictive.\n\nWe pick a Mixture model as an example. The handy Mixture (docs) distribution implicitly marginalizes over categorical index variables that identify the component that generates each observation.\n\nHere is how we can simulate some data from a Normal mixture:\n\n# ~30% of the draws come from component 0 and 70% from component 1\nw_true = [0.3, 0.7]\n\n# Components are Normals centered around -5 and 5, and with 2.5 std\nmu_true = [-1, 1]\nsigma_true = [0.5, 0.5]\n\nN = 20\nidxs = pm.Categorical.dist(w_true, shape=(N,))\ncomponents = [\n    pm.Normal.dist(mu_true[0], sigma_true[0], shape=(N, 1)),\n    pm.Normal.dist(mu_true[1], sigma_true[1], shape=(N, 1)),\n]\ny = pm.math.concatenate(components, axis=-1)[np.arange(N), idxs]\nidxs_true, y_obs = pm.draw([idxs, y], random_seed=rng)\n\nidxs_true, y_obs\n\n(array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1]),\n array([ 1.33624848,  0.57901453,  0.79282242,  0.93851528,  1.29247425,\n         0.73555422,  0.66878824,  0.89633854,  0.37092651,  0.07381751,\n         0.81795087, -1.5136106 , -0.56532478,  1.44692458,  0.85920656,\n        -0.02250421, -1.81445585,  1.48100228,  0.25457723,  1.31812146]))\n\n_, ax = plt.subplots(figsize=(6, 3))\n\n# Let's plot the density\nx = np.linspace(-3, 3, 10_000)\npdf = (\n    w_true[0] * pm.logp(components[0], x).exp()\n    + w_true[1] * pm.logp(components[1], x).exp()\n).eval()\n\nax.scatter(x, pdf, s=1, c=(x/3 + 1), cmap=\"bwr\")\nax.scatter(y_obs, np.zeros(N), c=idxs_true, cmap=\"bwr\", alpha=0.7)\nax.set_yticks([])\nax.set_title(\"y_obs\");\n\n\nThe dots in the plot correspond to the simulated values, color-coded by their original mixture component. We also plot the density of the marginalized mixture above.\n\nIn an applied setting, the true Mixture form is likely unknown and we would want to estimate it via sampling. To do this, we fit a model with wide component mean and noise priors, conditioned on our (simulated) observations. As mentioned, the Mixture distribution allows us to marginalize over the index categorical variables and perform sampling exclusively with NUTS.\n\nwith pm.Model() as m:\n    mu = pm.Normal(\"mu\", [-5, 5], 2.0)\n    sigma = pm.HalfNormal(\"sigma\", 1, shape=(2,))\n    w = pm.Dirichlet(\"w\", [1, 1])\n\n    comp_dists = [\n        pm.Normal.dist(mu[0], sigma[0]),\n        pm.Normal.dist(mu[1], sigma[1])\n    ]\n    pm.Mixture(\"y\", w=w, comp_dists=comp_dists, observed=y_obs)\n\n    idata = pm.sample(target_accept=0.9, random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mu, sigma, w]\n\n 100.00% [8000/8000 00:11<00:00 Sampling 4 chains, 1 divergences]\naz.summary(idata)\n\n\tmean\tsd\thdi_3%\thdi_97%\tmcse_mean\tmcse_sd\tess_bulk\tess_tail\tr_hat\nmu[0]\t-1.656\t1.152\t-3.535\t0.428\t0.051\t0.040\t823.0\t391.0\t1.02\nmu[1]\t0.803\t0.168\t0.521\t1.151\t0.005\t0.004\t1085.0\t899.0\t1.01\nsigma[0]\t0.855\t0.512\t0.079\t1.771\t0.031\t0.022\t154.0\t32.0\t1.02\nsigma[1]\t0.562\t0.164\t0.294\t0.890\t0.006\t0.005\t924.0\t613.0\t1.02\nw[0]\t0.190\t0.129\t0.007\t0.435\t0.004\t0.003\t790.0\t946.0\t1.00\nw[1]\t0.810\t0.129\t0.565\t0.993\t0.004\t0.003\t790.0\t946.0\t1.00\n\nEquipped with some knowledge of conditional probability in mixture processes, and draws from the posterior parameters, we can now recover the indexes by sampling from a suitably parametrized Categorical distribution.\n\nwith pm.Model() as recover_m:\n    # Remember: the prior form doesn't actually matter!\n    mu = pm.Normal(\"mu\", shape=(2,))\n    sigma = pm.HalfNormal(\"sigma\", shape=(2,))\n    w = pm.Dirichlet(\"w\", [1, 1])\n\n    comp_dists = [\n        pm.Normal.dist(mu[0], sigma[0]),\n        pm.Normal.dist(mu[1], sigma[1])\n    ]\n\n    # Compute the logp that each datapoint came from each component\n    log_probs = pm.math.concatenate([\n        [pm.math.log(w[0]) + pm.logp(comp_dists[0], y_obs)], \n        [pm.math.log(w[1]) + pm.logp(comp_dists[1], y_obs)],\n    ], axis=0)\n\n    # log_probs has shape (2, 20), we transpose it to (20, 2), so that the \n    # Categorical takes 20 batched draws from two possible values of [0, 1]\n    idx = pm.Categorical(\"idx\", logit_p=log_probs.T)\n\n    pp = pm.sample_posterior_predictive(idata, var_names=[\"idx\"], random_seed=rng)\n\nSampling: [idx]\n\n 100.00% [4000/4000 00:01<00:00]\naz.summary(pp, group=\"posterior_predictive\")\n\n\tmean\tsd\thdi_3%\thdi_97%\tmcse_mean\tmcse_sd\tess_bulk\tess_tail\tr_hat\nidx[0]\t0.976\t0.152\t1.0\t1.0\t0.003\t0.002\t2262.0\t4000.0\t1.00\nidx[1]\t0.952\t0.215\t1.0\t1.0\t0.005\t0.003\t2121.0\t4000.0\t1.00\nidx[2]\t0.968\t0.175\t1.0\t1.0\t0.004\t0.003\t1883.0\t4000.0\t1.00\nidx[3]\t0.976\t0.152\t1.0\t1.0\t0.003\t0.002\t2237.0\t4000.0\t1.00\nidx[4]\t0.978\t0.145\t1.0\t1.0\t0.003\t0.002\t2333.0\t4000.0\t1.00\nidx[5]\t0.968\t0.176\t1.0\t1.0\t0.004\t0.003\t2157.0\t4000.0\t1.00\nidx[6]\t0.967\t0.179\t1.0\t1.0\t0.005\t0.003\t1554.0\t4000.0\t1.00\nidx[7]\t0.974\t0.161\t1.0\t1.0\t0.004\t0.003\t2011.0\t4000.0\t1.00\nidx[8]\t0.919\t0.273\t0.0\t1.0\t0.006\t0.004\t1863.0\t1863.0\t1.00\nidx[9]\t0.849\t0.358\t0.0\t1.0\t0.009\t0.006\t1612.0\t1612.0\t1.00\nidx[10]\t0.970\t0.169\t1.0\t1.0\t0.004\t0.003\t2099.0\t4000.0\t1.00\nidx[11]\t0.080\t0.271\t0.0\t1.0\t0.013\t0.009\t430.0\t430.0\t1.01\nidx[12]\t0.527\t0.499\t0.0\t1.0\t0.018\t0.012\t812.0\t812.0\t1.01\nidx[13]\t0.973\t0.163\t1.0\t1.0\t0.003\t0.002\t2263.0\t4000.0\t1.00\nidx[14]\t0.976\t0.155\t1.0\t1.0\t0.003\t0.002\t2024.0\t4000.0\t1.00\nidx[15]\t0.823\t0.382\t0.0\t1.0\t0.010\t0.007\t1382.0\t1382.0\t1.00\nidx[16]\t0.062\t0.240\t0.0\t1.0\t0.012\t0.009\t383.0\t383.0\t1.01\nidx[17]\t0.970\t0.172\t1.0\t1.0\t0.004\t0.003\t1985.0\t4000.0\t1.00\nidx[18]\t0.904\t0.294\t0.0\t1.0\t0.007\t0.005\t1740.0\t1740.0\t1.00\nidx[19]\t0.974\t0.158\t1.0\t1.0\t0.003\t0.002\t2341.0\t4000.0\t1.00\nidx = pp.posterior_predictive[\"idx\"].mean((\"chain\", \"draw\"))\n\n_, ax = plt.subplots(figsize=(6, 3))\nax.bar(y_obs, 1-idx, width=0.1, label=\"idx==0\", color=\"b\")\nax.bar(y_obs, idx, bottom=1-idx, width=0.1, label=\"idx==1\", color=\"r\")\nax.scatter(y_obs, np.zeros(N) - 0.1, c=idxs_true, cmap=\"bwr\", alpha=0.7)\nax.legend(loc=(1.03, 0.75));\n\n\nIn the plot above we show the proportion of categorical draws for each observation, as the height of stacked bars.\n\nWe can see that inference about the latent indexes is reasonable for most observations. Uncertainty increases as the values get closer to the center, and can even flip when an observation is more probable under the opposite mixture component.\n\nIf we were interested in predicting the original component for new datapoints, we could just pass them instead of y_obs in the recovery model.\n\nConcluding remarks#\n\nThis blog post illustrated how PyMC's sample_posterior_predictive function can make use of learned parameters to predict variables in novel contexts. This is valid as long as the used parameters are expected to generalize.\n\nThe actual mechanism used by sample_posterior_predictive is pretty simple. We start with the generative process encoded by the PyMC model. Sampling directly from it would give us prior predictive draws (this is exactly what sample_prior_predictive does). But, for sample_posterior_predictive there is one extra step.\n\nAny variables that are found in the posterior group of the InferenceData (via name matching), and not requested to be resampled in the var_names argument, are replaced by the posterior draws. Any of the requested variables in var_names that happen to be downstream of these, will now be \"conditioned\" on the posterior draws and not the priors. There are some other subtleties that you can read about in forward.py, but this is more or less the gist of it.\n\nGenerating predictions like this is seemingly trivial but very powerful. As always, we invite you to try PyMC and see if it fits your needs!\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/2023-19-11-marketing-effectiveness": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nWhy Measuring Marketing Effectiveness Matters\nApproaches to Evaluating Marketing Performance: Comparing Options\nOutsourcing Marketing Evaluation vs. Building In-House Capabilities\nOptimizing the Marketing Effectiveness Workflow\nKey Methods in Marketing Measurement\nStrategic Budget Allocation for Marketing Analytics\nHow PyMC Labs Facilitates Strategic Marketing Analysis\nMastering Marketing Effectiveness: A Comprehensive Guide for Digital Marketers\n\nNovember 19, 2023\n\nBy Niall Oulton\n\nIn today's fast-paced digital marketing landscape, it's crucial to master measuring and understanding marketing strategies' effectiveness. This guide covers the importance of marketing effectiveness, explores various evaluation methods, and presents best practices for implementing an effective marketing measurement strategy.\n\nWhy Measuring Marketing Effectiveness Matters#\n\nMeasuring the impact of marketing initiatives is vital for several compelling reasons:\n\nSmart Resource Allocation: Businesses need to know which marketing strategies are working and which aren't. This knowledge allows them to allocate their resources wisely, ensuring investments are channelled into the most effective areas.\n\nInformed Strategic Decision-Making: By tapping into data-driven insights obtained from in-depth marketing analysis, businesses gain the power to make well-informed strategic decisions. This adaptability is essential for keeping up with ever-changing market trends and shifting consumer behaviours.\n\nGetting the Most from Your Investment: Identifying the most effective marketing strategies is crucial for maximizing a company's return on investment (ROI). Focusing on ROI is the key to achieving financial efficiency and success in marketing endeavours.\n\nApproaches to Evaluating Marketing Performance: Comparing Options#\nOutsourcing Marketing Evaluation vs. Building In-House Capabilities#\n\nOutsourcing to Specialized Agencies:\n\nAdvantages:\nGains access to a wide range of expertise and seasoned experience.\nEnsures an unbiased and objective evaluation of marketing performance.\nChallenges:\nMay involve significant financial investment.\nPossible issues with transparency and reduced control over marketing strategies.\n\nDeveloping In-House Marketing Analysis Teams:\n\nAdvantages:\nFacilitates the development of bespoke models, precisely tailored to meet unique business requirements.\nFosters a deeper understanding of the business, leading to more agile and responsive decision-making.\nMore cost-effective in the long term.\nChallenges:\nRequires substantial investment in resources and the development of specific skill sets.\nThere's a potential risk of bias stemming from internal perspectives.\nOptimizing the Marketing Effectiveness Workflow#\n\nFundamentals of Data Understanding: The foundation of successful marketing lies in identifying and meticulously tracking the appropriate data. This vital step involves a comprehensive analysis of internal and external data sources, providing an understanding of marketing dynamics.\n\nStrategic Model Identification: Selecting the appropriate model is pivotal, and it hinges on a deep understanding of industry-specific nuances and data characteristics. This choice is critical to tailoring marketing strategies effectively.\n\nOngoing Testing and Evaluation: To guarantee the relevance and accuracy of marketing models, regular testing, evaluation, and necessary retraining are essential. This ongoing process ensures that marketing efforts remain effective and up-to-date.\n\nKey Methods in Marketing Measurement#\n\nAttribution Modeling: Despite a shift in favorability, attribution modelling remains a useful tool for certain businesses, helping to trace the impact of specific marketing actions.\n\nExperiments and A/B Testing: Widely regarded as the gold standard in marketing measurement, these experiments, though limited in scope, provide invaluable insights into the effectiveness of specific strategies.\n\nHolistic Marketing Mix Models: These models offer a broader perspective, encompassing various elements to gauge overall performance.\n\nStrategic Budget Allocation for Marketing Analytics#\n\nStrategic budget allocation for marketing analytics is crucial for marketing effectiveness. Generally, allocating about 10% of the media budget for analytics is advised, but adjustments may be necessary depending on business size and market characteristics.\n\nHow PyMC Labs Facilitates Strategic Marketing Analysis#\n\nAt PyMC Labs, we're acutely aware of the challenges in striking an optimal balance between the complexity of sophisticated modelling and the practicality of its application.\n\nOur approach is firmly rooted in practical efficiency and tailored solutions. We reject one-size-fits-all models, opting instead to craft solutions tailored to your business's specific needs. This approach guarantees our models are actionable tools suited for real-world decision-making, not just theoretical ones.\n\nTransparency is the cornerstone of our philosophy at PyMC Labs. We go beyond offering insights, focusing on helping you understand the why and how behind each conclusion. This clarity empowers your team to make informed decisions, building both solutions and trust.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/Thomas_PyData_London": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nIntroduction\nAbout Speaker\nTimestamps\nImportant Links\nSolving Real-World Business Problems with Bayesian Modeling\n\nOctober 31, 2022\n\nBy Thomas Wiecki\n\nIntroduction#\n\nAmong Bayesian early adopters, digital marketing is chief. While many industries are embracing Bayesian modeling as a tool to solve some of the most advanced data science problems, marketing is facing unique challenges for which this approach provides elegant solutions. Among these challenges are a decrease in quality data, driven by an increased demand for online privacy and the imminent \"death of the cookie\" which prohibits online tracking. In addition, as more companies are building internal data science teams, there is an increased demand for in-house solutions.\n\nIn this talk Thomas explains how Bayesian modeling addresses these issues by:\n\n(i) Incorporating expert knowledge of the structure as well as about plausible parameter rangers.\n\n(ii) Connecting multiple different data sets to increase circumstantial evidence of latent user features.\n\n(iii) Principled quantification of uncertainty to increase robustness of model fits and interpretation of the results.\n\nInspired by real-world problems we encountered at PyMC Labs, we will look at Media Mix Models for marketing attribution and Customer Lifetime Value models and various hybrids between them.\n\nAbout Speaker#\n\nDr. Thomas Wiecki is an author of PyMC, the leading platform for statistical data science. To help businesses solve some of their trickiest data science problems, he assembled a world class team of Bayesian modelers founded PyMC Labs -- the Bayesian consultancy. He did his PhD at Brown University studying cognitive neuroscience.\n\nGitHub: https://github.com/twiecki\nTwitter: https://twitter.com/twiecki\nWebsite: https://twiecki.io/\nTimestamps#\n\n00:00 Welcome!\n\n0:05 Speaker introduction and PyMC 4 release announcement\n\n1:15 PyMC Labs- The Bayesian consultancy\n\n2:39 Why is marketing so eager to adopt Bayesian solutions\n\n3:49 Case Study: Estimating Marketing effectiveness\n\n6:00 Estimating Customer Acquisition Cost (CAC) using linear regression\n\n7:36 Drawbacks of linear regression in estimating CAC\n\n10:02 Blackbox Machine learning and its drawbacks\n\n11:27 Bayesian modelling\n\n11:52 Advantages of Bayesian modelling\n\n14:12 How does Bayesian modelling work?\n\n16:53 Solution proposals(priors)\n\n17:26 Model structure\n\n19:57 Evaluate solutions\n\n20:16 Plausible solutions(posterior)\n\n22:36 Improving the model\n\n23:38 Modelling multiple Marketing Channels\n\n24:51 Modelling channel similarities with hierarchy\n\n26:13 Allowing CAC to change over time\n\n28:00 Hierarchical Time Varying process\n\n30:05 Comparing Bayesian Media Mix Models\n\n30:47 What-If Scenario Forecasting\n\n31:53 Adding other data sources as a way to help improve or inform estimates\n\n33:00 When does Bayesian modelling work best?\n\n33:35 Intuitive Bayes course\n\n34:38 Question 1: Effectiveness of including variables seasonality?\n\n36:03 Question 2: What is your recommendation for the best way to choose priors?\n\n38:16 Question 3: How to test if an assumption about the data is valid?\n\n39:07 Question 4: Do you take the effect of different channels on each other into account?\n\n41:33 Thank you!\n\nImportant Links#\n\nPyMC Labs\n\nIntuitive bayes course\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/innovation-lab": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nAbstract\nReimagining Product Innovation with Synthetic Consumers and Agentic Frameworks\nWhere Product Innovation Breaks\nReal Value for High-Stakes Decisions\nKey Capabilities of the AI Innovation Lab\nFrom Prototype to Proof: Validating the Platform\nLooking Ahead: One Integrated System\nReady to Accelerate Product Innovation?\nAI Innovation Lab: An agentic platform for transforming product development\n\nJune 03, 2025\n\nBy Nina Rismal\n\nAbstract#\n\nAt PyMC Labs, we’re tackling a core problem in the CPG industry: product innovation is too slow, fragmented, and disconnected from real consumer needs.\n\nThe AI Innovation Lab is an end-to-end platform that uses AI-powered synthetic consumers and expert agent collaboration to streamline every stage of product development — from smarter concept briefs and design iteration to pricing simulation and market validation.\n\nValidated through custom metrics and real-world benchmarks, our U.S. panel replicates up to 90% of consumer behavior patterns. And we’re just getting started — this is the foundation of a broader vision: where virtual consumer panel insights can be used to evaluate product characteristics, pricing and even marketing creatives and branding providing quantitative assessments that can be fed as priors in Bayesian models such as Media Mix Models.\n\nReimagining Product Innovation with Synthetic Consumers and Agentic Frameworks#\n\nAcross the CPG industry, product innovation is falling behind. It’s too slow, too fragmented, and too costly — and too often, it’s disconnected from what real consumers actually want.\n\nIn response, a growing number of startups and industry leaders have turned to synthetic consumers: AI-generated personas built to mirror real-world behaviors, preferences, and decision patterns. The promise? Faster, cheaper feedback — without the delays and limitations of traditional market research.\n\nBut at PyMC Labs, we asked a harder question: What would it take to not just accelerate product development, but fundamentally rethink it — combining agentic workflows, multimodal vision models, and validated insights grounded in our expertise in data-driven simulations?\n\nWith one of our major clients in the CPG space, we have developed the AI Innovation Lab — a new way to turn ideas into market-ready products.\n\nWhere Product Innovation Breaks#\n\nDespite billions invested in research, design, and marketing, product innovation in CPG remains flawed. Across every stage, teams face systemic barriers that make it hard to build products real consumers actually want.\n\nProduct decisions are often made without meaningful data: Too often, products are developed in the absence of real insight. Early-stage concepts rely on gut instinct or fragmentary trend reports, especially at smaller or mid-sized CPG companies. Even at the larger players, insights tend to be consolidated late — pulled from siloed sources, often after key decisions have been made. As a result, product development is guided more by internal alignment than by actual consumer behavior or unmet need.\n\nFeedback loops are stuck in silos: In traditional product innovation workflow, each team operates in sequence: product writes the brief, R&D formulates, marketing positions, and research validates — often weeks or even months later. This rigid, siloed process leaves little room for iteration or collaboration. Once decisions are made, they’re hard to reverse, and by the time feedback arrives, it rarely changes the outcome.\n\nDesign is treated as a downstream detail: Visual identity — from packaging to typography to color palette — is often finalized after core product decisions have already been made. By then, there’s little time, budget, or flexibility left to test whether the design actually resonates with consumers. Yet in categories like skincare, beverages, and personal care, packaging is not just a wrapper — it’s the first interaction, the first impression, and often the deciding factor. Despite this, most teams have no structured way to evaluate how visual choices influence perception, relevance, or shelf appeal.\n\nPricing relies on guesswork: In most workflows, pricing decisions come late, based on loose benchmarks. It's rarely tested or modeled early, even though price directly shapes demand, positioning, and market size. Without a way to simulate price elasticity up front, teams risk launching too high, too low — or simply off the mark.\n\nThe result? A process that’s resistant to change, and misaligned with the consumers it’s meant to serve. Fixing it requires more than speeding things up — it demands rethinking how products are imagined, evaluated, and brought to life.\n\nReal Value for High-Stakes Decisions#\n\nOur solution is designed to bring clarity and confidence to every strategic Product Development choice. From early ideation to go-to-market, it provides data-driven ideation and consumer insights that are both quantitative and actionable. The tools enhances:\n\nBrainstorming: Quickly explore hundreds of concept directions, grounded in trend data and internal insights. Get to better ideas, faster — with agent-led briefs and early feasibility checks that surface what’s worth pursuing.\n\nProduct Refinement: Test pricing and forecast market potential before launch. Understand how price, positioning, and packaging impact demand — and align confidently on go-to-market strategy.\n\nKey Capabilities of the AI Innovation Lab#\n\nThe AI Innovation Lab is not just a tool — it’s an end-to-end platform to dramatically speed up the product development lifecycle, from initial idea to market validation. The platform includes five main capabilities that streamline and accelerate every stage of product development:\n\nSmarter Product Briefs: The Lab generates product briefs informed by trend data, competitive analysis, and your own historical product performance — giving teams a clear, evidence-based foundation from day one.\n\nAI Expert Evaluation: Each product brief is reviewed by a panel of AI agents simulating expert perspectives — covering feasibility, formulation constraints, regulatory risks, and strategic fit. Based on the review, the agents suggest targeted improvements, enabling teams to refine concepts early.\n\nDesign refinement: Product visuals are iterated using advanced multimodal models. Teams can explore and refine images, colors, typography, and claims — making design an integral part of the development process, not an afterthought.\n\nSynthetic Consumer Testing: Access feedback in minutes, not weeks. The platform taps into synthetic consumer panels built to reflect real-world demographics and behavioral patterns — allowing rapid testing of uniqueness, appeal, relevance and purchasing consideration.\n\nMarket Simulation: Forecast market impact before launch. Tools for price sensitivity analysis, market sizing, and competitive dynamics help teams make data-driven decisions about positioning, pricing, and portfolio strategy.\n\nTogether, these capabilities form a unified system that helps teams move from idea to market with greater speed, precision, and confidence.\n\nFrom Prototype to Proof: Validating the Platform#\nOur robust methodology, validated with data from thousands or real consumers, guarantees that synthetic responses not only correlate with real consumer feedback but also accurately represent its distribution.\n\nYou might ask: this all sounds promising — but how do I know it really works in practice?\n\nThat's why we've validated our platform through real-world testing:\n\nChallenge\tSolution\nIdeating Realistic Product Concepts\tMultiagent Expert Collaboration\n\nLeveraging our expertise in building multi-agent systems (such as our MMM Insight Agent), we've developed a dynamic, domain-aware agent collaboration framework for product ideation. This solution enables seamless interactions between AI roles—product designer, marketing strategist, and sustainability expert—using self-feedback loops to efficiently generate realistic products with minimal user input.\nReproducing Human Feedback\tAdvanced Prompt Engineering\n\nWe developed an innovative prompting method that involves asking synthetic consumers exclusively open-ended questions. We then convert these qualitative responses into standardized quantitative metrics using embeddings. This allows the LLM to allocate more test time compute to answering the questions, resulting in smoother and more realistic response distributions.\n\nThis case study, highlighting how LLMs handle open-ended questions, provides insight into our method’s practical effectiveness.\nEvaluating Synthetic Consumer Quality\tWe conducted a large-scale replication study with a major international CPG brand, assessing synthetic consumer panels across hundreds of products and thousands of real consumer responses. Our evaluation focused on two key metrics:\n\nResponse Correlation: Using Pearson correlation, we measured how closely synthetic consumers matched real consumers in product rankings—often sufficient for early-stage concept testing and directional insights.\n\nDistribution Similarity: Employing the Kolmogorov–Smirnov (KS) test, we compared full response distributions of synthetic versus real consumers, uncovering variations that averages alone might overlook.\n\nWithin the oral-care category, our synthetic panel successfully replicated up to 90% of real consumer responses when asked about purchase intent.\nDeveloping a General-Purpose Synthetic Panel\tHarnessing Large-Scale Public Datasets\n\nWe’ve created representative synthetic panels for major markets by modeling target demographics based on publicly available datasets such as ANES and GSS.\n\nValidation against real-world benchmarks, including voting behavior and TV preferences in the US demonstrates strong performance, providing initial evidence that this general purpose panel can effectively support diverse advertisers and brands.\n\n \n\nNot every challenge is yet fully solved — and that’s exactly the point. We’re building forward, with real progress, tested methods, and a clear path toward even more robust, reliable product decision-making.\nLooking Ahead: One Integrated System#\n\nAt PyMC Labs, we’re extending the AI Innovation Lab in two key directions:\n\nCreative Testing – expanding the Lab’s capabilities to evaluate ads and campaign assets early in the development cycle.\n\nMarketing Integration – linking it with our MMM Insight Agent, built on top of the PyMC-Marketing toolbox, to optimize media spend with precision and transparency.\n\nTogether, these systems form a unified, iterative workflow:\n\nWhat should we launch?\nThe AI Innovation Lab supports product and creative development — testing concepts, messaging, and packaging with synthetic consumers.\n\nHow should we market it?\nThe MMM Insight Agent simulates the impact of different budget allocations, creatives, and channels — helping teams maximize return on spend.\n\nAgents in synergy: Creating a closed-loop learning system for marketing strategy.\n\nThe real power comes from closing the loop. Synthetic panels can help inform priors for Media Mix Model campaign coefficients, while in turn underperforming creatives or channels feed back into the Innovation Lab for refinement, creating a continuous cycle of testing, optimization, and learning.\n\nBy breaking down silos between consumer research and marketing, we help brands align what they build with how they sell — unlocking smarter product decisions and more efficient growth. Marketing is already PyMC Labs’ strength — and now we’re extending that intelligence across the full innovation lifecycle.\n\nReady to Accelerate Product Innovation?#\n\nThe AI Innovation Lab could be the breakthrough your team needs — combining synthetic consumers, agentic workflows, and multimodal vision models. Contact us today to explore how this end-to-end platform can help you make smarter, faster product decisions with confidence.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/causal-sales-analytics-are-my-sales-incremental-or-cannibalistic": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nIntroduction\nNo obvious off-the-shelf solution\nIsn’t this trivial?\nCausal thinking - beyond statistics\nA simple first model - multivariate interrupted time series\nNot so easy!\nThe promise and challenge of estimating incrementality and cannibalization\nCausal sales analytics: Are my sales incremental or cannibalistic?\n\nSeptember 19, 2024\n\nBy Benjamin Vincent\n\nExecutive summary: In today’s competitive and saturated consumer markets, companies struggle to gain market share through new product releases. Traditional methods of estimating where sales originate—whether they are incremental or cannibalistic—often fall short in complex, dynamic environments. Colgate-Palmolive approached PyMC Labs in 2023 to develop causal sales analytics software that could accurately quantify these aspects to guide product decision-making. This blog post delves into the intricacies of this problem and why standard solutions are insufficient. It explores the need for a bespoke approach incorporating causal and counterfactual thinking to understand sales impacts. By outlining an initial simple model and its limitations, the post sets the stage for the more sophisticated solution developed by PyMC Labs, which will be detailed in a subsequent post. This advanced approach aims to provide actionable insights for better product portfolio management, ultimately enhancing sales, market share, and profitability.\n\nIntroduction#\n\nImagine you are a product company in a competitive and saturated market - think consumer staples like food, beverages, and household products. It won’t be possible to get easy wins by appealing to brand new customers (i.e. growing the pie) because everyone is buying either your or your competitors’ products already. Instead, both you and your competitors have been vying for market share (i.e. slices of the pie) over time. In addition to marketing efforts, you have been trying to do this by releasing new products with attributes that you believe have higher appeal to customers or higher profit margins.\n\nOver time, the products available in the marketplace change - some products are classic and persist, others fall out of favor and are discontinued. And others are newly released with the hope of increasing your market share. So we have a complex and dynamic ecosystem of products. Each company makes decisions about when to discontinue a product, when to release new products, and what kinds of attributes they should have.\n\nSay you released a new product last year and it sold 100,000 units. How do you know where those sales came from? What we really want is for many of those sales to be incremental in that they took sales away from competitor products. But some proportion of those sales may have been cannibalistic in that they took sales away from your own existing products.\n\nYou could build a statistical (or black-box Machine Learning) solution which might give correlational insights such as “When you release a product of type X, you tend to see Y% innovation.” However, you are aware that while statistical solutions are exactly the right approach in many situations, they can lead to unexpected outcomes once you start acting and intervening. And intervening, in the form of releasing or not releasing new products, is exactly what you want to do.\n\nSo you need a causal model of how product sales are affected by product introductions and withdrawals. If this model is a close approximation of the real world, then we’d be better positioned to make predictions about counterfactual scenarios to understand what would have happened in alternative product launch scenarios.\n\nClearly, companies have been making decisions about product releases for a long time. Some of this will be guided by good old human intuition, and some will be informed by sales data or customer research. But does this leave money on the table? Can we squeeze more insights from the data? Can we get quantitative estimates of where a product’s sales are coming from - what portion of sales are incremental versus cannibalistic? If so, these insights could be used to more effectively guide future product portfolio decisions with the ultimate goal of improving KPIs such as sales volume, market share, profits, or profit margins.\n\nBack in 2023, Colgate-Palmolive approached us with exactly these questions. While it is possible to buy retail sales data, the data alone does not give us all the answers we want. They asked us to build advanced sales analytics software to estimate where a product’s sales were coming from in order to guide their product decision making. While the end goal was a Python package which their data scientists could use going forward, the project was very research-heavy. A follow up blog post will describe the actual solution we provided, but the rest of this blog post outlines the richness of the problem and why a simple but sophisticated solution was not up to the task.\n\nNo obvious off-the-shelf solution#\n\nBecause many businesses share the same basic set of problems, an ecosystem of data science and modeling approaches have been applied to deliver business insights. The table below gives a brief overview of some common business questions and the solutions that are commonly deployed:\n\nQuestion\tSolution\nWhat will happen to consumer demand in the near future?\tTime series forecasting tools\nWhich are my most effective advertising channels and how should I allocate marketing budgets?\tMedia Mix Modeling. Read more in our MMM blog post series (Part 1, Part 2, Part 3), and find out about our open source pymc-marketing package here.\nHow much should I spend to acquire a customer and still expect to profit?\tCustomer Lifetime Value modeling. Read more about our work on CLV models here, and find out about our open source pymc-marketing package here.\nHow should I price my product?\tPrice elasticity estimation and price optimization\nSales incrementality/cannibalization\t???\n\nHowever, when it comes to asking questions about which of my product sales are incremental or cannibalistic, or which product’s sales are affected by a new product, we don't have a well known go-to solution. There are of course some companies offering to provide such insights, but there is no transparency in which algorithms they are using or how effective or accurate they may be. So we needed to do our own research and build a bespoke solution. This is what we at PyMC Labs get out of bed for!\n\nIsn’t this trivial?#\n\nAt first glance, estimating incrementality could seem trivial - you could just look at how the sales of existing products decline when a new product is launched, and there is your answer. In the scenario below, the new product sales (red) mostly come from product C (blue) and a bit from product A (green), but not from product B (orange).\n\nIn an ultra-simplified scenario it is possible to establish where the sales of a new product came from. In this case, we can tell that most of the sales of the new product came from product C and some came from product A. But doing this involves assumptions which are perhaps only reasonable in highly simplified scenarios.\n\nHowever, this kind of “eyeball it” approach is only feasible in toy scenarios where you have a few products with easily predictable sales. While there may be a couple of real markets where this might be feasible, this is absolutely not the case for consumer packaged goods.\n\nCausal thinking - beyond statistics#\n\nOne of the key things we first identified was the need to go beyond statistical thinking and have causal and counterfactual thinking at the heart of the project. The whole concept of a new product’s sales being ‘taken’ from sales of existing products necessitates counterfactual thinking - It requires us to think about what the sales of the existing products would have been in the counterfactual scenario that a new product had not been released (see below).\n\nA ‘reasonable’ counterfactual scenario, showing what would have happened to the sales of products A, B, and C if the new product had never been launched (after the time shown by the dashed line). These counterfactual sales are reasonable because the assumption that the constant sales before the new product launch would have carried on being at the same level is very plausible.\n\nBy comparing the sales of the existing products in the counterfactual scenario to the sales in the ‘actual scenario’, and summing over the time period since introduction of the new product, we can estimate the total causal impact of the new product. And because we know which products belong to which companies, it is possible to estimate how many sales are incremental (coming from competitor products) and cannibalistic (coming from one’s own products).\n\nA simple first model - multivariate interrupted time series#\n\nIn order to underscore the complexity of the problem, we’ll walk through the construction of a simple but reasonable model. It might initially appear very elegant, and while it may be useful in some situations, it will not be able to serve our needs.\n\nLet’s start with a simple scenario, shown in the figure below. We have the total sales across your product line up, and the total sales of all products from all competitors*️⃣. Finally, we have the sales data for your own new product that was introduced at the point shown by the dashed line.\n\n*️⃣ While it may be surprising to those outside of the retail sector, it is possible to get data on competitor sales. There are companies which purchase sales information from retailers, aggregate, and sell that information on as a service.\n\nA schematic of a simple multivariate interrupted time series model with a highly simplified scenario.\n\nWhat we could do is to build a kind of multivariate interrupted time series model. We could model your and competitor products as normally distributed around some expected value with some level of variance, σ. This expected value would have an intercept parameter (γ ) which describes the sales before the new product introduction. Then, the sales after the product introduction could be modeled as that baseline level minus some proportion of the new product sales. The parameter  will define the extent of cannibalization. If c=0 then your sales are unaffected, if c=1 then all of the new product sales will have come from your own existing products. We can define incrementality as i=1-c; when i=0 we have no incrementality and it’s all cannibalization, and when i=1 we have pure incrementality.\n\nThis model could be seen as a multivariate interrupted time series approach. In an interrupted time series approach you would typically have a single (univariate) time series and the task is to infer if an intervention delivered at a particular point in time had a causal effect. This model is different firstly in that we have multiple outcome variables, namely both your and your competitor’s total sales. Secondly, because the ‘treatment’ is effectively ‘graded’ because the number of new product sales (the ‘treatment’) varies over time.\n\nBelow we plot the result of a simulation study where your company has 40% market share before the introduction of a new product. When the new product is launched, it is wildly successful, capturing 10% of the total market. The question is, where did those sales come from. In our simulation we made it such that there was 50% incrementality and 50% cannibalization, so you can see that both your and your competitors' sales decreased when your new product was released. The model does a good job and estimates a posterior distribution for the incrementality parameter centered almost exactly on 0.5, i.e. 50%.\n\nSimulated scenario where new product sales are 50% incremental. The model’s parameter estimate (right) shows a very reasonable estimate - 94% credible intervals from 49-59%.Simulated scenario where new product sales are 50% incremental. The model’s parameter estimate (right) shows a very reasonable estimate - 94% credible intervals from 49-59%.\n\nSo we’ve solved the problem?\n\nNot so easy!#\n\nAnyone working in the data science field knows that one of the major challenges is getting clean data to build your modeling work upon. One of our first steps was to familiarize ourselves with the nature of the sales data. We engaged in a thorough exploratory data analysis and worked with the client’s data scientists to make sure we understood the domain and the in’s and out’s of the retail datasets.\n\nThere are a number of important aspects about the data which add to the complexity of the problem. In contrast to the simplified scenario presented above, real-world sales data involve many more products, sales are not flat over time, and within any given timeframe we can have multiple products (perhaps from different companies) being released or withdrawn. So the real data can look more like what we show schematically in the figure below. All of this combines to make it much more complex to model and estimate counterfactual sales data. In short, the task before us was not a trivial one!\n\nReal sales datasets involve many more products (sometimes multiple hundreds), with fluctuating sales, and multiple product introductions or withdrawals happening in overlapping periods. Here we have a new product’s sales in red. The gray lines represent the actual sales of existing products. But to work out where the new product sales came from we need to estimate the counterfactual sales of the existing products if the new product had never been launched. Even with this slightly more complex sales data, it is no longer trivial to estimate what these counterfactual sales would have been in the absence of a new product launch.\n\nIn a marginally more complex scenario (below) we will see that our simple and elegant model fails. Here, a competitor releases a new product at t=50 which is 100% incremental for them, we can see your market share decrease and theirs increase. In response, you launch a new product at t=100 which is 100% incremental for you, resulting in a decrease in competitor market share and no change to sales of your own existing products. Despite this being a rather minor addition to the complexity of the scenario, our simple and elegant model fails. What actually happened was your product was 100% incremental, but the model’s estimate falls quite far short of this, estimating somewhere between 64-76% incrementality.\n\nThis simulated scenario sees a competitor release a new product at  which is 100% incremental (for them), which reduces your market share. In response, you launch a new product at  which is 100% incremental (for you), resulting in a decrease in competitor market share. The model’s parameter estimate (right) under this more complex scenario now fails and underestimates incrementality by quite some margin. A simple hack to fix this would be to shrink the pre-introduction window to exclude the new introduction at . Though we would pay a price in terms of less data and more uncertain estimates, especially when product introductions or withdrawals are very frequent. Alternatively, we could attempt to build a more advanced model of sales in the pre-introduction period.This simulated scenario sees a competitor release a new product at  which is 100% incremental (for them), which reduces your market share. In response, you launch a new product at  which is 100% incremental (for you), resulting in a decrease in competitor market share. The model’s parameter estimate (right) under this more complex scenario now fails and underestimates incrementality by quite some margin. A simple hack to fix this would be to shrink the pre-introduction window to exclude the new introduction at . Though we would pay a price in terms of less data and more uncertain estimates, especially when product introductions or withdrawals are very frequent. Alternatively, we could attempt to build a more advanced model of sales in the pre-introduction period.\n\nWe found out early on in the project that a simple model like the multivariate interrupted time series would not help us solve our problems. Deviating even slightly from the idealized situation has resulted in the model producing biased estimates of incrementality and cannibalization. The toothpaste market is large with product introductions and withdrawals happening all the time, so it would be impossible to crop the data with sufficient pre- and post- product introduction observations without contamination from other product introductions/withdrawals.\n\nThe promise and challenge of estimating incrementality and cannibalization#\n\nThe core goal of our project was to estimate where new products’ sales were coming from - how much of products’ sales are incremental versus cannibalistic? Providing these business insights is enormously valuable for making product development decisions in a global market estimated at $20.8 billion in 2023.\n\nWhile we do of course want to know the net incrementality/cannibalization, we also really want insights on a product level. That is, we don’t just want to know if sales are coming from your own or any of your competitors' products, we want to know which products’ sales are being impacted and by how much. Launching a new whitening product is much more likely to take sales from other whitening products as compared to other categories such as childrens or cavity protection. And so a solution will have to operate on the product level - a much more challenging proposition than the simplified scenarios shown above. In fact, we would really want to incorporate product attribute information into our model as this will be highly informative about where sales may be coming from. Doing this with a generic time-series type approach would be hard, which makes the model we’ve presented so far even less satisfactory.\n\nWhile we’ve talked about saturated markets, this characteristic may not always hold, and so our modeling solution must be robust to this. There are a few reasons for this. It is plausible to assume the toothpaste market is close to saturated in most wealthy countries. However, because there are so many toothpaste brands and products, it is not necessarily true that all toothpaste products enter into the retail sales data. This can pose a challenge - from a modeling perspective we must take into account that there may be a small “dark pool” of toothpaste products that customers may enter or leave at different points in time. Additionally, while many markets may be considered as saturated, the global “market is experiencing significant growth, driven by increasing global awareness of oral hygiene and the expanding middle class in emerging economies.” And so our modeling solution needs to be flexible for these different situations.\n\nHow much confidence should we place in our incrementality and cannibalization estimates? It is less and less acceptable to provide so-called point-estimates, where we have single-number estimates such as “12.4% incrementality.” Instead, if we are to base high-value decisions on business insights, then they need to carry levels of confidence so that decision makers can be informed. For us, this was not hard! PyMC Labs is Bayesian at heart and so virtually everything we do sits within the Bayesian framework. This allows us to attach Bayesian credible intervals to all of our insights so we know when we know and know when we don’t know.\n\nHopefully at this point you appreciate the value that accurate incrementality and cannibalization estimates can provide to companies regularly making product launch and withdrawal decisions. And hopefully we have conveyed that it is complex, and non-trivial to provide these estimates. We have shown a simple yet elegant solution (multivariate interrupted time series), but shown that this is not robust enough to use in many complex markets. In the next post we will outline the solution that we handed over. We believe that it does a very good job - and we hope you are excited to learn about it soon. And even better - we are going to open-source a form of the model in the near future. Stay tuned!\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/causalpy-a-new-package-for-bayesian-causal-inference-for-quasi-experiments": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nWhat is CausalPy?\nFeatures\nAn example\nCall to action\nFind out more\nCausalPy - causal inference for quasi-experiments\n\nNovember 17, 2022\n\nBy Benjamin Vincent\n\nPyMC Labs is happy to announce the new open source python package, CausalPy! This is a PyMC Labs supported project, driven by Benjamin Vincent.\n\nWhat is CausalPy?#\n\nCausal claims are best made when we analyse data from experiments (ideally randomized control trials). The randomization process allows you to claim that differences in a measured outcome are likely due to an experimentally allocated treatment, as opposed to some other confounding factor or difference between the test and control groups.\n\nBut experiments and randomisation of treatment can be expensive and sometimes impossible. Let's consider just two examples:\n\nIt is not possible to randomise exposure of individuals to linear TV advertising campaigns, but we still want to know the causal impact of the advertising campaign.\nIt is not possible to randomly expose households to proximity to fracking operations but we still want to know what the causal consequences upon health may be.\n\nQuasi-experimental methods have been developed so that (when certain assumptions are satisfied) we can still make causal claims in the absence of experimental randomisation across treatment units (e.g. people, households, countries, etc.).\n\nCausalPy aims to have a broad applicability to making causal claims across a range of quasi-experimental settings.\n\nFeatures#\n\nWhile CausalPy is still a beta release, it already has some great features. The focus of the package is to combine Bayesian inference with causal reasoning with PyMC models. However it also allows the use of traditional ordinary least squares methods via scikit-learn models.\n\nAt the moment we focus on the following quasi-experimental methods, but we may expand this in the future:\n\nSynthetic control: This is appropriate when you have multiple units, one of which is treated. You build a synthetic control as a weighted combination of the untreated units.\nInterrupted time series: This is appropriate when you have a single treated unit, and therefore a single time series, and do not have a set of untreated units.\nDifference in differences: This is appropriate when you have a single pre and post intervention measurement and have a treament and a control group.\nRegression discontinuity: Regression discontinuity designs are used when treatment is applied to units according to a cutoff on a running variable, which is typically not time. By looking for the presence of a discontinuity at the precise point of the treatment cutoff then we can make causal claims about the potential impact of the treatment.\nAn example#\n\nBy way of example, we can run a regression discontinuity analysis to examine the effects of reaching legal drinking age (in the USA) upon all cause mortality. With mortality rate data from (Carpenter & Dobkin, 2009) loaded into df, we can run the analysis like this:\n\nNote that the exact API may change in future.\n\nAnd by calling result.plot() we get a pretty nice output.\n\nThe results show a clear discontinuity in all cause mortality rate at age 21.\n\nIn this observational dataset, there was no random allocation to drinking or no drinking conditions. But the logic is that if we find a discontinuity (which we did) then on balance of probability, it is likely that the legal drinking age is causally responsible, as opposed to some other confounding variable.\n\nCall to action#\n\nWe want to share the package with you all at this very early point - CausalPy could be considered as being in beta stage. So we are interested in your thoughts on the repository, suggestions for features, or bug reports. Once we are at a more stable point in CausalPy's development then we will open it up for code contributions, but we are not yet at that point.\n\nFind out more#\n\nStay tuned for more information here on the PyMC Labs blog, or follow @pymc_labs or the lead developer (Benjamin Vincent; @inferencelab) on Twitter for further updates.\n\nAnd check out the package here:\n\nGitHub repository: pymc-labs/CausalPy Star the GitHub repo\nDocumentation: causalpy.readthedocs.io\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/from-uncertainty-to-insight-how-bayesian-data-science-can-transform-your-business": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nSo what is Bayesian Data Science?\nBayes’ Theorem\nA different way of thinking\nBenefits of Bayesian Data Science in Business?\nApplications of Bayesian Models\nDealing with “Small” Data Problems\nBayesian superpowers\nTechnological Advances\nThe Impact of Open-Source Technology and Innovations in Bayesian Analysis\nConclusion\nReferences\nFrom Uncertainty to Insight: How Bayesian Data Science Can Transform Your Business\n\nSeptember 25, 2023\n\nBy Tiaan Van Der Merwe\n\nYou have a business and you struggle to quantify the value of your marketing efforts. Some examples include:\n\nWhen purchasing new stock, you feel there are too many factors influencing demand to make an accurate prediction of future sales.\nYou have a model to make predictions about customer churn, but you have no way to reliably calculate uncertainty in your estimates.\nSometimes you may be able to collect data, but often it is too costly or time-consuming.\n\nEven if you are an expert and have a thorough understanding of the factors driving your business, you may lack a systematic way to incorporate your knowledge and make sensible predictions. It can be frustrating to have disparate sources of information without a unified framework to combine them and objectively answer your business questions. It's not surprising that businesses find it increasingly difficult to extract insights and keep up in a fast-changing market.\n\nTraditional statistical methods often struggle to handle the nuances and uncertainties associated with real-world data. Furthermore, information often comes from different sources with varying levels of granularity and reliability. Machine learning can be used to solve problems once you have enough data with relevant inputs. Deep learning, in particular, is adept at handling some of the so-called \"ill-posed\" problems encountered in computer vision and natural language processing. Somehow, these computational powerhouses do not seem to be applicable to your problem, despite their availability to more than just experts these days.\n\nEnter Bayesian data science, a powerful and flexible approach to data analysis and modeling. It allows you to incorporate your knowledge and assumptions about a specific problem and provides a mechanism to update your knowledge as new information arrives. This blog explores the benefits of Bayesian data science from a business perspective and explains why it's worth considering for your organisation. We'll also discuss how it differs from the typical machine-learning workflow.\n\nSo what is Bayesian Data Science?#\nBayes’ Theorem#\n\nYou may have heard of Bayes' formula. It is a mathematical equation that describes the optimal method for updating your existing knowledge when new information is obtained from data. The equation consists of three basic components, each with specific terminology associated with it. It is important to become familiar with this terminology because you will encounter it in most traditional and modern texts. Let's go through the basic idea using an example.\n\nIf you own a shop, you may want to determine the average amount that customers spend per basket. Your shop is well-established, and you already have a good idea of what the average basket spend is, although you have not made any calculations yet. That is, you have some prior knowledge about the problem. This knowledge helps you evaluate the relative plausibility of different hypotheses, such as different values for average basket spend, without necessarily relying on any data. You know the average spend is about 80 Glimmerbits (our currency for this story), and you don’t expect it to be much less than 70 Glimmerbits or any more than 90 Glimmerbits.\n\nYou decide to obtain some data by examining the basket sizes for the next day. (Let's ignore complexities related to time-of-day effects). This way you obtained 30 samples in a day.\n\nBayes’ formula can be used to obtain the so-called posterior, which is an updated representation of your prior knowledge. It is a blend of your prior knowledge and the information you learned from the data through the likelihood (the probability of observing the data, conditional on your prior knowledge or beliefs). The more data you have, the more the posterior leans towards what the data tells you and flushes out the prior knowledge. If you have extremely limited data, your prior knowledge takes the driver's seat until you obtain new information. In a future iteration, your posterior can become your new prior. However, you do not need not understand how this works but appreciate the notion of information updating that can be done in a systematic way.\n\nLet’s look at the result.\n\nThere is a high level of uncertainty in our estimates for the average basket size prior to obtaining any data. However, our estimate becomes more accurate after obtaining 30 data points. The dots at the bottom are the observed basket amounts.\n\nAt this point, we might need to specify what we mean by \"distribution\". It refers to the relative number of ways in which different outcomes can occur. So the grey-blue distribution captures our specification of what we believe plausible values for the average basket size can be, with our highest expectation around 80 Glimmerbits. The values of 70 and 90 are highly unlikely, and more extreme values are even less probable. After incorporating the information from Bayes’ formula, the posterior rapidly converged to the true mean of 82, with much less uncertainty than before.\n\nIf you didn't understand everything on the first read, don't become discouraged. These concepts are challenging, but they are necessary to appreciate the power of Bayesian modeling and understand its full potential in your business. We will build on this example to demonstrate how Bayesian modeling can excel in problems surrounded by more significant uncertainty.\n\nA different way of thinking#\n\nMachine learning models are often referred to as \"black box models\". These models learn a complex function from data to represent the information contained in it. Since this function is normally beyond human comprehension, it is difficult to interrogate such a model for anything other than its original purpose. You do not have a way to ask different questions about the data and use the same model to arrive at an answer.\n\nIn a typical machine learning workflow, you fit various models, perform parameter optimizations, and use standard procedures to select your best model, from which you can make predictions. However, in this process, you are not too concerned about how the data was generated. These models \"free\" you from making and encoding your assumptions upfront, but it comes at a cost. Data scientists often make the mistake of attempting to fit machine learning models as soon as they obtain data.\n\nH. Ford. AZ Quotes. Retrieved April, 2023, from https://www.azquotes.com/quote/99162.\n\nIn Bayesian modeling, you must constantly consider the process that generates the data. Your Bayesian model should closely reflect this process. Although this may seem daunting, there are statistical principles and techniques that can be used to specify widely different and complex data-generating processes. Even if you must make simplifying assumptions, you are bound to think about them before encoding them in the model.\n\nThe upfront effort required for Bayesian modeling can sometimes be a barrier to even attempting it and this is where most data scientists falter. However, the belief that a faster black box approach will give superior results should be dispelled. There are some basic principles that can be used to specify a scientific model from which you can derive the Bayesian statistical model. However, there is nothing automatic about it. Despite having modern tools to fit complex models, it is important to think about the problem and formulate it clearly. This requires serious consideration of the question you are trying to answer and the knowledge you have about the problem.\n\nIt is relatively simple to run code to fit a model to data and to obtain results relatively quickly, so it is unsurprising that most data scientists are reluctant to put in the hard work of thinking about the problem beforehand. In Bayesian modeling, you should think before you fit. However, after the initial effort of creating a model, you gain access to a plethora of tools to help you move from uncertainty to insight.\n\nIn addition to forcing us to approach our research questions differently, it provides a framework to help us deal more effectively with our biases and incorporate even vague knowledge that we sometimes need to elicit from our brains.\n\nAs the British statistician George Box once said, \"All models are wrong, but some are useful.\" Bayesian data science can certainly assist you in finding a useful model, but it can be quite complicated at first. This is where experts in Bayesian modeling can be extremely helpful. Companies like PyMC Labs provide consulting services to assist with this process.\n\nBenefits of Bayesian Data Science in Business?#\n\nBy incorporating prior knowledge and uncertainty into models, Bayesian methods can produce more accurate predictions and help businesses make better decisions. This is especially useful in situations where data is scarce or incomplete. This is a common scenario in many business problems, which machine learning models often fail to address.\n\nIt's hardly ever a good idea to make a list of the advantages of using Bayesian data science, because you risk leaving out so many examples. However, I will take the liberty of mentioning just a few. Bayesian data science provides a systematic and consistent framework that enables:\n\nA framework to incorporate prior knowledge and deal with incomplete information.\nQuantification of uncertainty in predictions.\nCausal inference and more explainability than most machine learning models.\n\nBy leveraging these advantages, businesses can improve decision-making and risk management leading to better overall outcomes, improved profitability and increased competitive advantage.\n\nApplications of Bayesian Models#\n\nBayesian models are employed in various domains, such as demand forecasting, fraud detection, personalised marketing with recommender systems, churn prediction, and risk management in a wide range of business contexts.\n\nInsurance companies use Bayesian methods to model the probability of different events, such as natural disasters or accidents and then determine the policy price accordingly. This approach helps them manage risk exposure and ensure adequate preparedness for potential losses.\n\nIn the healthcare industry, Bayesian methods can be utilised to design and analyse clinical trials, incorporating prior knowledge about aspects of the study design. Researchers can create more efficient and effective trials by incorporating information on different treatments and the prevalence of certain diseases. This accelerates the development of new drugs and therapies, ultimately improving patient outcomes.\n\nIn the field of marketing, Bayesian models can be used to estimate the effectiveness of marketing campaigns. Specifically, data on past customer behaviour can be collected and combined with a company's prior knowledge about customer behaviour to update its beliefs on the probability of a customer making a purchase after seeing a particular advertisement. This information can then be used to make decisions on how to allocate a company's marketing budget.\n\nPyMC Labs has a strong track record of success in the marketing space, having worked with some of the world's leading brands. They have even developed a product specifically designed to optimize your marketing strategy.\n\nClearly, Bayesian models have a wide range of applications across various domains, providing a flexible, but scientifically rigorous framework to tackle difficult problems.\n\nDealing with “Small” Data Problems#\n\nDespite living in a world of \"big data,\" where machine learning, particularly deep neural networks, is becoming increasingly accurate at making predictions as sample size increases, we often encounter a neglected subgroup of \"small data\" problems in business. Many business problems are hindered by a lack of data. Additionally, the scientific fields of medicine, anthropology, psychology, and others are typically constrained in the data they can collect due to ethical, financial, or practical reasons. For such problems, we cannot simply rely on mountains of data and apply machine learning algorithms. Instead, we must systematically incorporate domain knowledge into our models.\n\nBayesian modeling has the advantage of requiring little data to learn and reducing uncertainty about a particular outcome. Once you specify your model and incorporate all of your prior knowledge, you can generate different outcomes from the model and evaluate its validity. In a generative model, data is not even needed to begin with! Generated model outcomes can be tested against industry or domain expert knowledge, and adjustments can be made to produce more plausible outcomes. This is known as prior predictive modeling, an important step in the Bayesian workflow. As data is obtained, regardless of its size, the model can be updated to obtain a new posterior probability distribution.\n\nBayesian superpowers#\n\nBayesian models have a powerful feature called hierarchical modeling, which allows information to be shared among related data points. This sharing of information creates a form of memory within the model, enabling it to borrow knowledge from different groups. While it may come with conceptual and computational challenges, harnessing the hierarchical model greatly enhances the benefits of Bayesian analysis.\n\nHierarchical models possess a memory-like ability, generating expectations and facilitating learning when there are discrepancies between expectations and observations. This property allows them to adapt and improve their understanding [1].\n\nThe hierarchical structure of a problem arises from various factors. For instance, market conditions can influence all marketing channels similarly, causing them to perform better or worse together. However, it is essential to capture the individual variations of each channel, considering factors such as effectiveness and seasonal patterns. Bayesian models excel in handling diverse levels of information, making them ideal for marketing attribution in business [2].\n\nTechnological Advances#\nThe Impact of Open-Source Technology and Innovations in Bayesian Analysis#\n\nAlthough the theory of Bayesian analysis has been around for centuries, its practical application in business was limited by computational complexity. Fortunately, advancements in algorithms and the emergence of powerful open source frameworks with user-friendly interfaces have simplified the process of specifying and building these models, while managing computational complexity behind the scenes. Hardware advancements have also significantly accelerated processing speeds. Today, we can fit complex models in a fraction of the time it would have taken just a few years ago. Even on a regular PC, we can handle models of complexity that previously required specialised hardware and expertise.\n\nWhile it can be overwhelming to stay abreast of the latest innovations, it is crucial to acknowledge the remarkable opportunities they present. The rise of open-source technology has democratised access to powerful tools, benefiting scientific communities and the general public alike. This accessibility has contributed to enhanced productivity, optimised processes, and more precise predictions. By embracing these technologies, businesses can experience noticeable improvements within a very short timeframe.\n\nConclusion#\n\nBayesian modeling provides a scientifically rigorous framework to help humans think about their problems and learn from data. By incorporating prior knowledge and uncertainty into models, Bayesian methods can produce more accurate predictions and help businesses make better decisions, especially when data is scarce or incomplete. Bayesian modeling requires significant upfront thinking, but the rewards are worth the effort. With the latest advances in open-source technology, it is easier than ever to leverage the benefits of Bayesian data science and by adopting these methods, businesses can stay ahead of the competition and achieve better outcomes overall.\n\nReferences#\n\n[1] Mcelreath, R. (2016). Statistical rethinking: a Bayesian course with examples in R and Stan. Boca Raton: Crc Press/Taylor & Francis Group.\n\n[2] Solving real-world business problems with Bayesian modeling. (31st October 2022). PyMC Labs. Available at: https://www.pymc-labs.io/blog-posts/Thomas_PyData_London/ (Accessed: April 15, 2023).\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/write-me-a-pymc-model": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nHacking an agent to generate hallucination-free Bayesian models\nHow it works\nHow it’s going\nFuture Direction\nWorking with PyMC\nWrite Me a PyMC Model\n\nJuly 01, 2025\n\nBy Bernard (Ben) Mares, Allen Downey, Alexander Fengler\n\nHacking an agent to generate hallucination-free Bayesian models#\n\nWriting a PyMC model is not easy. It requires familiarity with both statistical distributions and Python libraries. And if at first you don’t succeed, debugging can be a challenge.\n\nLarge language models like GPT-4 are surprisingly capable at generating probabilistic models. You can give them a few sentences describing a Bayesian problem, and they will often respond with runnable PyMC code. But, as anyone who’s tried this knows, the quality of the results varies – and demonstrates a variety of failure modes.\n\nSometimes the code is out of date — using pymc3 imports or old syntax. Sometimes it violates best practices, and occasionally the model won’t even compile, thanks to hallucinated functions or subtle shape mismatches.\n\nWe wanted to fix that.\n\nSo, during our internal Probabilistic AI hackathon, we built ModelCraft — an LLM-powered modelling agent that doesn't just generate PyMC code, but also checks its own work. Before it returns anything to the user, it runs the model through a remote compiler, catches errors, and rewrites the model if necessary. The goal: make it easier for beginners and experts alike to go from modelling ideas to validated PyMC code — all through natural language.\n\nHow it works#\n\nModelCraft acts as a simple conversational agent that helps users write and validate PyMC models through natural language. You describe the model you want — the context, priors, data, whatever — and the agent responds with a complete PyMC model that’s been checked for correctness, at least in the sense that it compiles.\n\nBehind the scenes ModelCraft uses an LLM and LangGraph to generate candidate models and evaluate them using a secure code sandbox. This isn’t just code generation — the agent actually compiles each model before returning it, so you’re never stuck debugging a hallucinated pm.MagicUnicorn.\n\nWe also built a custom tool called compile_pymc_model that runs inside a remote E2B sandbox. This tool takes PyMC code, attempts to compile it (without sampling), and returns either a success message or a detailed error traceback. That output is routed back to the agent, which uses it to decide whether to revise the model or move on.\n\nThe agent is exposed through a FastAPI server with an OpenAPI interface, and we connected it to a frontend built with Onyx. The result is a fluid workflow where users can type modeling problems in plain language, and get structured, idiomatic PyMC models in return — already validated, and ready for sampling or refinement.\n\nHow it’s going#\n\nIn our demo, ModelCraft successfully tackled several medium-complexity modeling tasks — including problems drawn from Think Bayes. The agent was able to generate valid, modern PyMC models with good structure and style, and thanks to its built-in validation step, it avoided many of the common pitfalls we see when using LLMs for code generation.\n\nIn some cases, it took the agent multiple iterations to arrive at a model that compiled cleanly. This added some latency, but it also highlighted the value of the retry-and-validate loop — instead of handing off a broken model to the user, the agent caught its own mistakes and improved on them.\n\nWe found, as others have noted in the literature before, that subtle changes in the system prompt can make a big difference in the quality of the model produced . For example, the “lions and tigers and bears” problem from Think Bayes can be elegantly modeled with a Dirichlet distribution. However, when the system prompt included an instruction to use Python type annotations, most of the models produced involved a convoluted solution involving categorical distributions because the extra steps involved allowed for the use of type annotations. Model quality improved substantially after we removed this instruction.\n\n\n# Hypergeometric likelihood\nk = pm.HyperGeometric(\"k\", N=N, M=n1, n=n2, observed=k_observed)\n\n\nThis line generates an error because the keyword arguments of HyperGeometric are not correct.\n\nTypeError: HyperGeometric.dist() missing 1 required positional argument: 'k'\n\n\nWith this information, ModelCraft generates a second iteration that compiles and samples:\n\n\nimport pymc as pm  \n\nimport arviz as az\n\n# Observed data  \nn1 = 25  # bears tagged in first capture  \nn2 = 20  # bears captured in second capture  \nk_observed = 4  # recaptured bears that were tagged\n\n# Reasonable upper bound for the total population  \nN_max = 500\n\nwith pm.Model() as grizzly_model:  \n\n   # Prior over total population size N (must be >= n1 and n2)  \n   N = pm.DiscreteUniform(\"N\", lower=max(n1, n2), upper=N_max)\n\n   # Likelihood using hypergeometric distribution  \n   # args: k (obs), N, M (tagged), n (second capture size)  \n   \n   k = pm.HyperGeometric(\"k\", N, n1, n2, observed=k_observed)\n\n   # Sample from the posterior  \n   trace = pm.sample(3000, tune=1000, return_inferencedata=True)\n\n# Summarize posterior of N  \naz.summary(trace, var_names=[\"N\"])\n\n\nThis is not the only solution to the problem. In particular, we might want to iterate on the form of the prior distribution. But it helps to start with a correct model!\n\nFuture Direction#\n\nModel Craft was developed during an internal hackathon to help our team deliver client solutions more efficiently when coding complex Bayesian models. For a one-day project, we think it’s a compelling proof of concept: an AI modeling assistant that’s not just smart, but careful. The experience underscored the immense power we can unlock with today’s LLM orchestration tools—even with relatively minimal effort.\n\nGiven more time, we plan to extend Model Craft to support full model diagnostics, posterior visualization, and user-driven refinement loops. Another promising direction is wrapping this agent in an MCP server to communicate with Claude, Cursor, and other MCP clients. This will allow PyMC developers to use AI-powered assistant IDEs more effectively, all within their preferred coding environment.\n\nWe’ll soon be releasing a full MCP server for PyMC—so stay tuned!\n\nWorking with PyMC#\n\nIf you are interested in seeing what we at PyMC Labs can do for you, then please email info@pymc-labs.com. We work with companies at a variety of scales and with varying levels of existing modeling capacity. We also run corporate workshop training events and can provide sessions ranging from introduction to Bayes to more advanced topics.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/bayesian-vector-autoregression": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nA little bit of history\nBack to the present\nAutoregression models\nVector Autoregression models\nGoing Bayesian with BVAR\nVAR + PyMC = BVAR\nShall we do some armchair economics?\nEasier to forecast\nBayesian Vector Autoregression in PyMC\n\nJune 24, 2022\n\nBy Ricardo Vieira\n\nA little bit of history#\n\nIn the seminal 1993 paper \"Bayes regression with autoregressive errors: A Gibbs sampling approach\", Siddhartha Chib presented to economists the potential of Gibbs sampling in \"realistic settings that were previously considered intractable from a Bayesian perspective\".\n\nA cursory reading reveals a pattern that is quite characteristic of this early type of work. A time-series statistical model is first described, usually in a handful of equations —1 in this case—, followed by an intricate analysis of how the model joint-probability can be cut into conditionally independent blocks appropriate for Gibbs sampling. In that paper this amounts to no less than 9 equations! If one's model could not be easily cut into sample-able blocks, \"auxiliary\" variables could always be added as topping, until such cut was found.\n\nAs if this was not hard enough, early pioneers had to write their custom Gibbs sampling algorithms by hand, taking care to accommodate the existing computing limitations. Chib's paper makes mention of a powerful 33MHz machine. This process was somewhat alleviated by increasing compute power and by the advent of general purpose MCMC samplers like BUGS and JAGS, and later more powerful Hamiltonian Monte Carlo samplers like those found in Stan and PyMC.\n\nSurprisingly, many are still relying on custom Gibbs sampler algorithms to this day. Perhaps begging their joint probabilities to factor nicely or chasing the auxiliary variables needed for their latest model ideas. Needless to say, with PyMC, we won't be doing any of this!\n\nBack to the present#\n\nWe will implement a Vector Autoregression (VAR) model, a powerful time series tool, to examine and forecast complex dynamic relationships between variables. VAR models are routinely used by most macroeconomic and policy-making institutions, and have been increasingly adopted in other areas. Some interesting applications include:\n\nModel subject\tField\tPublications\nIndividual patient health from digital records\tMedicine\tA, B\nBehavioral activitity recognition from sensor data\tMachine Classification\tC\nCOVID risk interactions across age groups\tEpidemiology\tD\nGene expression regulatory networks\tBiology\tE\nSupply and demand of IT devices\tBusiness\tF\nShort-term changes in solar and wind power\tEnergy\tG, H\nNational and city level pollution\tEnvironment\tI, J, K\nExchange rate and inflation\tEconomics\tL\nEconomic effects of oil price shocks\tEconomics\tM\n\nTo understand how VAR works, it helps to first take a brief look at the simpler Autoregression models.\n\nAutoregression models#\n\nAutoregression models (AR) try to predict current observations as a linear combination of the past observations.\n\nAssuming that the most recent  values influence the current observation, the model is described by the following equation:\n\nSimulating data from an AR process is quite simple:\n\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\n\naz.style.use(\"arviz-darkgrid\")\nseed = sum(map(ord, \"PyMC LABS - BVAR\"))\nrng = np.random.default_rng(seed=seed)\n\ndef simulate_ar(intercept, coef1, coef2, noise=1, *, warmup=10, steps=200):   \n    # We sample some extra warmup steps, to let the AR process stabilize\n    draws = np.zeros(warmup+steps)\n    # Initialize first draws at intercept\n    draws[:2] = intercept\n    for step in range(2, warmup+steps):\n        draws[step] = (\n            intercept \n            + coef1 * draws[step-1]\n            + coef2 * draws[step-2]\n            + rng.normal(0, noise)\n        )\n    # Discard the warmup draws\n    return draws[warmup:]\n\n\nIn this first example, new values for  at time  depend only on the intercept and the previous value . This would be an autoregression model of first order or AR(1).\n\nar1 = simulate_ar(18, -0.8, 0)\n\n_, ax = plt.subplots(figsize=(8, 1.8))\nax.plot(ar1)\nax.set_xlabel(\"t\")\nax.set_ylabel(\"y\")\nax.set_title(\"$y_t = 18 - 0.8y_{t-1}$\");\n\n\nIn an AR(2), new values are influenced by the two preceding values.\n\nar2 = simulate_ar(8, 1.3, -0.7)\n\n_, ax = plt.subplots(figsize=(8, 1.8))\nax.plot(ar2, color=\"C1\")\nax.set_xlabel(\"t\")\nax.set_ylabel(\"x\")\nax.set_title(\"$x_t = 8 + 1.3y_{t-1} - 0.7y_{t-2}$\");\n\n\nAs the order of an AR model increases, so does the range of temporal dependencies and the richness of patterns that can be captured. On the other hand, the effect of each coefficient will be harder to determine, as each one plays a proportionally smaller role in the model outcomes.\n\nVector Autoregression models#\n\nVector autoregression models (VAR) are the multivariate generalization of AR. In a similar vein, they assume that a set of variables, observed at time , can be well predicted by a linear combination of the previous observations of that same set of variables.\n\nAssuming that only the most recent two values have an effect on the current observations, the VAR(2) model is described by:\n\nThis type of model allows for bidirectional effects among variables. The current values of  are not only affected by its past values, but also by the past values of . In addition, the past values of  also affect the current values of . This type of bidirectional effects make VAR models extremely flexible and powerful.\n\nLet us simulate data from a VAR(2) model:\n\ndef simulate_var(intercepts, coefs_yy, coefs_xy, coefs_xx, coefs_yx, noises=(1, 1), *, warmup=100, steps=200):\n    draws_y = np.zeros(warmup+steps)\n    draws_x = np.zeros(warmup+steps)\n    draws_y[:2] = intercepts[0]\n    draws_x[:2] = intercepts[1]\n    for step in range(2, warmup+steps):\n        draws_y[step] = (\n            intercepts[0]\n            + coefs_yy[0] * draws_y[step-1]\n            + coefs_yy[1] * draws_y[step-2]\n            + coefs_xy[0] * draws_x[step-1]\n            + coefs_xy[1] * draws_x[step-2]\n            + rng.normal(0, noises[0])\n        )\n        draws_x[step] = (\n            intercepts[1]\n            + coefs_xx[0] * draws_x[step-1]\n            + coefs_xx[1] * draws_x[step-2]\n            + coefs_yx[0] * draws_y[step-1]\n            + coefs_yx[1] * draws_y[step-2]\n            + rng.normal(0, noises[1])\n        )\n    return draws_y[warmup:], draws_x[warmup:]\n\nvar_y, var_x = simulate_var(\n    intercepts=(18, 8),\n    coefs_yy=(-0.8, 0),\n    coefs_xy=(0.9, 0),\n    coefs_xx=(1.3, -0.7),\n    coefs_yx=(-0.1, 0.3),\n)\n\n_, ax = plt.subplots(2, 1, figsize=(8, 4), sharex=True)\nax[0].plot(var_y)\nax[0].set_title(\"$y_t = 18 - 0.8y_{t-1} + 0.9x_{t-1}$\", fontsize=12)\nax[0].set_ylabel(\"y\")\n\nax[1].plot(var_x, color=\"C1\")\nax[1].set_title(\"$x_t = 8 + 1.3x_{t-1} - 0.7x_{t-2} - 0.1y_{t-1} + 0.3y_{t-2}$\", fontsize=12)\nax[1].set_ylabel(\"x\")\nax[1].set_xlabel(\"t\");\n\n\nUnlike the simulations in the previous section, the two time-series of  and  are now interdependent. Concretely,  depends on  and  depends on both  and . If the coefficients for the cross-effects were all set to zero, these effects would vanish and we would be observing two independent AR processes just like before.\n\nGoing Bayesian with BVAR#\n\nBayesian Vector Autoregression models (BVAR), are the Bayesian interpretation of vanilla VAR models. Parameter uncertainty is explicitly modeled and updated via the Bayesian rule, conditioned on observed data. Like most Bayesian models, there are no hidden assumptions or special conditions under which a statistical model can or cannot be used. What you see write is what you get.\n\nThis single letter difference gives a lot of power to BVAR models. Model parameters can be directly constrained by expert information. Relevant information from different datasets can be pooled together via hierarchical modelling. Different assumptions about the extent of lagged effects or noise terms can be easily changed and contrasted. It is also quite simple to extend and combine BVAR with other types of models. As with most Bayesian tools, imagination is the only limitation.\n\nWell, that and compute time...\n\nVAR + PyMC = BVAR#\n\nThe good news is that if you are using PyMC, your VAR model is necessarily a BVAR model! Let's see how easy it is to do it:\n\nTo honor the economic tradition of VARs, we will look at U.S. macroeconomic data for our example, and model GDP growth and Term spread between long- and short-term Treasury bonds. If you have no idea what that means... you are probably not an economist. Neither am I, so do not worry!\n\nThis analysis is heavily inspired by this chapter of Introduction to Econometrics with R. Another useful reading can be found in the respective chapter of Forecasting: Principles and Practice.\n\nPer usual, we must start with some scraping and data transformation...\n\nimport pandas as pd\n\n# 100% HDI that this link will go stale in the next 6 months = (0, 1)\nlink = \"https://www.princeton.edu/~mwatson/Stock-Watson_3u/Students/EE_Datasets/us_macro_quarterly.xlsx\"\ndf = pd.read_excel(link, storage_options={'User-Agent': 'Mozilla/5.0'}, index_col=0)\n\ndf.index = pd.to_datetime(\n    df.index\n    .str.replace(\":01\", \"-01-01\")\n    .str.replace(\":02\", \"-04-01\")\n    .str.replace(\":03\", \"-07-01\")\n    .str.replace(\":04\", \"-10-01\")\n)\n\ndf.rename(columns={\"GDPC96\": \"GDP\"}, inplace=True)\n\ndf = df[[\"GDP\", \"GS10\", \"TB3MS\"]]\n# This is an approximation of the annual growth rate\ndf[\"GDPGrowth\"] = 400 * np.log(df[\"GDP\"] / df[\"GDP\"].shift())\n# Term spread as the difference between 10-year and 3-month U.S. Treasury bonds\ndf[\"TSpread\"] = df[\"GS10\"] - df[\"TB3MS\"] \ndf = df[\"1981\":]\ndf\n\n\tGDP\tGS10\tTB3MS\tGDPGrowth\tTSpread\n1981-01-01\t6628.634\t12.960000\t14.390000\t8.200270\t-1.430000\n1981-04-01\t6580.199\t13.750000\t14.906667\t-2.933505\t-1.156667\n1981-07-01\t6655.692\t14.846667\t15.053333\t4.562976\t-0.206667\n1981-10-01\t6578.035\t14.086667\t11.750000\t-4.694544\t2.336667\n1982-01-01\t6468.003\t14.293333\t12.813333\t-6.747465\t1.480000\n...\t...\t...\t...\t...\t...\n2012-10-01\t15539.628\t1.706667\t0.086667\t0.145281\t1.620000\n2013-01-01\t15583.948\t1.950000\t0.086667\t1.139201\t1.863333\n2013-04-01\t15679.677\t1.996667\t0.050000\t2.449602\t1.946667\n2013-07-01\t15839.347\t2.710000\t0.033333\t4.052698\t2.676667\n2013-10-01\t15965.569\t2.746667\t0.063333\t3.174922\t2.683333\n\n132 rows × 5 columns\n\ndata = df[[\"GDPGrowth\", \"TSpread\"]][:-10]\ntest_data = df[[\"GDPGrowth\", \"TSpread\"]][-10:]\ndata.shape, test_data.shape\n\n((122, 2), (10, 2))\n\n+ Show Code\nfrom matplotlib.colors import ColorConverter\ncolors = (\"C0\", \"C1\")\n\ndark = {\n    \"C0\": tuple(c * .35 for c in (ColorConverter.to_rgb(\"C0\"))),\n    \"C1\": tuple(c * .35 for c in (ColorConverter.to_rgb(\"C1\"))),\n}\n\n_, ax = plt.subplots(2, 1, figsize=(8, 4), sharex=True)\nax[0].plot(data[\"GDPGrowth\"], color=dark[\"C0\"], label=\"train\")\nax[0].plot(test_data[\"GDPGrowth\"], color=\"C0\", label=\"test\")\nax[0].set_ylabel(\"quarterly change\", fontsize=10)\nax[0].set_title(\"GDP growth\")\n\nax[1].plot(data[\"TSpread\"], color=dark[\"C1\"], label=\"train\")\nax[1].plot(test_data[\"TSpread\"], color=\"C1\", label=\"test\")\nax[1].set_ylabel(\"quarterly change\", fontsize=10)\nax[1].set_title(\"Term spread\");\n\nfor axi in ax:\n    axi.axvline(df.index[-10], ls=\"--\", color=\"k\")\n    axi.legend(loc=(1, 0.5), fontsize=10)\n\n\nTime to model! We will consider a simple BVAR model of second order.\n\nimport pymc as pm\n\nlags = 2\n\ncoords={\n    \"lags\": reversed(range(-lags, 0)),\n    \"vars\": (\"GDPGrowth\", \"TSpread\"),\n    \"cross_vars\": (\"GDPGrowth\", \"TSpread\"),\n    \"time\": range(len(data) - lags),\n}\n\nwith pm.Model(coords=coords) as BVAR_model:\n    # The first B bit:\n    # Priors for the model intercept, lagged coefficients and noise terms\n    intercept = pm.Normal(\"intercept\", mu=0, sigma=1, dims=(\"vars\",))\n    lag_coefs = pm.Normal(\"lag_coefs\", mu=0, sigma=1, dims=(\"lags\", \"vars\", \"cross_vars\"))\n    noise = pm.HalfNormal(\"noise\", dims=(\"vars\",))\n    \n    # The VAR bit:\n    # Compute autoregressive expectation for each variable\n    # We convolve the lagged coefficients of each variable with the whole dataset\n    ar_GDPGrowth = pm.math.sum([\n        pm.math.sum(lag_coefs[i, 0] * data.values[lags-(i+1): -(i+1)], axis=-1)\n        for i in range(lags)\n    ], axis=0)        \n    ar_TSpread = pm.math.sum([\n        pm.math.sum(lag_coefs[i, 1] * data.values[lags-(i+1): -(i+1)], axis=-1)\n        for i in range(lags)\n    ], axis=0)   \n    \n    # Stack autoregressive terms and add intercepts\n    mean = intercept + pm.math.stack([ar_GDPGrowth, ar_TSpread], axis=-1)\n    \n    # The second B bit:\n    # For modelling simplicity, we ignore the first observations, where we could\n    # not observe the effects of all the lagged coefficients\n    obs = pm.Normal(\"obs\", mu=mean, sigma=noise, observed=data[lags:], dims=(\"time\", \"vars\"))\n\npm.model_to_graphviz(BVAR_model)\n\n\nwith BVAR_model:\n    trace = pm.sample(chains=4, random_seed=rng)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [intercept, lag_coefs, noise]\n\n 100.00% [8000/8000 00:16<00:00 Sampling 4 chains, 0 divergences]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 17 seconds.\n\naz.plot_trace(trace);\n\n\nShall we do some armchair economics?#\n\nSampling seems to have gone well, let's look at the results:\n\naz.plot_forest(trace, var_names=[\"intercept\", \"lag_coefs\"], combined=True, textsize=10)\nplt.axvline(0, color=\"k\", ls=\"--\")\nplt.title(\"Posterior 94% HDI\");\n\n\nWe can see GDP growth is positively correlated with growth in the two preceding quarters, [-1, GDPGrowth, GDPGrowth] and [-2, GDPGrowth, GDPGrowth], as the posteriors of both coefficients are positive and far from zero.\n\nTerm spread is strongly correlated with the term spread of the past quarter [-1, Tspread, Tspread] and slightly negatively correlated with that of two quarters past [-2, TSpread, TSpread].\n\nNow for the interesting part of VAR, we want to look at the cross-effects from one variable to the other. The model suggests past Term spreads may be strongly related to GDP growth, with [-1, GDPGrowth, TSpread], [-2, GDPGrowth, TSpread] having an absolute large mean. However, the uncertainty around these parameters is rather large.\n\nIn contrast, there seems to be almost no effect in the other direction, from GDP growth to term spread [-1, TSpread, GDPGrowth], [-2, TSpread, GDPGrowth], with posteriors of the coefficients confidently close to zero.\n\nEasier to forecast#\n\nLooking at numbers can only be so useful. What do they imply? Let's build a helper function to forecast future GDP growth and Term spread, according to our freshly inferred posterior parameters.\n\n+ Show Code\n# Function that takes a single draw of parameters and forecasts n steps\ndef _forecast(intercept, lag_coefs, noise, forecast=10):\n    len_data = len(data)\n    new_draws = np.zeros((data.shape[0]+forecast, data.shape[1]))\n    # Fill the new array with the observed data\n    new_draws[:len_data] = data[:]\n    for i in range(forecast):\n        ar_GDPGrowth = np.sum(lag_coefs[:, 0] * new_draws[len_data+i-lags: len_data+i])\n        ar_TSpread = np.sum(lag_coefs[:, 1] * new_draws[len_data+i-lags: len_data+i])\n        mean = intercept + np.stack([ar_GDPGrowth, ar_TSpread])\n        new_draws[len_data+i] = rng.normal(mean, noise)\n    # Replace all observed data with nan, so they don't show when we plot it\n    new_draws[:-forecast-1] = np.nan\n    return new_draws\n\n# Vectorized forecast function to handle multiple parameter draws\nforecast = np.vectorize(\n    _forecast,\n    signature=(\"(v),(l,v,v),(v)->(o,v)\"),\n    excluded=(\"forecast\",),\n)\n\n# Take a random subset of 100 draws from the posterior\ndraws = rng.integers(4000, size=100)\n\npost = trace.posterior.stack(sample=(\"chain\", \"draw\"))\nintercept_draws = post[\"intercept\"].values.T[draws]\n\nlag_coefs_draws = post[\"lag_coefs\"].values.T[draws].T\nlag_coefs_draws = np.moveaxis(lag_coefs_draws, -1, 0)\n\nnoise_draws = post[\"noise\"].values.T[draws]\n\nintercept_draws.shape, lag_coefs_draws.shape, noise_draws.shape\n\n((100, 2), (100, 2, 2, 2), (100, 2))\n\n# Forecast 10 quarters into the future\nppc_draws = forecast(intercept_draws, lag_coefs_draws, noise_draws, forecast=10)\nppc_draws = np.swapaxes(ppc_draws, 0, 1)\n\n_, ax = plt.subplots(2, 1, sharex=True)\n\nax[0].set_title(\"GDP Growth (rescaled)\")\nax[0].plot(df.index, ppc_draws[..., 0], color=\"C0\", alpha=0.05)\nax[0].plot(df[\"GDPGrowth\"], color=\"k\", label=\"observed\")\nax[0].plot([], color=\"C0\", label=\"forecast\")\n\nax[1].set_title(\"Term spread\")\nax[1].plot(df.index, ppc_draws[..., 1], color=\"C1\", alpha=0.05)\nax[1].plot(df[\"TSpread\"], color=\"k\", label=\"observed\")\nax[1].plot([], color=\"C1\", label=\"forecast\")\n\n\nfor axi in ax:\n    axi.axvline(test_data.index[0], ls=\"--\", color=\"k\")\n    axi.legend(fontsize=10, loc=(1, .4))\n    axi.set_ylabel(\"quartely change\", fontsize=12)\n\n\n\nThat will be all for today. We hope you enjoyed the basics of Bayesian Vector Autoregression in PyMC.\n\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/can-llms-play": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nThe Showcase Experiment\nHow It Works\nContestants and Rounds\nMetrics\nResults\nSome models are surprisingly accurate, and some are much worse\nSome models are more strategic than others\nWinning is a combination of accuracy and strategy\nOpenAI dominates the top of the leaderboard\nWhy This Matters\nWork with PyMC Labs\nCan LLMs play The Price is Right?\n\nJuly 23, 2025\n\nBy Allen Downey\n\nSynthetic consumers—LLMs simulating human survey participants—are becoming a powerful tool for marketing and behavioral research. They promise faster iteration, lower costs, and broader flexibility than traditional panels. But for them to be useful, they need not only to sound realistic, but also to demonstrate some level of real-world reasoning.\n\nA core question in this space: do LLMs “understand” prices? That is, can they recognize how much everyday items cost, and make decisions based on that understanding?\n\nTo explore this, we built a synthetic version of the Showcase game from The Price is Right, a game show where contestants try to estimate the value of consumer products. The result is a lightweight but surprisingly informative benchmark: a test of both knowledge and reasoning under constraints.\n\nThe Showcase Experiment#\n\nIn the TV version of The Price is Right, contestants bid on lavish collections of prizes—cars, appliances, and tropical vacations. In our version, the prizes are more mundane: consumer packaged goods (CPGs) like toothpaste, snack bars, and household cleaners. Instead of $30,000 showrooms, the total value is usually around $20.\n\nThe rules are simple:\n\nTwo LLMs (contestants) see the same showcase of items.\n\nThey each bid on the total value of the showcase.\n\nThe closest bid without going over wins.\n\nIf both overbid, neither wins.\n\nWe use this setup to probe several model abilities:\n\nEstimating real-world prices of common goods.\n\nUsing reference examples to calibrate those estimates.\n\nStrategically adjusting estimates to avoid disqualification.\n\nHow It Works#\n\nEach “round” of the tournament follows this format:\n\nGenerate a random showcase of three CPG items from our dataset.\n\nProvide 10 example prices from similar products to calibrate the models.\n\nSend the same prompt to two models, including the showcase and example prices.\n\nParse the bid and rationale, enforcing strict formatting and short responses.\n\nCompare bids to the actual retail price, determine the winner, and update metrics.\n\nHere's what the prompt looks like (excerpted):\n\nYou are a contestant on The Price is Right.\nGOAL: Bid as close as possible to the total retail price WITHOUT GOING OVER. CRITICAL RULE: If your bid is over the actual price, you lose. GOAL: Bid as close as possible to the total retail price WITHOUT GOING OVER. CRITICAL RULE: If your bid is over the actual price, you lose. Showcase Items:\n{showcase.description} Example Prices:\n{showcase.example_prices} CRITICAL: You must respond with ONLY a single JSON object in this format:\n{\"bid\": 1234.56, \"rationale\": \"Brief explanation...\"}\n\nIncluding the format in the prompt discourages verbose outputs and facilitates parsing – even though not all models follow the instructions.\n\nContestants and Rounds#\n\nWe started with a broad field of 90 models—everything from foundation models to smaller instruction-tuned variants. We ran two preliminary rounds to eliminate models that were clearly not in the running, and to refine our prompt to elicit better responses. During each round, every model saw the same 20 showcases, and we computed the mean absolute percentage error (MAPE) of their bids, ignoring overbidding. In the first round, most versions of Gemini Flash were eliminated, along with several versions of Llama and DeekSeek R1. In the second round, we lost more of the same, and a few GPT minis. The top 50 models (lowest MAPE) moved on to the finals.\n\nIn the finals, we ran 50 showcases. For each showcase, we\n\nPaired the contestants at random,\nSolicited bids from each pair, and\nRecorded the bids, whether or not the contestants overbid, and who won.\nMetrics#\n\nWe evaluate the models along several dimensions:\n\nMean Absolute Percentage Error (MAPE)\nMeasures how close each bid was to the actual price, ignoring overbids. Lower is better.\nOverbid rate\nThe percentage of bids that exceed the actual price—resulting in automatic loss. A high overbid rate suggests poor calibration or lack of strategic conservatism.\nWin rate and Elo rating\nTracks direct wins and losses across matchups using an Elo rating system. A model that consistently outbids (without overbidding) its peers rises in the rankings.\n\nThese metrics reflect both accuracy and adaptiveness—some models may have low MAPE but lose frequently due to risky bidding.\n\nResults#\nSome models are surprisingly accurate, and some are much worse#\n\nHere are the top 10 models by MAPE:\n\nRank\tModel\tRating\t# wins\tMAPE\tOverbid %\n1\to3\t1025.8\t22\t13.5\t42.0\n2\to1-2024-12-17\t1220.1\t22\t14.6\t44.0\n3\to3-2025-04-16\t922.1\t18\t14.6\t44.0\n4\to1\t1092.2\t21\t14.9\t42.0\n5\tgpt-4.1\t1285.2\t29\t17.9\t22.0\n6\to1-preview-2024-09-12\t889.0\t17\t18.7\t46.0\n7\tgpt-4.1-2025-04-14\t1103.0\t24\t18.7\t24.0\n8\to3-mini\t1070.7\t21\t19.4\t38.0\n9\tgpt-4.5-preview\t1072.4\t23\t20.3\t28.0\n10\to1-preview\t1147.5\t23\t20.6\t51.0\n11\to3-mini-2025-01-31\t1003.9\t23\t20.6\t30.0\n12\tclaude-3-5-sonnet-20241022\t1058.9\t21\t20.7\t42.0\n13\tgpt-4o\t1303.9\t29\t21.1\t22.0\n14\tclaude-3-7-sonnet-20250219\t1021.6\t20\t21.1\t50.0\n15\tgpt-4\t1101.6\t24\t21.3\t28.0\n16\tgpt-4o-2024-05-13\t1164.7\t28\t22.1\t20.0\n17\tgpt-4-0613\t1126.6\t23\t22.2\t36.0\n18\tgpt-4o-2024-08-06\t992.3\t20\t22.3\t30.0\n19\to4-mini\t892.4\t16\t22.9\t54.0\n20\tclaude-opus-4-20250514\t719.9\t14\t23.4\t32.0\n21\to4-mini-2025-04-16\t960.3\t16\t23.4\t60.0\n22\tgpt-4.5-preview-2025-02-27\t1113.6\t23\t23.6\t20.0\n23\tclaude-3-5-sonnet-20240620\t949.8\t19\t24.0\t46.0\n24\tclaude-sonnet-4-20250514\t1012.2\t21\t24.1\t34.0\n25\tqwen2-vl-72b-instruct\t803.4\t16\t25.0\t38.0\n26\tgpt-4o-search-preview-2025-03-11\t876.8\t16\t25.6\t54.0\n27\tchatgpt-4o-latest\t1209.6\t31\t25.9\t2.0\n28\tgpt-4o-search-preview\t1154.4\t26\t25.9\t36.0\n29\tllama-v3p3-70b-instruct\t1068.7\t21\t26.7\t42.0\n30\tgpt-4o-2024-11-20\t1151.3\t27\t26.8\t12.0\n31\tllama-3.3-70b-versatile\t1071.6\t21\t27.1\t40.0\n32\tgpt-3.5-turbo-1106\t1156.3\t26\t27.3\t28.0\n33\tllama3-70b-8192\t949.6\t20\t27.4\t38.0\n34\tgemini-1.5-pro\t1072.8\t21\t27.6\t34.0\n35\tllama-v3p1-405b-instruct\t860.0\t16\t27.7\t44.0\n36\tclaude-3-opus-20240229\t930.0\t20\t28.5\t16.0\n37\tgemini-1.5-pro-002\t1025.1\t21\t29.1\t32.0\n38\tgpt-4.1-mini-2025-04-14\t985.1\t21\t30.9\t22.0\n39\tcompound-beta\t874.4\t16\t31.0\t43.8\n40\tgemini-1.5-flash-8b-001\t962.4\t19\t31.2\t42.0\n41\tgemini-1.5-flash-8b\t935.0\t15\t31.7\t48.0\n42\tqwen3-30b-a3b\t844.8\t17\t31.9\t20.0\n43\tgpt-4-turbo\t858.4\t14\t32.2\t68.0\n44\tgpt-4-turbo-2024-04-09\t696.6\t11\t32.7\t72.0\n45\tgpt-3.5-turbo\t884.3\t19\t34.7\t12.0\n46\tgpt-3.5-turbo-16k\t993.9\t23\t34.9\t14.0\n47\tgpt-3.5-turbo-0125\t897.2\t20\t35.9\t10.0\n48\to1-mini\t981.3\t15\t39.6\t56.0\n49\tqwen-qwq-32b\t815.4\t16\t41.1\t38.0\n50\tllama4-scout-instruct-basic\t692.0\t10\t53.1\t70.0\n\n\n\nThe lowest mean absolute percentage error (MAPE) is 14%, which is better than the performance of human contestants on the show, about 18% (see Chapter 9 of Think Bayes). The highest MAPE is 53%, barely better than guessing without looking at the showcase.\n\nThe following scatterplot shows the actual values of the showcases and the bids from the best and worst model.\n\nThe bids from OpenAI o3 track the actual values closely, with only one notable miss on the highest-valued showcase. The correlation of bids and actual values is 0.89. In contrast, the bids from Llama4 are unrelated to the actual values – the correlation is effectively 0.\n\nSome models are more strategic than others#\n\nIn the previous plot, it looks like o3 is bidding strategically — more bids are below the actual value than above (58%). But some models are substantially better at avoiding overbidding. The most conservative model overbids only 2% of the time. The most aggressive model overbids 72% of the time. For comparison, contestants on the show overbid about 25% of the time.\n\nThe following scatter plot shows what the bid patterns look like for the models with the highest and lowest overbid percentages:\n\nFor both models the bids are well correlated with the actual values (0.69 and 0.90) but in one case they are consistently high and in the other case consistently low.\n\nWinning is a combination of accuracy and strategy#\n\nHere are the top models sorted by Elo rating:\n\nRank\tModel\tRating\t# wins\tMAPE\tOverbid %\n1\tgpt-4o\t1303.9\t29\t21.1\t22.0\n2\tgpt-4.1\t1285.2\t29\t17.9\t22.0\n3\to1-2024-12-17\t1220.1\t22\t14.6\t44.0\n4\tchatgpt-4o-latest\t1209.6\t31\t25.9\t2.0\n5\tgpt-4o-2024-05-13\t1164.7\t28\t22.1\t20.0\n6\tgpt-3.5-turbo-1106\t1156.3\t26\t27.3\t28.0\n7\tgpt-4o-search-preview\t1154.4\t26\t25.9\t36.0\n8\tgpt-4o-2024-11-20\t1151.3\t27\t26.8\t12.0\n9\to1-preview\t1147.5\t23\t20.6\t51.0\n10\tgpt-4-0613\t1126.6\t23\t22.2\t36.0\n11\tgpt-4.5-preview-2025-02-27\t1113.6\t23\t23.6\t20.0\n12\tgpt-4.1-2025-04-14\t1103.0\t24\t18.7\t24.0\n13\tgpt-4\t1101.6\t24\t21.3\t28.0\n14\to1\t1092.2\t21\t14.9\t42.0\n15\tgemini-1.5-pro\t1072.8\t21\t27.6\t34.0\n16\tgpt-4.5-preview\t1072.4\t23\t20.3\t28.0\n17\tllama-3.3-70b-versatile\t1071.6\t21\t27.1\t40.0\n18\to3-mini\t1070.7\t21\t19.4\t38.0\n19\tllama-v3p3-70b-instruct\t1068.7\t21\t26.7\t42.0\n20\tclaude-3-5-sonnet-20241022\t1058.9\t21\t20.7\t42.0\n21\to3\t1025.8\t22\t13.5\t42.0\n22\tgemini-1.5-pro-002\t1025.1\t21\t29.1\t32.0\n23\tclaude-3-7-sonnet-20250219\t1021.6\t20\t21.1\t50.0\n24\tclaude-sonnet-4-20250514\t1012.2\t21\t24.1\t34.0\n25\to3-mini-2025-01-31\t1003.9\t23\t20.6\t30.0\n26\tgpt-3.5-turbo-16k\t993.9\t23\t34.9\t14.0\n27\tgpt-4o-2024-08-06\t992.3\t20\t22.3\t30.0\n28\tgpt-4.1-mini-2025-04-14\t985.1\t21\t30.9\t22.0\n29\to1-mini\t981.3\t15\t39.6\t56.0\n30\tgemini-1.5-flash-8b-001\t962.4\t19\t31.2\t42.0\n31\to4-mini-2025-04-16\t960.3\t16\t23.4\t60.0\n32\tclaude-3-5-sonnet-20240620\t949.8\t19\t24.0\t46.0\n33\tllama3-70b-8192\t949.6\t20\t27.4\t38.0\n34\tgemini-1.5-flash-8b\t935.0\t15\t31.7\t48.0\n35\tclaude-3-opus-20240229\t930.0\t20\t28.5\t16.0\n36\to3-2025-04-16\t922.1\t18\t14.6\t44.0\n37\tgpt-3.5-turbo-0125\t897.2\t20\t35.9\t10.0\n38\to4-mini\t892.4\t16\t22.9\t54.0\n39\to1-preview-2024-09-12\t889.0\t17\t18.7\t46.0\n40\tgpt-3.5-turbo\t884.3\t19\t34.7\t12.0\n41\tgpt-4o-search-preview-2025-03-11\t876.8\t16\t25.6\t54.0\n42\tcompound-beta\t874.4\t16\t31.0\t43.8\n43\tllama-v3p1-405b-instruct\t860.0\t16\t27.7\t44.0\n44\tgpt-4-turbo\t858.4\t14\t32.2\t68.0\n45\tqwen3-30b-a3b\t844.8\t17\t31.9\t20.0\n46\tqwen-qwq-32b\t815.4\t16\t41.1\t38.0\n47\tqwen2-vl-72b-instruct\t803.4\t16\t25.0\t38.0\n48\tclaude-opus-4-20250514\t719.9\t14\t23.4\t32.0\n49\tgpt-4-turbo-2024-04-09\t696.6\t11\t32.7\t72.0\n50\tllama4-scout-instruct-basic\t692.0\t10\t53.1\t70.0\n\n\n\n\n\nThe models with the most wins and the highest rankings are the ones that balance accuracy and strategy. OpenAI o3, which has the lowest MAPE, is ranked only #21 out of 50 because it overbids too often. Most top models have MAPE near 20% and overbid percentages less than 30% – although there are a few top performers that deviate from this strategy.\n\nThe top models are substantially better than the worst. In the Elo model, we expect the best model, with rating 1210, to beat the worst, with rating 845, about 90% of the time.\n\nOpenAI dominates the top of the leaderboard#\n\nOpenAI models took the top 14 spots, and 16 of the top 20. Granted, part of this success is that they started with the most models (42 out of 90). And a few of them ended up near the bottom as well. But at least part of their success is earned – out of 42 models, 31 made it through the preliminary rounds.\n\nThe scatterplot below shows MAPE and overbid percentages for all 50 models, with each point colored by the backend used to access the model. OpenAI provides GPT-series models and related variants. Anthropic develops and hosts the Claude family of models. The Gemini backend provides Google’s Gemini models. Groq hosts high-speed inference for open or licensed models from other developers, including Meta’s LLaMA, Mistral, and Qwen. Similarly, Fireworks provides open-source or community-developed models including some from Meta, Mistral, and DeepSeek.\n\nThe best models are in the lower-left corner, with low MAPE and low overbid percentages. And we can see that this corner is populated entirely with OpenAI models. However, of the models that made it to the finals, the worst are also from OpenAI.\n\nWhy This Matters#\n\nAlthough this benchmark is light-hearted, it reflects a serious capability: using background knowledge and context to make constrained real-world decisions. This kind of reasoning underpins tasks like:\n\nEstimating costs in consumer surveys\nRecommending products within budgets\nSimulating realistic user behavior in test environments\n\nBy turning a game into a benchmark, we get a structured, repeatable way to measure models' real-world sensibility—not just their ability to talk about it.\n\nRelated articles:\n\nSynthetic Consumers: The Promise, The Reality, and The Future\nHow Realistic Are Synthetic Consumers?\nCan Synthetic Consumers Answer Open-Ended Questions?\nCurious how GenAI powers the future of Bayesian MMM and consumer testing? → Watch our talk\nWork with PyMC Labs#\n\nIf you are interested in seeing what we at PyMC Labs can do for you, then please email info@pymc-labs.com. We work with companies at a variety of scales and with varying levels of existing modeling capacity. We also run corporate workshop training events and can provide sessions ranging from introduction to Bayes to more advanced topics.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/expert-access-program": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nTLDR: You can skip ahead and book a call with our experts to learn more 👉\nThe Challenge: When Expertise Matters Most\nIntroducing the Expert Access Program\nWhy PyMC Labs: From Open Source to Industry Partner\nHow the Expert Access Program Works\nBase Package: Your Expert Lifeline\nPro Package: Deep Partnership and Strategic Guidance\nWhat Our Clients Say\nVoices from the Team\nShaping the Future of Measurement Together\nAnnouncing the Expert Access Program (EAP)\n\nAugust 26, 2025\n\nBy PyMC Labs\n\nTLDR: You can skip ahead and book a call with our experts to learn more 👉#\nThe Challenge: When Expertise Matters Most#\n\nImagine your team has to build a comprehensive in-house marketing measurement framework. It needs to be ready, robust, and credible in time for your company's Annual Business Planning cycle. The deadline is not negotiable, and the quality of the framework will shape budget decisions for the year ahead.\n\nOr picture another scenario: your team must pass an internal review set by your parent company. Only if you can demonstrate that your models are rigorous and production-ready will you be allowed to own analytics in-house, rather than having measurement dictated by third-party consultancies.\n\nThese are not hypotheticals. They are real situations our clients have faced.\n\nMany organizations recognize the value of advanced models, but they struggle to get the right expertise at the right time. Hiring consultants for one-off projects delivers short-term gains, but when the project ends, so does the support. Expanding the team with new employees can be just as difficult and expensive, especially when you already have a capable data science group in place. What these teams need is not more headcount or one-off advice, but a way to access expert guidance exactly when challenges arise.\n\nIntroducing the Expert Access Program#\n\nSince our founding in 2020, PyMC Labs has focused on two core pillars. The first is building best-in-class bespoke models for global clients, delivering insights that drive meaningful business outcomes. One of the most notable examples of this work is our partnership with HelloFresh. The second pillar has always been knowledge transfer. From the beginning, we believed that the models we deliver should not sit idle, but actively support clients in making informed business decisions. That is why we pair bespoke model builds with workshops designed to upskill client teams, giving them the confidence and expertise to fully leverage the tools in front of them.\n\nBut as our client base grew, something new became clear. While projects and workshops gave teams a strong foundation, they increasingly asked for more: ongoing access to our expertise. They wanted guidance not just at the outset of a project, but as their teams engaged daily with our open source frameworks, experimented with new approaches, and encountered fresh challenges.\n\nIn response, we created the Expert Access Program (EAP). It provides a structured way for clients to stay connected with our experts, receive timely guidance, and make sure their investment in advanced models continues to deliver value long after the initial build. With EAP, teams accelerate their progress, avoid common pitfalls, and stay at the cutting edge of modern analytics practices.\n\nWhy PyMC Labs: From Open Source to Industry Partner#\n\nOur story begins in open source software, but today we sit at the table with CMOs and strategy leaders. PyMC Labs emerged from the core team behind PyMC, one of the world's most widely-used probabilistic modeling frameworks, trusted by thousands of organizations globally. As PyMC matured and attracted a vibrant community of researchers and developers, it established itself as the foundation for modern probabilistic programming in Python.\n\nPyMC Labs was created to take this open source innovation and make it directly useful for industry. Since then, our work has expanded into specialized tools like PyMC-Marketing, which brings proven approaches for media mix modeling, customer lifetime value, and experimentation into production-ready libraries. Alongside it, we built CausalPy, a toolkit for causal inference and uplift modeling. These frameworks have quickly become the backbone for many data science teams, with PyMC-Marketing now used by thousands of practitioners and trusted by enterprise teams as part of their analytics infrastructure.\n\nIn this way, PyMC Labs has grown from an open source project into a partner for companies seeking to turn cutting-edge statistical research into practical, decision-driving tools.\n\nHow the Expert Access Program Works#\n\nThe Expert Access Program was created to meet the need for ongoing expertise access. Instead of leaving teams to figure things out alone, the program provides continuity through two structured tiers:\n\nBase Package: Your Expert Lifeline#\n\nThe foundation of EAP ensures your team never gets stuck. When you hit a roadblock - whether it's interpreting model diagnostics, debugging sampling issues, or deciding between modeling approaches- you have direct access to our experts through a dedicated communication channel.\n\nWhat's included:\n\nDirect expert communication channel with priority response within one business day\nAccess to our growing library of implementation guides and best practices\nPro Package: Deep Partnership and Strategic Guidance#\n\nFor organizations with higher stakes - those facing critical deadlines, internal reviews, or complex implementation challenges - the Pro package adds structured, proactive support.\n\nEverything in Base Package, plus:\n\nBi-weekly coaching calls with dedicated experts matching to your domain and challenges\nBi-monthly Expert Exchange Sessions featuring case studies, emerging methods, and industry trends\nPriority access to new PyMC Labs tools and early feature previews\nCustom workshop development for your specific modeling challenges\nStrategic consultation on measurement frameworks, analytics roadmaps and stakeholder gaining stakeholder buy-in\n\nBoth tiers give you what traditional consulting can't: continuity. Instead of starting from scratch each time you face a new challenge, you build on an ongoing relationship with experts who understand your context, your data, and your goals.\n\nWhat Our Clients Say#\n\nHearing directly from clients shows the tangible impact of our work and how the Expert Access Program builds on trusted partnerships:\n\n“The PyMC Labs Coaching program has been transformative for our small Data Science team, enabling us to deliver results at the level of a full-scale department. We’ve been able to leverage the coaching sessions at every stage of our delivery cycle, from early research and experimentation to implementation, deployment, and long-term roadmapping. The PyMC Labs coaches brought both technical expertise and practical guidance, helping us refine our models, review results, and even troubleshoot complex code issues. Their support not only accelerated our projects but also gave our team the confidence and structure to take on more ambitious work than we thought possible.”\n\nEugene Kwok, Executive Director Research & Analytics, Fox Entertainment\n\n\"PyMC Labs has significantly enhanced our testing capabilities by leveraging the full power of Bayesian programming, maximizing the potential of the PyMC software. Their advisory role in delivering new feature requests and training our team has been invaluable, driving substantial improvements in our operations.\"\n\nNathan Kafi, Principal Data Scientist, Haleon\n\n\"PyMC Labs was instrumental in helping us implement time-varying coefficients to better capture seasonality in our marketing mix models. This significantly improved both the accuracy and interpretability of our models, while also revealing additional opportunities for enhancement. Working with the PyMC Labs team was an absolute pleasure; they were collaborative, insightful, and consistently supportive throughout the engagement.\"\n\nKate Hirth, Senior Data Scientist, Fabletics\nVoices from the Team#\n\nThe EAP is not just a service, it is a collaboration. We asked some of the PyMC Labs experts who work most closely with clients what they enjoy about the program:\n\n“The EAP program is fantastic for both parties. On the one hand, clients get clear and actionable mentorship on how to use statistical models for efficient decision-making. We provide resources, examples, and detailed advice. We empathize with them because we have previously solved similar problems. On the other hand, we learn more about our users (and colleagues) to steer the development of our open-source solutions. ”\n\nJuan Orduz, Principal Data Scientist, PyMC Labs\n\n\"What I enjoy most about the EAP is how collaborative it feels. We are not just solving technical problems, we are helping clients move past obstacles and gain confidence in their modeling approach. It is rewarding to be part of the model building journey, supporting clients as they move from one-off builds to creating sustained, impactful models.\"\n\nTim McWilliams, Principal Data Scientist, PyMC Labs\n\n\"The EAP lends itself to top-notch work. Our deep expertise in Bayesian methods allows us to quickly clear out roadblocks that would ordinarily consume weeks. That frees up our clients' data scientists to focus on being experts in their own data and stakeholder interests. I have really enjoyed working on EAP because everyone gets to do what they do best.\"\n\nDaniel Saunders, Principal Data Scientist, PyMC Labs\n\n\"What I value most about working with clients through the EAP is the opportunity to bridge advanced statistical thinking with real-world decision making. It is great to see how our expertise can transform uncertainty into clarity, helping teams not only find solutions but also strengthen their own capacity to think probabilistically. My favorite part is witnessing that shift-when teams begin to see Bayesian methods not as a hurdle, but as a powerful ally in their scientific and business challenges.\"\n\nCarlos Trujillo, Principal Data Scientist at PyMC Labs\n\n“To me, the most rewarding aspect of the EAP is the collaboration itself. We get to bring our deep expertise in Bayesian modeling and statistical workflows, while our clients possess rich domain knowledge and business expertise. This combination allows us to build models that are not only technically robust but also genuinely aligned with real-world needs.”\n\nTeemu Säilynoja, Junior Researcher at PyMC Labs.\n\n“My favorite part about the EAP process is being able to teach clients not just how to make the one model that they are asking about, but also the fundamentals of Bayesian modeling. I love when things click for a client and then they are able to diagnose and solve their own problems without needing our help. Then we can collaborate on the really interesting problems.”\n\nBill Engels, Principal Data Scientist at PyMC Labs\n\n“At PyMC, I get the most (EAP) satisfaction from working with curious, collaborative clients who are not only focused on solving problems but also open to exploring new ideas and unconventional solutions.\n\nThrough our Expert Access Program, you gain direct access to specialists who guide you in investigating, experimenting, and applying cutting-edge approaches with confidence. What makes PyMC unique is our ability to draw on expertise from a wide range of disciplines: Statistics, Physics, Engineering, Economics, Marketing Analytics, Neuroscience, Computer Programming, AI Development, and Business Strategy. This cross-disciplinary perspective allows us to bring in proven solutions from other industries; while not always necessary it’s having that advantage that makes these projects enjoyably challenging.”\n\nKemble Fletcher, Director - Product Development\nShaping the Future of Measurement Together#\n\nBuilding and scaling advanced measurement frameworks is now a strategic necessity. Organizations cannot afford delays or uncertainty when credibility, planning, and investment decisions depend on the strength of their analytics.\n\nThe Expert Access Program is designed to meet that challenge. It gives your team the confidence to overcome roadblocks, scale models, and earn the trust of stakeholders. This is not abstract theory but practical support from the very experts who build and maintain the frameworks you use.\n\nIf you want to learn how the EAP can accelerate your team's progress and help you take ownership of your measurement future, we would love to talk.\n\n👉 Book a conversation with us here\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/bayesian-marcel": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nKey Features of the MARCEL Model\nA Bayesian MARCEL?\nCase study: Hard hit rate\nData Preparation\nModel formulation\nModel Checking\nxarray.Dataset\nParameter Estimates\nGenerating Projections\nWhen Things Go Wrong\nConclusions\nBayesian Baseball Monkeys\n\nAugust 01, 2024\n\nBy Chris Fonnesbeck\n\nThe MARCEL baseball projection model, developed by Tom Tango, is a deliberately simple forecasting system for Major League Baseball (MLB) player performance. Named humorously after Marcel the Monkey, it aims to represent the \"minimum level of competence\" that should be expected from any forecaster.\n\nActually, it is the most basic forecasting system you can have, that uses as little intelligence as possible. So, that's the allusion to the monkey. It uses 3 years of MLB data, with the most recent data weighted heavier. It regresses towards the mean. And it has an age factor. -- Tom Tango\n\nYet, this minimalist approach makes it a transparent, reliable starting point for projecting player performance without requiring extensive computational resources or complex algorithms.\n\nKey Features of the MARCEL Model#\n\nData Utilization:\n\nThe model uses three years of MLB data, with the most recent year weighted more heavily. This approach ensures that the projections are based on a player's recent performance while still considering their historical data.\n\nRegression to the Mean:\n\nMARCEL incorporates a regression towards the mean, which helps to temper extreme performances and provide more balanced projections.\n\nAge Factor:\n\nThe model includes an age factor to account for the natural aging process of players, which can affect their performance. For most performance measures, this typically means improvement at younger ages, followed by decline at older ages.\n\nHere is Tango's recipe for the model, taken from his website describing the 2004 version of the model:\n\nWeight each season as 5/4/3. 2003 counts as \"5\" and 2001 counts as \"3\".\nDetermine each player's league average. I removed all pitchers' hitting totals from the league average. I lumped in AL and NL together. I weighted the player's league average using the 5/4/3 process and that's player's PA for that season. I then forced in that player's league average to come in at a total of 1200 PA for each player (2 weights x 600 PA). This is the regression towards the mean component.\nAdd the above two.\nDetermine the projected PA = 0.5 * 2004PA + 0.1 * 2003PA + 200. I take the result of #3, and prorate it to this projected PA.\nDetermine an age adjustment. Age = 2004 - yearofbirth. If over 29, AgeAdj = (age - 29) * .003. If under 29, AgeAdj = (age - 29) * .006. Apply this age adjustment to the result of #4.\nRebaseline the results against an assumed league average of 2003.\n\nThis description relates speficically to batter data, but the methodology can be applied to any annual player statistics.\n\nA Bayesian MARCEL?#\n\nAn interesting exercise is to consider how we might implement an analog of MARCEL using Bayesian methods. By incorporating a Bayesian framework, we can introduce uncertainty estimates into the projections, which can be valuable for risk assessment and decision-making. Moreover, a hierarchical model structure would provide a natural way to implement regression to the mean.\n\nIn the spirit of MARCEL, the goal here is not to have a sophisticated model for making accurate projections, but rather to have a simple, general purpose model that can be used to project arbitrary metrics, usually intended to provide baseline one-year-ahead projections. The only difference is that we will now have probabilistic uncertainty estimates around the resulting projections.\n\nTo this end, I've adapted the three core components as follows:\n\nData weights for previous seasons estimated from the data using a Dirichlet distribution.\nA triangular aging function, whose midpoint is estimated from the data.\nA hierarchical random effect for league-wide mean regression.\nCase study: Hard hit rate#\n\nAs a motivating example, we will look at projecting batter hard hit rate. Hard hit rate (Hard%) measures the percentage of batted balls that are hit with exit velocities exceeding 95 miles per hour. Developed by Sports Info Solutions, it was originally based on a subjective classification of batted balls as either \"soft,\" \"medium,\" or \"hard\" hit. The introduction of Statcast data (via HitFX, Trackman, and now Hawkeye) allowed batted ball exit velocities to be measured directly.\n\nHard hit rate is often a leading indicator of performance in other statistics such as home run (HR) per fastball rate and batting average on balls in play (BABIP), while for pitchers, it can help explain their expected fielding-independent pitching (xFIP), among others. The league average hard hit rate is around 35%, so consistently exceeding this mark is generally considered positive for hitters (and concerning for pitchers). The relationship between hard hit rate and weighted on-base average (wOBA), a measure of batting performance that accounts for the quality of hit outcomes, is shown below, demonstrating an inflection around 95 MPH.\n\nImage: mlb.com\n\nData Preparation#\n\nIt's generally pretty easy to get your hands on baseball data. We will use the pybaseball package to obtain the data that we need for this model. The package provides a convenient interface to the Statcast data, which contains detailed player statistics for each season.\n\nSince MARCEL models require three seasons of data, we will use hard hit rates for years 2020 trhough 2022 to fit values observed in 2023, and then use the resulting parameter estimates to predict the 2024 rate from seasons 2021 through 2023. Therefore, we will query all batter data from 2020 through the games in 2024 at the time of this writing.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport pymc as pm\nimport arviz as az\nfrom pybaseball import batting_stats\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\ndata = batting_stats(2020, 2024, qual=1)\n\nhard_hit_subset = data[[\"Name\", \"Season\", \"Team\", \"Age\", \"HardHit\", \"Events\"]].copy().reset_index(drop=True)\n\n\nSo, we will be using years 2020 to 2022 to fit to 2023 outcomes, where the target variable is observed hard hits (HardHit). The sample size will be the corresponding number of batted balls (Events) in each year.\n\nFor convenience, we are using players who have complete data over all 4 seasons. Incidentally, it would be easy to impute missing data to allow for the inclusion of players with partial data, but we are trying to keep things simple!\n\nWe need to do a little bit of data processing. Specifically, we need to pivot the data from long- to wide-format so that we have a row for each batter and columns representing each year of data.\n\nhard_hit_by_season = (\nhard_hit_subset.pivot_table(index=['Name'], values=[\"Events\", \"HardHit\", \"Age\"], columns=['Season'])\n.dropna(subset=[('HardHit', y) for y in range(2020, 2024)])\n)\nhard_hit_by_season[('Age', 2024)] = hard_hit_by_season[('Age', 2023)] + 1\n\nModel formulation#\n\nLet's start by creating a PyMC Model and adding our data to it. Rather than just using NumPy arrays or pandas DataFrames, creating Data nodes in our model graph will permit us to easily generate predictions for new data after we fit the model.\n\nfit_seasons = [2020, 2021, 2022]\n\ncoords = {\n    'batter': hard_hit_by_season.index.to_numpy(), \n    'season': fit_seasons,\n}\n\nwith pm.Model(coords=coords) as marcel:\n\n    events_x = pm.Data(\"events_x\", hard_hit_by_season[\"Events\"][fit_seasons].to_numpy().astype(np.int32))\n    hard_hit_x = pm.Data(\"hard_hit_x\", hard_hit_by_season[\"HardHit\"][fit_seasons].to_numpy().astype(np.int32))\n    age_y = pm.Data(\"age_y\", hard_hit_by_season[\"Age\"][2023].to_numpy().astype(np.float32))\n    events_y = pm.Data(\"events_y\", hard_hit_by_season[\"Events\"][2023].to_numpy().astype(np.int32))\n    hard_hit_y = pm.Data(\"hard_hit_y\", hard_hit_by_season[\"HardHit\"][2023].to_numpy().astype(np.int32))\n\n\nLet's start with the mean regression component of the model.\n\nsns.histplot(data=data, x=\"HardHit%\")\nsns.despine(offset=10, trim=True);\n\n\nIn the original MARCEL model, Tango dealt with this problem by calculating a weighted average of each player's observed data and the league average. Here, we will use a hierarchical model to partially pool the data according to the relative quantity of data for each player-season. Since we are modeling the number of hard hits per batted balls, a binomial sampling distribution with beta-distributed rates is appropriate.\n\nAs we were told that the league average hard hit rate is around 35 percent, we can take advantage of the mean-variance parameterization of the beta distribution in PyMC:\n\nThen, we can assign a weakly-informative priors to  and :\n\n \n\nwith marcel:\n\n    # Empirical rates, regressed to mean with beta-binomial\n    mu_p = pm.Beta(\"mu_p\", mu=0.35, sigma=0.2)\n    sigma_p = pm.Uniform(\"sigma_p\", 0, 0.5)\n    p = pm.Beta(\"p\", mu=mu_p, sigma=sigma_p, dims=(\"batter\", \"season\"))\n    pm.Binomial(\"rate_like\", n=events_x, p=p, observed=hard_hit_x)\n\n\nThe core of the Marcel projection model is the weighted averaging of previous seaons' observations. While the original model is a hard-coded a 5/4/3 weighting scheme, we will estimate optimal weights from the data, which is straightforward in a Bayesian framework. Thus, we are looking to project  via:\n\nwhere the  are the latent partially-pooled rates from above.\n\nThe set of weights , in turn, are modeled with a Dirichlet distribution:\n\nwith marcel:\n\n    # Marcel weights\n    w = pm.Dirichlet(\n        \"w\",\n        a=np.array([3, 4, 5]),\n        dims='season'\n    )\n\n    # Raw projection\n    p_proj = p @ w\n\n\nFinally, we implement the aging effect as a simple triangular (piecewise-linear) model on the logit scale, where the hard hit rate increases up to some \"peak\" age, and then decreases at the same rate thereafter. Both the value of the slope and the peak age are parameters of the model that we can estimate.\n\n  \n\nwith marcel:\n\n    # Coefficient for triangular aging\n    beta = pm.Normal(\"beta\")\n    # Estimate of peak age\n    peak_age = pm.Uniform(\"peak_age\", data.Age.min(), data.Age.max())\n\n    age_effect = beta * (peak_age - age_y)\n\n\nThe projected rate is the age-adjusted weighted average of the latent rates over the three-year period, appropriately transformed to stay on the unit interval (see below).\n\nwith marcel:\n    projected_rate = pm.Deterministic(\n        \"projected_rate\",\n        pm.math.invlogit(pm.math.logit(p_proj) + age_effect),\n        dims=\"batter\"\n    )\n\n    # # Likelihood of observed hard hits\n    pm.Binomial(\"prediction\", n=events_y, p=projected_rate, observed=hard_hit_y, dims=\"batter\")\n\n\nBefore we go ahead and fit the model, let's look at the model graph. It serves as a useful graphical summary of the model, and allows you to verify that you have specified the model correctly.\n\nThe shaded nodes are related to observed quantities in our problem, specifically the two likelihoods (ovals) and the data itself (rectangles). Open circles are the unboserved random variables, while the open rectangle is the expected hard hit rate, a deterministic sum of other model components. Enclosing plates denote vector-valued nodes, here confirming that we have included 332 players over 3 seasons in our analysis.\n\nWe will estimate the model using Markov chain Monte Carlo (MCMC) sampling, specifically the No-U-Turn Sampler (NUTS) algorithm, as implemented in PyMC v5. It should only take a few seconds to sample.\n\nwith marcel:\n    trace = pm.sample()\n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 29 seconds. The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n\nYou want to keep an eye on this output for any warning messages that may require additional intervention in order to improve the model. This example should run fine under the default arguments.\n\nModel Checking#\n\nEven in the absence of warnings, its always important to check your model. In model checking, we want to know at least two things: did the model fitting algorithm work, and does the model fit the data adequately?\n\nFor the first part, we want to do convergence diagnostics because we have used a stochastic sampling approach (MCMC) to fit the model. That is, we want some assurance that the sampler converged to what the model specifies as the posterior distribution. This implies that the algorithm has had a chance to adequately explore the parameter space of the model.\n\nThe az.plot_energy function will produce an energy plot of the fiitted model that gives a visual indication as to whether the model has been able to freely explore the parameter space. You will see two distributions: a marginal energy distribution and an energy transition distribution. It's not important to understand exactly what these are, but you are looking for these two distribtions to mostly overlap, as they do below. When they do not, the difference is usually rather dramatic.\n\naz.plot_energy(trace);\n\n\nAnother useful convergence diagnostic is the R-hat statistic, also known as the potential scale reduction factor. It compares the variance of chain means to the mean of chain variances across multiple chains, with values close to 1 indicating good convergence. If R-hat is substantially greater than 1, it suggests that the chains have not yet converged to a common distribution, indicating that more iterations may be needed.\n\nThe sampler will return warnings if it detects large R-hat values for any of the parameters, but we can manually check them using the rhat function:\n\naz.rhat(trace).max()\n\n\n<xarray.Dataset> Size: 56B\nDimensions: ()\nData variables:\n\nbeta float64 8B 1.001\nmu_p float64 8B 1.001\np float64 8B 1.011\npeak_age float64 8B 1.002\nprojected_rate float64 8B 1.007\nsigma_p float64 8B 1.0\nw float64 8B 1.005\nxarray.Dataset#\n\nDimensions:\n\nNo dimensions available.\n\nCoordinates:\n\nNo coordinates available.\n\nData variables:\n\nbeta: float64, 8B, 1.001\nmu_p: float64, 8B, 1.001\np: float64, 8B, 1.011\npeak_age: float64, 8B, 1.002\nprojected_rate: float64, 8B, 1.007\nsigma_p: float64, 8B, 1.0\nw: float64, 8B, 1.005\n\nIndexes:\n\nNo indexes available.\n\nAttributes:\n\nNo attributes available.\n\nDespite the warning during sampling, these all look fine -- one value is borderline, but nothing to worry about.\n\nIf we are satisfied that the model has converged, we can move on to checking the model fit. This is largely the same idea as a model calibration check: seeing whether the model predictions correspond adequately to observed values. Since we have a Bayesian model, we can use posterior predictive checks for this. This involves generating simulated datasets from the fitted model, and comparing the simulations to the actual observed data (in this case, the data used to fit the model). If the model fits, we would expect the observed data to be indistinguishable from some random draw from the model.\n\n    pm.sample_posterior_predictive(trace, extend_inferencedata=True)\naz.plot_ppc(trace, mean=False);\n\n\nThis doesn't look bad -- the observed data behaves like just another sample from the model.\n\nParameter Estimates#\n\nHaving confirmed the fit of the model, we can move on to looking at some of the latent parameters and seeing if they make sense.\n\nThe estimated set of weights decline in magnitude with time, as we would expect. They correspond approximately to a 6/2/1 weighting, which gives the most recent season much more influence relative to the original MARCEL model and its 5/4/3 weighting. Recall that we used 5/4/3 as a prior weighting, but the evidence in the data shifted the weights away from this starting point.\n\naz.plot_forest(trace, var_names='w', combined=True);\n\n\nFrom the posterior estimates below, we see that the mean hard hit rate is around 39%. Realistically, this value is probably best regarded as temporally dynamic, but we are resisting the urge to expand the model right now.\n\naz.plot_posterior(trace, var_names=['mu_p', 'sigma_p'])\nplt.tight_layout();\n\n\nWhat about the aging effect? The model estimates \"peak\" hard-hit age to be approximately 28 years (albeit with a wide range of posterior uncertainty), which seems a reasonable inflection point. The corresponding beta parameter is positive, indicating a positive slope at ages below 28 and negative above.\n\naz.plot_posterior(trace, var_names=['peak_age', 'beta'])\nplt.tight_layout();\n\n\nGenerating Projections#\n\nOnce we have checked the model and are comfortable with the evaluation, we can use it to generate projections for 2024, using input data from 2021 through 2023. Here, we are not estimating anything; we are merely applying the fitted model parameters to a new set of inputs.\n\nIn PyMC, the set_data function is used to replace existing data in the model with new data, after which we can use the sample_posterior_predictive function to generate predictions. This performs forward sampling, drawing values from the posterior distributions we have estimated and using them to simulate projections. For this model, predictive sampling should be almost instantaneous.\n\npred_seasons = [2021, 2022, 2023]\n\nwith marcel:\n    pm.set_data(\n        {'age_y': hard_hit_by_season[\"Age\"][2024].to_numpy().astype(np.float32),\n         'hard_hit_x': hard_hit_by_season[\"HardHit\"][pred_seasons].to_numpy().astype(np.int32),\n         'events_x': hard_hit_by_season[\"Events\"][pred_seasons].to_numpy().astype(np.int32)\n        }\n    )\n\n    pm.sample_posterior_predictive(trace, var_names=['projected_rate'], predictions=True, extend_inferencedata=True)\n\n\nLet's take a peek and see how the model performs. Below are predictive intervals for each player's expected hard hit rate in 2024, along with the observed rate (which, keep in mind, is only for about a half-season). To help evaluate the predictions, the observed rate is color-coded by sample size -- darker red indicates a larger sample size.\n\nThe model performs well, considering how stripped-down it is. The weighted averaging and partial pooling components have done a nice job of protecting the model from overfitting. The sharing of information via the hierarchical component also means that the predictive intervals are narrow, even for players with relatively sparse data.\n\nIf we focus on the players with substantive sample size (i.e. those with dark red data points), we can potentially identify hitters that are over- or under-performing. Certainly, Juan Soto, Ketel Marte, and Marcell Ozuna are having great seaons so far, while Whitt Merrifield was released by the Phillies shortly before this writing.\n\np_obs = (hard_hit_by_season['HardHit'] / hard_hit_by_season['Events'])[2024].fillna(0)\n\naxes = az.plot_forest(trace.predictions['projected_rate'], combined=True, figsize=(6, 72))\naxes[0].scatter(p_obs.values[::-1], axes[0].get_yticks(), c=list(hard_hit_by_season.Events[2024].fillna(0).values[::-1]), cmap='Reds');\n\n\nA scatter plot of projections vs observtions (shaded proportionally to sample size) shows the model to be reasonably well-calibrated.\n\np_proj = trace.predictions['projected_rate'].mean(dim=('chain', 'draw'))\nn = hard_hit_by_season[\"Events\"][2024].fillna(0).to_numpy()\np_obs =  hard_hit_by_season[\"HardHit\"][2024].to_numpy() / n\n\nprojection = pd.DataFrame(dict(projection=p_proj, observed=p_obs))\nax = sns.regplot(x='projection', y='observed', data=projection, scatter_kws={'alpha':n/n.max()})\nsns.despine(ax=ax, offset=10, trim=True);\n\n\nWhen Things Go Wrong#\n\nWith such a simple model that can be applied to virtually any metric, inevitably things will go wrong with certain datasets. The first place to look for symptoms of a problem are in the warnings during model fitting. For example, you may read something like the following:\n\nUserWarning: Chain 0 contains 6 diverging samples after tuning. Increase `target_accept` or reparameterize.\n\n\nThis means that the model is (occasionally) having trouble sampling from the model, which results in a diverging sample. If this happens only a few times things are probably okay, but if there are hundreds or even entirely divergent samples, you will not be able to reliably use inference from the model.\n\nThe underlying PyMC model tries to give advice when it can: while you cannot reparameterize this model, it is possible to change the target_accept parameter passed to sample.\n\npm.sample(target_accept=0.99)\n\n\ntarget_accept is the target acceptance rate of the MCMC algorithm, which in PyMC is 0.8 by default. For difficult models, upping this value to 0.9, 0.95 or 0.99 can sometimes solve this issue. Note that higher acceptance rates result in longer runtimes, since it involves taking more steps at every iteration of the model.\n\nIf you run into issues with convergence--for example, if the energy plots do not look slimiar--the easiest remedy is to run the MCMC model longer by adding more tuning steps. The default number is 1000, but you can change this with the tune argument.\n\npm.sample(tune=5000)\n\n\nOf course, there will be some scenarios where a simple, general model like this will just not work. If there are issues with the data, or the data do not conform to a simple statistical likelihood like a binomial, then you may need to build a bespoke model to account for the particular characteristics of the quantity you are trying to project.\n\nConclusions#\n\nIn one sense, the MARCEL model represents the core of almost any viable player projection model: it uses past performance to predict future performance, regresses projections to some population mean, and adjusts for the effects of aging. While I used it here to project hard hit rate via a binomial sampling process, we could just as easily have modeled pitcher fastball velocity with essentially the same model, swapping out the binomial likelhood for a Gaussian. For that matter, it could have been used to project wide receiver sprint speed in football or goalie save percentage in ice hockey.\n\nRe-casting MARCEL in the Bayesian paradigm proved to be a straightforward enhancement that preserved the overall simplicity of the original model. It gave us a natural way of regressing projections to the mean, a principled was of weighting past performance, and a probabilistic measure of uncertainty in our projections. We used the same ingredients for the model, merely implementing them in a different way.\n\nOf course, the Bayesian monkey might not be the best choice for all of your projection needs. Clearly, we've left a lot on the table, and this is by design. It is best to think of MARCEL (Bayesian or otherwise) as a starting point in the development of a performant projection system. With relatively little effort (or additional data), the model could be expanded to account for obvious shortcomings. For example:\n\nAging curves for most performance time series are not triangular, as we've assumed here. One might, for example, swap in a quadratic or some other polynomial function, or get fancy and use a spline.\nFurther, there is often age-related selection bias at play, whereby older players are not representative of the general population at that age, since only top veterans tend to play into their late thirties and forties. Some type of censoring model can be used to account for this.\nPerformance can often be influenced by location. With sufficiently large data sets, its possible to estimate venue effects that can be used to adjust projections to be venue-neutral.\nThe weighting scheme implemented by MARCEL is essentially a third-order autoregressive model. It may be worth considering alternative time series models, depending on the metric in question. For example, there may be processes operating at multiple time scales that may influence performance, in which case Gaussian processes can be effective for describing them.\n\nThe workflow for constructing sports projection models is really no different than what we recommend for other types of models. Rather than attempting to build a feature-complete model at the outset, start with a baseline model that is easy to understand and implement, then expand according to how that model falls short relative to the goals of the project. The Bayesian MARCEL provides a reasonable starting point for a baseball projection system, not unlike the ordinary least squares model does for regression problems.\n\nThis is the sort of thing we do here at PyMC Labs -- squeezing extra value and insights from your data using the power of probabilistic programming! Feel free to get in touch to chat about our approach to sports analytics, if you think we could be helpful 😉 If you want to learn more about hierarchical modeling in particular, this free PyMC tutorial is full of tips and tricks that you may find useful. The \"Learn Bayes Stats\" podcast also has a huge catalogue of technical episodes, or if you're more of a visual learner, the Intuitive Bayes platform has several high-quality courses.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/hierarchical_clv": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nExecutive Summary\nIntroduction\nThe Challenge: Capturing Complex Customer Behavior\nA Stepping Stone: The Cohort Approach\nBayesian Hierarchical Models\nA Practical Example\nThe Hierarchical Model Solution\nConclusion: Elevating CLV Predictions\nAppendix: Latent Beta Distribution of the BG/NBD Model\nHierarchical Customer Lifetime Value Models\n\nJuly 26, 2024\n\nBy Juan Orduz\n\nExecutive Summary#\n\nThis post explores the application of hierarchical Bayesian models to Customer Lifetime Value (CLV) prediction, offering significant improvements over traditional methods. Key takeaways include:\n\nHierarchical models address limitations in handling seasonality and cohort differences that challenge conventional CLV models.\n\nThese advanced models provide more stable and accurate predictions, especially for new or small customer cohorts.\n\nBy balancing individual cohort behavior with overall population trends, hierarchical models offer a more nuanced understanding of customer value.\n\nImplementation of these models can lead to more informed business decisions, better resource allocation, and improved customer strategies.\n\nWe demonstrate the practical application and benefits using the CDNOW dataset and the BG/NBD model as examples.\n\nFor businesses looking to enhance their CLV modeling capabilities, hierarchical Bayesian models represent a powerful tool for gaining deeper, more actionable customer insights.\n\nIntroduction#\n\nIn our previous exploration of customer lifetime value (CLV), we introduced the Pareto/NBD model—a flexible approach to predicting CLV. We showcased its modern implementation in PyMC-Marketing, which enables various inference methods, including maximum a posteriori estimation and MCMC techniques. This powerful tool allows us to set custom priors and estimate prediction uncertainty. However, it's just the beginning of what's possible in CLV modeling.\n\nIn this blog post, we explore additional benefits of Bayesian analytics. We'll explore how hierarchical models can elevate our CLV predictions, leading to more accurate insights and, ultimately, greater business value.\n\nLet's unpack the ideas, opportunities, and implications of applying hierarchical models to customer lifetime value modeling.\n\nThe Challenge: Capturing Complex Customer Behavior#\n\nTraditional probabilistic purchase models, like Pareto/NBD and BG/NBD, excel at modeling customer transaction behavior in non-contractual, continuous scenarios (think grocery shopping). These models focus on two key aspects: purchase frequencies and the dropout process. While they can incorporate time-invariant factors like demographics and acquisition channels, they struggle with time-varying elements such as seasonality.\n\nFor long-term CLV estimation, seasonal patterns tend to smooth out. However, for mid-term predictions, addressing these patterns becomes crucial. This is where hierarchical models offer a compelling solution.\n\nA Stepping Stone: The Cohort Approach#\n\nOne practical workaround is to model user cohorts independently. That is, fit one model per cohort. By grouping customers based on their acquisition month, we can partially account for seasonal patterns. This approach involves fitting separate BG/NBD models for each cohort.\n\nWhile reasonable, this method has limitations:\n\nModel proliferation: The number of models can quickly become unwieldy. Imagine a company with 10 markets and two years of user data —you're looking at 240 models, with more added each month.\n\nThe cold start problem: How do we model new cohorts with minimal transaction data?\n\nArbitrary boundaries: Is there a meaningful difference between customers who join on May 31st versus June 1st?\n\nDespite its imperfections, this cohort-based approach often outperforms a one-size-fits-all model. But can we push our analytics further?\n\nBayesian Hierarchical Models#\n\nBayesian hierarchical models offer a sophisticated framework for tackling complex data relationships. They're particularly valuable when dealing with grouped or nested data structures. These models strike a balance between group-specific estimates and overall population trends, allowing us to:\n\nPool information across groups\n\nBorrow statistical strength from the entire dataset\n\nInterpolate between pooled and unpooled estimates\n\nThe result? More stable and reliable predictions, especially when dealing with varying group sizes or limited data.\n\nIn our CLV context, we can view cohorts as members of a global customer population, making them prime candidates for hierarchical Bayesian analysis.\n\nA Practical Example#\n\nLet's illustrate these concepts using the classic CDNOW dataset and the BG/NBD model. Without going into details, when fitting this model, we are trying to estimate four parameters: r, α, a, and b. Once we know these parameters, we can derive many exciting insights like the number of future purchases per user or the probability of being active; for more details, see the example notebook in the PyMC-Marketing documentation page. Here is a simple diagram of our model:\n\nHere's a simplified diagram of our model:\n\nUsing PyMC-Marketing, we can fit this model and visualize the posterior distributions of our four parameters:\n\nNow, for the sake of illustrating the grouping challenge, let's segment our users into four groups (e.g., different cohorts) with varying sizes:\n\n  g1    1065\n  g2     815\n  g3     353\n  g4     124\n  Name: group, dtype: int64\n\n\nNotice how group four is significantly smaller than groups one and two.\n\nWhen we fit individual BG/NBD models for each group, we get these estimates:\n\nThe volatility in estimates for the smaller groups (especially for parameters a and b) is concerning. This instability could lead to unreliable CLV predictions for new or small cohorts.\n\nThe Hierarchical Model Solution#\n\nTo address this issue, we can implement a hierarchical structure. We'll assume the parameters for each group are drawn from a global prior distribution:\n\nThis approach allows us to regularize parameter estimation by sharing information across groups. Let's examine the results:\n\nWe now have four posterior distributions (one per group) that cluster more closely around the global estimate (shown in gray).\n\nTo visualize the impact, let's compare three approaches:\n\nGlobal BG/NBD model (pooled)\n\nIndividual group models (unpooled)\n\nHierarchical model\n\nThe hierarchical model estimates show less volatility than the individual models while centering around the global mean. This \"shrinkage\" effect is a hallmark of Bayesian hierarchical models, leading to more robust and reliable inferences —especially valuable when dealing with sparse data or multi-level parameter estimation.\n\nConclusion: Elevating CLV Predictions#\n\nBy applying hierarchical Bayesian methods to classic probabilistic CLV models, we've unlocked a more robust approach to customer value estimation. This technique is particularly powerful for addressing seasonality effects across different customer cohorts, resulting in more accurate and actionable CLV predictions.\n\nThese advanced statistical methods empower businesses to:\n\nGain deeper insights into customer behavior patterns\n\nMake more informed, data-driven decisions\n\nOptimize strategies for customer acquisition and retention\n\nAs you look to enhance your CLV modeling capabilities, consider exploring the potential of hierarchical models. They offer a sophisticated yet practical way to turn the challenges of seasonality and cohort relationships into opportunities for more precise customer analytics.\n\nThis example was based on the original blog post BG/NBD Model in PyMC.\n\nAppendix: Latent Beta Distribution of the BG/NBD Model#\n\nFor those interested in the technical details, setting priors for the latent Beta distribution in the BG/NBD model requires some finesse. Here is an approach that has proven effective in practice, inspired by this PyMC discourse post and implemented in Colt Allen's btyd package, a predecessor of PyMC-Marketing:\n\nSetting priors on the latent Beta distribution in the BG/NBD model:\n\n# Hierarchical pooling of hyperparams for beta parameters.\nphi_prior = pm.Uniform(\n    \"phi\",\n    lower=self._hyperparams.get(\"phi_prior_lower\"),\n    upper=self._hyperparams.get(\"phi_prior_upper\"),\n)\nkappa_prior = pm.Pareto(\n    \"kappa\",\n    alpha=self._hyperparams.get(\"kappa_prior_alpha\"),\n    m=self._hyperparams.get(\"kappa_prior_m\"),\n)\n\n# Beta parameters.\na = pm.Deterministic(\"a\", phi_prior * kappa_prior)\nb = pm.Deterministic(\"b\", (1.0 - phi_prior) * kappa_prior)\n\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/2023-01-12-Akili": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nIntroduction\nTimestamps\nResources\nLikelihood Approximations for Cognitive Modeling with PyMC\n\nJanuary 12, 2023\n\nBy Thomas Wiecki\n\nIntroduction#\n\nDigital therapeutics are evidence-based, clinically evaluated software and devices that can be used to treat an array of diseases and disorders, according to the Digital Therapeutics Alliance, the industry's trade association. They can be used independently or with medications, devices, and other therapies to treat physical and behavioral health conditions, including pain, diabetes, anxiety, post-traumatic stress disorder, and asthma.\n\nIn this talk, PyMC Labs and Akili discuss using Bayesian methods and PyMC to test a range of computational models of cognition, specifically with an eye towards ADHD (Attention-deficit/hyperactivity disorder). They focus on some technical challenges and how ideas from likelihood-free inference and machine learning can help overcome them.\n\nTimestamps#\n\n00:00 Thomas Wiecki introduction\n\n01:27 Alex introduction\n\n01:51 Titi introduction\n\n02:55 Andy introduction\n\n03:42 Akili background\n\n05:11 EndeavorRx by Akili and PyMC's involvement\n\n09:49 Likelihood approximation networks in PyMC\n\n10:15 NeuroRacer\n\n15:44 Two important aspects of the Model\n\n20:39 Inference with model variants\n\n21:05 Inference from access to simulators\n\n21:55 Inference with models\n\n22:56 Training\n\n24:06 Previous toolbox: HDDM\n\n24:59 Properties inherited from Neural Networks\n\n26:31 Graphical representation of model in PyMC\n\n29:42 Code in PyMC\n\n30:07 Neural network (LAN, CPN)\n\n31:17 Proof of concept (Parameter Recovery)\n\n31:42 Proof of concept (Speed)\n\n32:41 Thomas question on speed\n\n36:11 Thomas on Before PyMC vs after PyMC\n\n36:32 Titi on before PyMC vs after PyMC\n\n38:11 Andy on production use case\n\n39:00 Thomas question on application\n\n39:16 Andy explains the use case and the application\n\n40:28 Titi on impact in applications\n\n41:15 Thomas on Knowledge transfer to Akili research team and collaboration\n\n43:02 Andy on working with PyMC team\n\n44:55 Thomas question to Alex on applying this method to other applications across industries\n\n47:15 Why does Akili care about these kinds of models ?\n\n49:31 PyMC's work and impact towards Akili's mission\n\n51:04 Audience Q/A (What other conditions can this be applied other than ADHD?)\n\n52:03 Audience Q/A (Is Data enough to conduct experiments ?)\n\n56:32 Closed form solution vs Neural Networks\n\n56:52 Optimizing LAN for faster forward pass, primary metric and designing networks\n\nResources#\nAkili Interactive\nWhat are digital therapeutics and their use cases?\nUnderstanding DTx - Digital Therapeutics Alliance\nLikelihood approximation networks (LANs) for fast inference of simulation models in cognitive neuroscience\nOverview of Approximate Bayesian Computation\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/causal-inference-in-pymc": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nCounterfactual inference: asking what if?\nExcess deaths in England and Wales\nA Bayesian causal model\nBuilding a Bayesian model in PyMC\nUsing the model\nPrior predictive distribution\nInferring a posterior distribution over parameters\nLet's do causal inference!\nResources\nAcknowledgements\nWhat if? Causal inference through counterfactual reasoning in PyMC\n\nJuly 13, 2022\n\nBy Benjamin Vincent\n\nHave you started to hear people talk about causal inference and wanted to know more? Have you looked into causal inference but been daunted by unfamiliar concepts, notation, and software packages? Do you wonder what the relationship between causal inference and Bayesian modeling is? Fear not! While the field of causal reasoning is complex and nuanced, we can make meaningful inroads pretty easily.\n\nThis first post on causal reasoning will demonstrate that it is accessible and that we can use a bunch of Bayesian concepts that we are already familiar with. Not only that, we'll show how this can be done in Python and PyMC.\n\nObligatory xkcd.\nCounterfactual inference: asking what if?#\n\nImagine you had just finished a $1/4 million advertising campaign and your company wants to know what the impact of that was. Maybe they will ask \"Did the campaign work?\" \"How much revenue did the campaign generate?\" \"How many customers did the campaign acquire?\"\n\nThese are questions about causal effects of the campaign, and questions like this can be answered by considering specific counterfactual, or \"what if\" questions. For example, \"How many more sales did we get, compared to a hypothetical scenario where we did not run the campaign?\" In order to answer questions like this, we could calculate excess sales caused by the advertising campaign by comparing the actual number of sales to the expected number of sales in a hypothetical timeline where we did not run the campaign.\n\nThis post will cover how we can do counterfactual inference, and how we can do this in PyMC, using the example of calculating excess deaths due to COVID-19 (and all related effects, such as changes in availability of medical care). This approach could be applied to many other domains of course, including estimating excess sales due to a marketing campaign, or improvement in school achievement given an educational intervention.\n\nExcess deaths in England and Wales#\n\nExcess deaths are defined as:\n\nwhere the number of reported deaths are a (potentially noisy or lagged) measurement of a real objectively observable aspect of the world. However expected deaths is a counterfactual concept - it is not observable in our timeline. This represents the number of reported deaths that we would expect if COVID-19 had not occurred.\n\nLet's take a look at the data we have. We have the total reported deaths per month in England and Wales, going back a good few years and up to the time of writing.\n\nNumber of reported deaths (all causes) in England and Wales over a number of years. Observations in the pre and post COVID-19 period are shown in different colours.\nA Bayesian causal model#\n\nNow that we have a feel for the data, we are going to consider a reasonably simple model of the number of reported deaths, as shown in the causal DAG below.\n\nOur causal DAG. Time and temperature as continuous variables, and month is a discrete variable with one level for each month of the year.\n\nThis model considers causal effects of:\n\nTime: To be clear, we are using time as a proxy variable to capture an increasing number of susceptible (i.e. older) individuals over time, because of the particular population pyramid in England and Wales.\nMonth: To capture the clear seasonality we can see in the data. Bear in mind that we are not claiming that the abstract concept of a month kills people, but we are using this as a proxy variable for a whole host of seasonal factors.\nTemperature: Which is also seasonal and has a clear causal pathway in which it can impact the number of deaths. We are using monthly average temperatures across the UK, but it may be that looking at temperature extremes have a clearer causal influence.\n\nOur end goal here is to calculate excess deaths. Therefore we want to build a model focussing on explaining expected deaths before COVID-19. So even though we know for a fact that many people died from COVID-19 (both directly, and indirectly) we do not include COVID-19 as a predictor, neither as a binary before/after onset, nor the prevalence of cases. Instead, we use our model to predict expected deaths in the post-COVID period (without giving it any information about COVID-19), then subtract the number of reported deaths to arrive at our target: excess deaths.\n\nIn addition to defining the causal structure in the DAG above, we need to specify the relationships (i.e. the edges, or arrows). Specifically, this means we need to define\n\nwhere there are  monthly observations.\n\nThere are many different modelling approaches we could take here to define , but for simplicity we will treat this as a linear regression model. So we could consider a model like this:\n\nwhere  is a vector of 12 monthly deflection parameters.\n\nBuilding a Bayesian model in PyMC#\n\nThe good news is that if you already know how to write simple Bayesian models, then you can probably follow this:\n\nThe core of the model here is the linear regression equation which defines  above. The rest just defines the likelihood (our prior over the data) and our priors over the model parameters . We don't show the PyMC code here, but see the full notebook for all the implementation details.\n\nA nice feature of PyMC is that we can generate plots of the DAG, see below. These are clearly more involved than the simple causal DAG we have above - but these are in fact doing much the same thing. It is simply that the PyMC DAG is more detailed, including nodes for the parameters and intermediate computations (i.e. ).\n\nThe PyMC graphviz output of our Bayesian model.\n\nWhat we've done here with our PyMC model is to describe the full joint distribution .\n\nUsing the model#\nPrior predictive distribution#\n\nWe query what the model would predict before having observed any data. We can do this in PyMC, sampling from the prior predictive distribution .\n\nA summary of the prior predictive distribution for the pre COVID-19 era only, which tells us the predicted number of deaths (with 95 and 50% credible regions) based on our prior knowledge, before having seen the data.\nInferring a posterior distribution over parameters#\n\nWe can use PyMC in order to generate MCMC samples which approximate a distribution over parameters, conditioned on the data:\n\nwhere , ,  are vectors of observations before the onset of COVID-19.\n\nLet's do causal inference!#\n\nFirstly, we want to use the model in order to get the models predictions (technically retrodictions) about the number of deaths we observed before the onset of COVID-19. This is an important step - if the model does not do a good job of predicting the observed deaths before the onset then why would we expect it to make good counterfactual predictions of future deaths in the counterfactual world of no COVID-19?\n\nSecond, we can use the famous do-operator. This is the crux of the lesson here on counterfactual inference - we are querying what the model forecasts if we were to surgically intervene in some variables. In this case, we will make an intervention and set the time, month, and temp variables equal to the post COVID-19 era. In other words, we are doing a counterfactual forecasting, querying what we believe the deaths would have been from January 2020 onwards in the case where COVID-19 never happened. This query can be expressed as:\n\nwhere , , and  are vectors of values of time indexes, months, and temperatures in the forecast time period we are interested in. Practically, we do this in PyMC using the pm.set_data() function which allows us to change the value of our input variables to now represent those from the post-COVID-19 period. That way, our predictions generated by pm.sample_posterior_predictive() will be our expected deaths in our period of interest.\n\nwith model:\n    # do-operator\n    pm.set_data({\"month\": month_post, \"time\": time_post, \"temp\": temp_post})\n    # sample from this out of sample posterior predictive distribution\n    counterfactual = pm.sample_posterior_predictive(idata, var_names=[\"obs\"])\n\n\nwhere month_post, time_post, and temp_post are vectors of the months, time indexes and temperatures in the post COVID-19 onset period we are considering.\n\nSo how did we do?\n\nShaded regions before the onset of COVID-19 represent 95 and 50% credible regions of the the posterior predictive number of deaths. The shaded regions after the onset of COVID-19 are our counterfactual inferences. The top panel shows this in terms of absolute number of deaths. The middle panel shows excess deaths. The bottom panel shows cumulative excess deaths.\n\nFirst, looking at the pre COVID-19 era (before January 2020), we can see that the model does a reasonable job of accounting for the actual observed pre COVID-19 deaths. Second, we can see in the post COVID-19 era, the observed number of deaths is meaningfully higher than our counterfactual expected number of deaths had COVID-19 not happened. We can then use the formula above to calculate the excess deaths (middle panel), and also cumulate to estimate the distribution of cumulative excess deaths (lower panel).\n\nAnd there we have it! We used our existing knowledge about Bayesian inference and outlined how we can approach counterfactual reasoning in PyMC. We've taken a small but important step into the world of causal inference!\n\nResources#\n\nSee the full notebook for all the implementation detail that shows how to implement the material covered in this post.\n\nAcknowledgements#\n\nThanks to Eric Ma for his causality notebooks which were particularly useful in preparing this post.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/likelihood-approximations-through-neural-networks": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nTable of Contents\nSetting the Stage\nWhat kind of data\nThe model(s)\nMotivating Simulation Based Inference\nFrom model simulation to PyMC model\nSimulating Data\nTurning it into Training Data\nBuilding and Training the Network\nConnecting to PyMC\nBuilding a custom distribution\nPlug the custom likelihoods into a PyMC model\nInference example\nLikelihood Approximations with Neural Networks in PyMC\n\nMarch 31, 2023\n\nBy Ricardo Vieira, Alexander Fengler, Aisulu Omar, And Yang Xu\n\nLeaning on an applied data-analysis problem in the cognitive modeling space, this post develops the tools to use neural networks trained with the Flax package (a neural network library based on JAX) as approximate likelihoods in likelihood-free inference scenarios. We will spend some time setting up the data analysis problem first, including the modeling framework used and computational bottlenecks that may arise (however if you don't care about the particulars, feel free to skip this part). Then, step by step, we will develop the tools necessary to go from a simple data simulator without access to a likelihood function to Bayesian Inference with PyMC via a custom distribution.\n\nWe will try to keep the code as general as possible, to facilitate other use cases with minimal hassle.\n\nTable of Contents#\nSetting the Stage\nThe Data Analysis Problem\nWhat kind of Data?\nThe model(s)\nMotivating likelihood free inference\nFrom model simulation to PyMC model\nSimulating Data\nTurning it into Training Data\nBuilding and Training the Network\nConnecting to PyMC\nBuilding a custom distribution\nPlug the custom distribution into a PyMC model\nInference example\nSetting the Stage #\n\nTo motivate the modeling effort expounded upon below, let's start by building the case for a particular class of models, beginning with an (somewhat stylized) original data analysis problem.\n\nConsider a dataset from the NeuroRacer experiment, illustrated below with an adapted figure from the original paper.\n\nThe player/subject in this experiment is tasked with steering a racing car along a curvy racetrack, while reacting appropriately to appearing traffic signs under time pressure. Traffic signs are either of the target or no target type, the players' reaction appropriately being a button press or no button press respectively.\n\nIn the lingo of cognitive scientists, we may consider this game a Go / NoGo type task (press or withhold depending on traffic sign), under extra cognitive load (steering the car across the racetrack).\n\nThis leaves us with four types of responses to analyse (see the figure below):\n\nCorrect button press (Correct Go)\nCorrect withhold (Correct NoGo)\nFalse button press (False Go)\nFalse withhold (False NoGo)\n\nWhat kind of data #\n\nCollecting reaction times (rt) and choices (responses) for each of the trials, our dataset will eventually look as follows.\n\nimport numpy as np\nimport pandas as pd\n\n# Generate example data\ndata = pd.DataFrame(np.random.uniform(size = (100,1)), columns = ['rt'])\ndata['response'] = 'go'\ndata['response'].values[int(data.shape[0] / 2):] = 'nogo'\ndata['trial_type'] = 'target'\ndata['trial_type'].values[int(data.shape[0] / 2):] = 'notarget'\n\ndata\n\n\trt\tresponse\ttrial_type\n0\t0.776609\tgo\ttarget\n1\t0.706098\tgo\ttarget\n2\t0.395347\tgo\ttarget\n3\t0.337480\tgo\ttarget\n4\t0.751433\tgo\ttarget\n...\t...\t...\t...\n\n100 rows × 3 columns\n\nThe model(s) #\n\nCognitive scientists have powerful framework for the joint analysis of reaction time and choice data: Sequential Sampling Models (SSMs).\n\nThe canonical model in this framework is the Drift Diffusion Model (or Diffusion Decision Model). We will take this model as a starting point to explain how it applies to the analysis of NeuroRacer data.\n\nThe basic idea behind the Drift Diffusion Model is the following. We represent the decision process between two options as a Gaussian random walk of a so-called evidence state. The random walk starts after some non-decision time period  following the stimulus appearance. The process then starts at a specific value (parameter ) and evolves according to a deterministic drift (parameter ), perturbed by Gaussian noise. Eventually this process reaches a value that crosses one of two boundaries, which determines the response given by the participant: go or no-go. The distance between the two boundaries is given by a fourth parameter ().\n\nWhich bound is reached, and the time of crossing, jointly determine the reaction time and choice. Hence, this model specifies a stochastic data generating process and we can define a likelihood function for this.\n\nHowever, this likelihood function may be quite hard to derive, and possibly too computationally expensive. As an alternative, we will use a general function approximation via Neural Networks in this tutorial.\n\nLet's first look at an illustration of the model and identify the quantities relevant for our example.\n\nA nice aspect of the Drift Diffusion Model (or Diffusion Decision Model) is that the parameters are interpretationally distinct.\n\n, the non-decision time component captures all aspects of decision-time not explicitly modeled as per the random walk process (e.g. motor-preparation, initial time-to-attentive-state etc. etc.)\n, provides global bias of the process towards one or the other choice. One can think of it as an a priori estimate of the underlying frequency of correct choices as per the experiment design.\n, is the rate with which evidence is consistently accumulated toward one or the other bound (in favor of one or the other choice). One can think of it as speed of processing.\n, represent a measure of the desired level of certainty before a decision is committed to. It is also referred to as decision caution.\n\nThe two quantities we will make explicit in the analyses are the following are (see also figure above),\n\n, the likelihood of observing a Go choice at time \n, the likelihood of \"observing\" a withheld button press, defined as the integral of  over .\n\nWe will focus on a simple analysis case, in which we observe hypothetical data from a single player, who plays the game for  trials,  of which are Go / target trials (the participant should press the button) and  of which are NoGo / no target trials (the participant should not press the button).\n\nWe will make a simplifying modeling assumption: the rate of evidence accumulation for a NoGo has the same magnitude as that of Go trials but with a flipped sign, meaning participants are less likely to press a button as time goes by. This allows us to estimate a single  parameter.\n\nHence we get  .\n\nMotivating Simulation Based Inference #\n\nThe Drift Diffusion Model actually has a (cumbersome) analytical likelihood, with specialized algorithms for fast evaluation. There are however many interesting variants for which fast computations are hampered by a lack of closed form solutions (see for example here and here).\n\nTake as one example the model illustrated in the figure below,\n\nConceptually the only difference is that the decision criterion, described in our simple Drift Diffusion Model above with a single parameter , does now vary with time (as a parametric function). This makes sense if e.g. the decision is supposed to be completed under deadline pressure (in fact this would be the case in our NeuroRacer example). As time progresses, it may be rational to decrease ones decision caution to force an, at least somewhat, informed decision over a guaranteed failure due to missing the deadline (for some reasearch along those lines see for example this paper).\n\nOn the other hand simulators for such variants tend to remain easy to code up (often a few lines in python do the job). A simulator but no likelihood? Welcome to the world of simulation based inference (SBI).\n\nSurveying the field of SBI is beyond the scope of this blog post (the paper above is a good start for those interested), but let it be said that SBI is the overarching paradigm from which we pick a specific method to construct our approach below.\n\nThe idea is the following. We start with a simulator for the DDM from which, given a set of parameters (, , , ) we can construct empirical likelihood functions for both  and  . For  we construct smoothed histograms (or Kernel Density Estimates), while for  we simply collect the respective choice probability from simulation runs.\n\nFrom these building blocks, we will construct training data to train two Multilayer Perceptrons (MLPs, read: small Neural Networks), one for each of the two parts of the overall likelihood.\n\nThese MLPs are going to act as our likelihood functions. We will call the network which represents  a LAN, for Likelihood Approximation Network. The network for  will be called a CPN, for Choice Probability Network. As we will see later, can then evaluate our data-likelihood via forward passes through the LAN and CPN.\n\nWe will then proceed by wrapping these trained networks into a custom PyMC distribution and finally get samples from our posterior of interest  via the Blackjax NUTS sampler, completing our walkthrough.\n\nWith all these steps ahead, let's get going!\n\nFrom model simulation to PyMC model #\nSimulating Data #\n\nIn favor of a digestible reading experience, we will use a convenience package to simulate data from the DDM model. This package not only allows us to simulate trajectories, but also includes utilities to directly produce data in a format suitable for downstream neural network training (which is our target here). The mechanics behind training data generation are described in this paper.\n\nFor some intuition, let's start with simulating and plotting a simple collection of  DDM trajectories, setting the parameters  as .\n\nfrom ssms.basic_simulators import simulator\n\nn_trajectories = 1000\nparameter_vector = np.array([1.0, 1.5, 0.5, 0.5, 0.5])\n\nsimulation_data = simulator(model = 'angle',\n                            theta = parameter_vector,\n                            n_samples = n_trajectories,\n                            random_state = 42)\n\nsimulation_data.keys()\n\ndict_keys(['rts', 'choices', 'metadata'])\n\n\nThe simulator returns a dictionary with three keys.\n\nrts, the reaction times for each choice under 2.\nchoices, here coded as  for lower boundary crossings and  for upper boundary crossings.\nmetadata, extra information about the simulator settings\n\nLet's use this to plot the reaction time distribution (negative reals refer to  choices) and choice probabilities. We will plot this for a few parameter settings to give some intuition about how the model behaves in response. Specifically we will vary the  parameter, holding all other parameters constant the values reported above.\n\n+ Show Code\nfrom matplotlib import pyplot as plt\nparameter_matrix = np.zeros((3, 4))\n\n# vary the first parameter across rows (the 'v' parameter in our case')\nparameter_matrix[:, 0] = np.linspace(-0.5, 0.5, 3)\n\n# set the rest to the values used above\nfor i in range(1, 4, 1):\n    parameter_matrix[:, i] = parameter_vector[i]\n    \n# Make Figure\nfig, axs = plt.subplots(3,2, figsize = (6, 8))\nfig.suptitle('DDM Simulations: vary v')\nfor i in range(3):\n    simulation_data_tmp = simulator(model = 'ddm',\n                                theta = parameter_matrix[i, :],\n                                n_samples = n_trajectories)\n    for j in range(2):\n        if j == 0:\n            # Reaction Times + Choices\n            axs[i, j].hist(np.squeeze(simulation_data_tmp['rts']) * np.squeeze(simulation_data_tmp['choices']),\n                       histtype = 'step',\n                       color = 'black',\n                       bins = 40,\n                       )\n            axs[i, j].set_title('v = ' + str(round(parameter_matrix[i, 0], 2)))\n            axs[i, j].set_xlim(-10, 10)\n            axs[i, j].set_xlabel('Reaction Times')\n            axs[i, j].set_ylabel('Freq')\n            axs[i, j]\n        else: \n            # Choice probabilities\n            p_up = np.sum(simulation_data_tmp['choices'] == 1.) / n_trajectories\n            choice_ps = [1 - p_up, p_up]\n            axs[i, j].bar(['choice = -1', 'choice = 1'], choice_ps, fill = None)\n            axs[i, j].set_ylabel('Probability')\n            axs[i, j].set_ylim(0, 1)\nfig.tight_layout()\nplt.show()\n\n\nTurning it into Training Data #\n\nWe will now use a couple of convenience functions from the ssm-simulators package, to generate training data for our Neural Networks. This will proceed in two steps. We first define two config dictionaries to specify properties of the simulation runs that will serve as the basis for our training data set.\n\nThe generator_config which specifies how to construct training data on top of basic simulations runs.\nThe model_config which specifies the properties of the core simulator.\n\nSecond, we will actually run the necessary simulations.\n\nLet's make the config dictionaries.\n\nNOTE:\n\nThe details here are quite immaterial. We simply need some way of generating training data of two types.\n\nOne (for the LAN), which has as features vectors of the kind  and as labels corresponding empirical log-likelihood evaluations .\n\nOne (for the CPN), which takes as features simply the parameter vectors  and as labels corresponding empirical choice probabilities .\n\n+ Show Code\n# MAKE CONFIGS\nfrom ssms.config import data_generator_config\nfrom ssms.config import model_config\nfrom copy import deepcopy\n\n# Generator Config\n\n# (We start from a supplied example in the ssms package)\nddm_generator_config = deepcopy(data_generator_config['lan']) \n\n# Specify generative model \n# (one from the list of included models in the ssms package / or a single string)\nddm_generator_config['dgp_list'] = 'ddm'\n\n# Specify number of parameter sets to simulate\nddm_generator_config['n_parameter_sets'] = 1000\n\n# Specify how many samples a simulation run should entail\n# (To construct an empirical likelihood)\nddm_generator_config['n_samples'] = 10000\n\n# Specify how many training examples to extract from \n# a single parameter vector\nddm_generator_config['n_training_samples_by_parameter_set'] = 2000\n\n# Specify folder in which to save generated data\nddm_generator_config['output_folder'] = 'data/training_data/ddm_high_prec/'\n\n# Model Config\nddm_model_config = model_config['ddm']\n\n\nWe are now in the position to actually run the simulations.\n\nIf you run this by yourself,\n\nBe aware that the next cell may run for a while (between a few minutes and an hour)\nMake sure the output_folder specified above exists.\n# MAKE DATA\nfrom ssms.dataset_generators import data_generator\nn_datasets = 20\n\n# Instantiate a data generator (we pass our configs)\nmy_dataset_generator = data_generator(generator_config = ddm_generator_config,\n                                      model_config = ddm_model_config)\n\nfor i in range(n_datasets):\n    print('Dataset: ', i + 1, ' of ', n_datasets)\n    training_data = my_dataset_generator.generate_data_training_uniform(save = True,\n                                                                        verbose = True)\n\n\nLet's take a quick look at the type of data we generated here (if you run this by yourself, pick one of the unique file names generated during your run):\n\nimport pickle\ntraining_data_example = pickle.load(open('data/training_data/ddm_high_prec/training_data_167fc318b85511ed81623ceceff2f96e.pickle', \n                                         'rb'))\n\ntraining_data_example.keys()\n\ndict_keys(['data', 'labels', 'choice_p', 'thetas', 'binned_128', 'binned_256', 'generator_config', 'model_config'])\n\n\nUnder the data key (this is a legacy name, it might more appropriately called features directly) we find the feature set we need for LANS. A matrix that contains columns [v, a, z, ndt, rt, choice]. In general, across simulator models, the leading columns contain the parameters of the model, the remaining columns contain columns concerning the output data (in our case: responses and choices).\n\ntraining_data_example['data'][:10, :]\n\narray([[-1.320654 ,  2.4610643,  0.7317903,  1.5463215,  3.5173764,\n        -1.       ],\n       [-1.320654 ,  2.4610643,  0.7317903,  1.5463215,  5.126489 ,\n        -1.       ],\n       [-1.320654 ,  2.4610643,  0.7317903,  1.5463215,  4.1766562,\n        -1.       ],\n       [-1.320654 ,  2.4610643,  0.7317903,  1.5463215,  5.331864 ,\n        -1.       ],\n       [-1.320654 ,  2.4610643,  0.7317903,  1.5463215,  3.1934366,\n        -1.       ],\n       [-1.320654 ,  2.4610643,  0.7317903,  1.5463215,  3.8244245,\n        -1.       ],\n       [-1.320654 ,  2.4610643,  0.7317903,  1.5463215,  5.069471 ,\n        -1.       ],\n       [-1.320654 ,  2.4610643,  0.7317903,  1.5463215,  6.12916  ,\n        -1.       ],\n       [-1.320654 ,  2.4610643,  0.7317903,  1.5463215,  4.563048 ,\n        -1.       ],\n       [-1.320654 ,  2.4610643,  0.7317903,  1.5463215,  4.2055674,\n        -1.       ]], dtype=float32)\n\n\nThe labels key, contains the empirical  labels.\n\ntraining_data_example['labels'][:10]\n\narray([-0.9136966 , -1.7495332 , -1.1017948 , -1.9210151 , -1.0180298 ,\n       -0.96904343, -1.7043545 , -2.6625948 , -1.3647802 , -1.1204832 ],\n      dtype=float32)\n\n\nThe final keys we will be interested in, concern the feature and label data useful for training the CPN networks. This network is a function from the model parameters (theta key) directly to choice probabilities (choice_p key).\n\ntraining_data_example['thetas'][:10]\n\narray([[-1.320654  ,  2.4610643 ,  0.7317903 ,  1.5463215 ],\n       [ 0.6651015 ,  2.032305  ,  0.43455952,  0.39412627],\n       [-0.50315803,  2.434122  ,  0.225012  ,  0.9221967 ],\n       [-0.2956373 ,  1.1780746 ,  0.3984416 ,  1.361724  ],\n       [-0.39936534,  1.170804  ,  0.57806057,  1.2206811 ],\n       [-1.1943408 ,  0.68256736,  0.6976298 ,  1.172115  ],\n       [ 0.7937759 ,  2.049422  ,  0.45930618,  0.48710603],\n       [-2.0405245 ,  2.453905  ,  0.7521208 ,  1.9120167 ],\n       [-2.8156106 ,  2.2226427 ,  0.69219965,  1.0444195 ],\n       [ 0.37362418,  1.775074  ,  0.42867982,  0.22735284]],\n      dtype=float32)\n\ntraining_data_example['choice_p'][:10]\n\narray([2.930e-02, 9.146e-01, 1.540e-02, 2.471e-01, 3.542e-01, 3.422e-01,\n       9.555e-01, 6.300e-03, 7.000e-04, 7.356e-01], dtype=float32)\n\n\nThere are a few other keys in the training_data_example dictionary. We can ignore these for the purposes of this blog post.\n\nWe are ready to move forward by turning our raw training data into a DataLoader object, which directly prepares for ingestion by the Neural Networks.\n\nThe DataLoader is supposed take care of:\n\nEfficiently reading in datafiles and\nturning them into batches to be ingested when training a Neural Network.\n\nAs has become somewhat of a standard, will work off of the Dataset class supplied by the torch.utils.data module in the PyTorch deep learning framework.\n\nThe key methods to define in our custom dataset are __getitem__() and __len__().\n\n__len__() helps us to understand the amount of batches contained in a complete run through the data (epoch in machine learning lingo). __getitem_() is the method called to retrieve the next batch of data.\n\nLet's construct it.\n\n+ Show Code\nimport torch\nfrom __future__ import annotations\nfrom typing import List\nfrom typing import Type\n\nclass DatasetTorch(torch.utils.data.Dataset):\n    def __init__(self,\n                 file_ids: List[str] | None = None,\n                 batch_size: int = 32,\n                 label_lower_bound: float | None = None,\n                 features_key: str = 'data',\n                 label_key: str = 'labels',\n                 ) -> Type[torch.utils.dataDataset]:\n        \n        # Initialization\n        self.batch_size = batch_size\n        self.file_ids = file_ids\n        self.indexes = np.arange(len(self.file_ids))\n        self.label_lower_bound = label_lower_bound\n        self.features_key = features_key\n        self.label_key = label_key\n        self.tmp_data = None\n\n        # Get metadata from loading a test file\n        self.__init_file_shape()\n\n    def __len__(self):\n        \"\"\"\n        Calculates number of batches per epoch.\n        \"\"\"\n        return int(np.floor((len(self.file_ids) * ((self.file_shape_dict['inputs'][0] // self.batch_size) * self.batch_size)) / self.batch_size))\n\n    def __getitem__(self, index: int):\n        \"\"\"\n        Return next batch.\n        \"\"\"\n        # Check if it is time to load the next file from disk\n        if index % self.batches_per_file == 0 or self.tmp_data == None:\n            self.__load_file(file_index = self.indexes[index // self.batches_per_file])\n\n        # Generate batch_ids\n        batch_ids = np.arange(((index % self.batches_per_file) * self.batch_size), \n                              ((index % self.batches_per_file) + 1) * self.batch_size, 1)\n        \n        # Make corresponding batch\n        X = self.tmp_data[self.features_key][batch_ids, :]\n        y = np.expand_dims(self.tmp_data[self.label_key][batch_ids], axis = 1)\n        \n        # Apply lower bound on labels\n        if self.label_lower_bound is not None:\n            y[y < self.label_lower_bound] = self.label_lower_bound \n        return X, y\n\n    def __load_file(self, file_index: int):\n        \"\"\"\n        Load new file if requested.\n        \"\"\"\n        # Load file and shuffle the indices\n        self.tmp_data = pickle.load(open(self.file_ids[file_index], 'rb'))\n        shuffle_idx = np.random.choice(self.tmp_data[self.features_key].shape[0], \n                                        size = self.tmp_data[self.features_key].shape[0],\n                                         replace = True)\n        self.tmp_data[self.features_key] = self.tmp_data[self.features_key][shuffle_idx, :]\n        self.tmp_data[self.label_key] = self.tmp_data[self.label_key][shuffle_idx]\n        return\n\n    def __init_file_shape(self):\n        \"\"\"\n        Set data shapes during initialization.\n        \"\"\"\n        # Function gets dimensionalities form a test data file \n        # (first in the supplied list of file names)\n        init_file = pickle.load(open(self.file_ids[0], 'rb'))\n        self.file_shape_dict = {'inputs': init_file[self.features_key].shape, \n                                'labels': init_file[self.label_key].shape}\n        self.batches_per_file = int(self.file_shape_dict['inputs'][0] / self.batch_size)\n        self.input_dim = self.file_shape_dict['inputs'][1]\n\n        if len(self.file_shape_dict['labels']) > 1:\n            self.label_dim = self.file_shape_dict['labels'][1]\n        else:\n            self.label_dim = 1\n        return\n\n\nLet's construct our training dataloaders for both our LAN and CPN networks (which we will define next). We use the DataLoader class in the torch.utils.data module to turn our Dataset class into an iterator.\n\nNOTE:\n\nTo not explode code blocks in this blog post, we will only concern ourselves with training data here, instead of including (as one should in a serious machine learning application) DataLoader classes for validation data as well. Defining validation data works analogously.\n\nNotice how we change the features_key and label_key arguments to access the relevant part of our training data files respectively for the LAN and CPN.\n\n+ Show Code\nimport os \nimport pickle\n\n# MAKE DATALOADERS\n\n# List of datafiles (here only one)\nfolder_ = 'data/training_data/ddm_high_prec/'\nfile_list_ = [folder_ + file_ for file_ in os.listdir(folder_) if '.ipynb' not in file_]\n\n# Training datasets\ntraining_dataset_lan = DatasetTorch(file_ids = file_list_,\n                                    batch_size = 8192,\n                                    label_lower_bound = np.log(1e-7),\n                                    features_key = 'data',\n                                    label_key = 'labels',\n                                    )\n\ntraining_dataset_cpn = DatasetTorch(file_ids = file_list_,\n                                    batch_size = 512,\n                                    features_key = 'thetas',\n                                    label_key = 'choice_p',\n                                    )\n\n# Training dataloaders\ntraining_dataloader_lan = torch.utils.data.DataLoader(training_dataset_lan,\n                                                      shuffle = True,\n                                                      batch_size = None,\n                                                      num_workers = 1,\n                                                      pin_memory = True\n                                                     )\n\ntraining_dataloader_cpn = torch.utils.data.DataLoader(training_dataset_cpn,\n                                                      shuffle = True,\n                                                      batch_size = None,\n                                                      num_workers = 1,\n                                                      pin_memory = True\n                                                     )\n\nBuilding and Training the Network #\n\nWe used the simulator to construct training data and constructed dataloaders on top of that. It is time to build and train our networks!\n\nWe will use the Flax python package for this purpose. Let's first define a basic neural network class, constrained to minimal functionality. We build such a class by inheriting from the nn.Module class in the flax.linen module and specifying two methods.\n\nThe setup() method, which will be run as a preparatory step upon instantiation.\nThe __call__() metod defines the forward pass through the network.\n+ Show Code\nfrom flax import linen as nn\nfrom frozendict import frozendict\nfrom typing import Sequence\n\nclass MLPJax(nn.Module):\n    \"\"\"\n    Basic Neural Network class as per the Flax package for neural network \n    modeling with Jax.\n    \"\"\"\n    layer_sizes: Sequence[int] = (100, 100, 100, 1)\n    activations: Sequence[str] = ('tanh', 'tanh', 'tanh', 'linear')\n    train: bool = True # if train = False, output applies transform f such that: f(train_output_type) = logprob\n    train_output_type: str = 'logprob'\n    activations_dict = frozendict({'relu': nn.relu,\n                                   'tanh': nn.tanh,\n                                   'sigmoid': nn.sigmoid\n                                  })\n        \n    def setup(self):\n        # Assign layers and activation functions as class attributes\n        self.layers = [nn.Dense(layer_size) for layer_size in self.layer_sizes]\n        self.activation_funs = [self.activations_dict[activation] for \\\n                                activation in self.activations if (activation != 'linear')]\n    \n    def __call__(self, inputs: Type[jax.numpy.array]) -> Type[jax.numpy.array]:\n        \"\"\"\n        This is used to define the forward pass, which will later be called via\n        mymodel.apply(state, input)\n        \"\"\"\n        \n        # Define forward pass\n        x = inputs\n        \n        # Cycle through layers\n        for i, lyr in enumerate(self.layers):\n            x = lyr(x)\n            if i != (len(self.layers) - 1):\n                x = self.activation_funs[i](x)\n            else:\n                if self.activations[i] == 'linear': \n                    pass\n                else:\n                    x = self.activation_funs[i](x)\n        \n        # Apply potential transform of outputs if in eval model\n        if not self.train and self.train_output_type == 'logits':\n            return - jax.numpy.log((1 + jax.numpy.exp(-x)))\n        else:\n            return x\n    \n    def make_forward_partial(self,\n                             state = None,\n                            ):\n        \"\"\"\n        Make a single-argument forward pass function (only network input needed instead\n        of needing to pass the network state as well).\n        \"\"\"\n\n        net_forward = partial(self.apply, state)\n        net_forward_jitted = jax.jit(net_forward)\n\n        return net_forward, net_forward_jitted\n\n\nNext we define a Neural Network trainer class. This will take a MLPJax instance and build the necessary infrastructure for network training around it. The approach roughly follows the suggestions in the Flax documentation.\n\n+ Show Code\nimport jax\nimport optax\nfrom optax import warmup_cosine_decay_schedule\nfrom optax import huber_loss\nfrom optax import sigmoid_binary_cross_entropy\nfrom optax import l2_loss\n\nfrom flax.training import train_state\nfrom functools import partial\n\nfrom tqdm import tqdm\n\nclass ModelTrainerJaxMLP:\n    def __init__(self,\n                 model: Type[nn.Module] = None,\n                 loss: Dict[str, Any] | str | None = None,\n                 train_dl: Type[torch.utils.data.DataLoader] | None = None,\n                 seed: int | None = None):\n        \n        # Provide some options for loss functions\n        self.loss_dict = {'huber': {'fun': huber_loss,\n                               'kwargs': {'delta': 1}},\n                          'mse': {'fun': l2_loss,\n                                 'kwargs': {}},\n                          'bcelogit': {'fun': sigmoid_binary_cross_entropy,\n                                      'kwargs': {}}\n                         }\n        \n        self.model = model\n        self.train_dl = train_dl\n        self.loss = loss\n        self.dataset_len = self.train_dl.dataset.__len__()\n        self.seed = seed\n        \n        self.__get_loss()\n        self.apply_model = self.__make_apply_model()\n        self.update_model = self.__make_update_model()\n            \n    def __get_loss(self):\n        self.loss = partial(self.loss_dict[self.loss]['fun'], \n                            **self.loss_dict[self.loss]['kwargs'])\n    \n    def __make_apply_model(self):\n        \"\"\"\n        Construct jitted forward and backward pass.\n        \"\"\"\n        @jax.jit\n        def apply_model_core(state, features, labels):\n            def loss_fn(params):\n                pred = state.apply_fn(params, features)\n                loss = jax.numpy.mean(self.loss(pred, labels))\n                return loss, pred\n            \n            grad_fn = jax.value_and_grad(loss_fn, has_aux = True)\n            (loss, pred), grads = grad_fn(state.params)\n            return grads, loss\n        \n        return apply_model_core\n    \n    def __make_update_model(self):\n        \"\"\"\n        Construct jitted optimizer step\n        \"\"\"\n        @jax.jit\n        def update_model(state, grads):\n            return state.apply_gradients(grads = grads)\n        return update_model\n        \n    def create_train_state(self, rng: int, n_epochs: int):\n        \"\"\"\n        Create a TrainState object that is essentially a convenience object for \n        storing a given networks' foward pass, parameter state, and optimizer state.\n        \"\"\"\n        params = self.model.init(rng, jax.numpy.ones((1, self.train_dl.dataset.input_dim)))\n        lr_schedule = warmup_cosine_decay_schedule(init_value = 0.0002,\n                                                   peak_value = 0.02,\n                                                   warmup_steps = self.dataset_len,\n                                                   decay_steps = self.dataset_len * \\\n                                                                 n_epochs,\n                                                   end_value = 0.0)\n        tx = optax.adam(learning_rate = lr_schedule)\n        return train_state.TrainState.create(apply_fn = self.model.apply,\n                                             params = params,\n                                             tx = tx)\n    \n    def run_epoch(self,\n                  state,\n                  train: bool = True):\n        \"\"\"\n        Run single epoch\n        \"\"\"\n        \n        epoch_loss = []\n        for X, y in tqdm(self.train_dl):\n            X_jax = jax.numpy.array(X)\n            y_jax = jax.numpy.array(y)\n            \n            grads, loss = self.apply_model(state, X_jax, y_jax)\n            state = self.update_model(state, grads)\n            epoch_loss.append(loss)\n                    \n        mean_epoch_loss = np.mean(epoch_loss)\n        return state, mean_epoch_loss\n    \n    def train(self,\n              n_epochs: int = 25):\n        \"\"\"\n        Train the network for the chosen number of epochs.\n        \"\"\"\n        \n        # Initialize network\n        rng = jax.random.PRNGKey(self.seed)\n        rng, init_rng = jax.random.split(rng)\n        state = self.create_train_state(init_rng,\n                                        n_epochs = n_epochs)\n\n        # Training loop over epochs\n        for epoch in range(n_epochs):\n            state, train_loss = self.run_epoch(state, train = True)\n            print('Epoch: {} / {}, test_loss: {}'.format(epoch, n_epochs, \n                                                         train_loss))\n\n        self.state = state\n        return state\n\n\nPreparations are all you need! We can now train our LAN and CPN with a few lines of code, making use of our previously defined classes.\n\n# Initialize LAN\nnetwork_lan = MLPJax(train = True, # if train = False, output applies transform f such that: f(train_output_type) = logprob\n                     train_output_type = 'logprob')\n\n# Set up the model trainer                                \nModelTrainerLAN = ModelTrainerJaxMLP(model = network_lan,\n                                     train_dl = training_dataloader_lan,\n                                     loss = 'huber',\n                                     seed = 123)\n\n# Train LAN\nmodel_state_lan = ModelTrainerLAN.train(n_epochs = 10)\n\n100%|██████████| 4880/4880 [00:30<00:00, 159.70it/s]\n\n\nEpoch: 1 / 10, test_loss: 0.14862245321273804\n\n\n ...\n\n\n100%|██████████| 4880/4880 [00:28<00:00, 169.85it/s]\n\n\nEpoch: 10 / 10, test_loss: 0.01756889559328556\n\n# Initialize CPN\nnetwork_cpn = MLPJax(train = True,\n                     train_output_type = 'logits')\n\n# Set up the model trainer                                \nModelTrainerCPN = ModelTrainerJaxMLP(model = network_cpn,\n                                     train_dl = training_dataloader_cpn,\n                                     loss = 'bcelogit',\n                                     seed = 456)\n\n# Train CPN\nmodel_state_cpn = ModelTrainerCPN.train(n_epochs = 20)\n\n100%|██████████| 20/20 [00:02<00:00,  9.14it/s]\n\n\nEpoch: 1 / 20, test_loss: 0.42765116691589355\n\n\n...\n\n\n100%|██████████| 20/20 [00:00<00:00, 27.14it/s]\n\nEpoch: 20 / 20, test_loss: 0.30409830808639526\n\n\n\nConnecting to PyMC #\n\nAt this point we have two networks ready (we will later see example output that illustrate the behavior / quality of the approximation), which can be used as differentiable approximations to likelihood evaluations. The figure below should illustrate the respective function of each network (e.g. in the Go condition). This may help as a guiding visualization for the subsequent content.\n\nA CPN, which we will use as an approximator to,\n\nand,\n\nA LAN, which we will use as an approximator to,\n\nwhere  refers to the log-likelihood.\n\nTogether the CPN and the LAN allow us to construct a likelihood for a complete dataset from the NeuroRacer game.\n\nTake the complete likelihood for a dataset of size , for trials in which the traffic sign warrants a button press (Go Condition). We can split our dataset into two parts.\n\nGo condition, Go choice (we observe a reaction time): \nGo condition, NoGo choice (we don't observe a reaction time): \n\nThe log likelihood of the Go condition data can now be represented as: \n\nFor the NoGo Condition, we essentially apply the same logic so that the log likelihood of the NoGo condition data can now be represented as: \n\nAs per our modeling assumption we switch set  , to get the full data log-likelihood,\n\nBuilding a custom distribution #\n\nAll pieces are lined up to start building a custom distribution for eventual use in a PyMC model. The starting point has to be the construction of a custom likelihood, as a valid PyTensor Op. For this purpose we use the NetworkLike class below. It allows us to construct proper log-likelihoods from our two networks.\n\nWhat do we mean by proper log-likelihood?\n\nA valid Jax function that takes in parameters, processes the input data, performs the appropriate forward pass through the networks, and finally sums the resulting trial-wise log-likelihoods to give us a data-log-likelihood. This is taken care of by the make_logp_jax_funcs() method.\n\nFinally we need to turn these isolated likelihood functions into a valid PyTensor Op, which is taken care of by the make_jax_logp_ops() function. Note how we also register our log-likelihood function directly as a Jax log-likelihood (unwrap it) using the jax.funcify decorator with the logp_op_dispatch() method. This log-likelihood function does not need to be compiled (note how we pass the logp_nojit likelihood there), which will instead be taken care of by any of the Jax sampler that PyMC provides (via NumPyro, or BlackJax)\n\nNOTE:\n\nThe below code is a little involved and could be hard to digest on a first pass. Consider looking into the excellent tutorials in the PyMC docs and the PyMC Labs Blog on similar topics.\n\nSpecifically, the tutorial on using a blackbox likelihood function, the tutorial on custom distributions, the tutorial on wrapping jax functions into PyTensor Ops.\n\nFinally there is an excellent tutorial from PyMC Labs, which incorporates Flax to train Bayesian Neural Networks (amongst other things): A different spin on our story here, not exactly equivalent, but helpful to understand the scope of use-cases encompassed at the intersection of Neural Networks and the Bayesian workflow.\n\n+ Show Code\nfrom os import PathLike\nfrom typing import Callable, Tuple\n\nimport pytensor \npytensor.config.floatX = \"float32\"\nimport pytensor.tensor as pt\nimport jax.numpy as jnp\nimport numpy as np\nfrom pytensor.graph import Apply, Op\nfrom pytensor.link.jax.dispatch import jax_funcify\nfrom jax import grad, jit\nfrom numpy.typing import ArrayLike\n\nLogLikeFunc = Callable[..., ArrayLike]\nLogLikeGrad = Callable[..., ArrayLike]\n\nimport pymc as pm\nfrom pytensor.tensor.random.op import RandomVariable\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nclass NetworkLike:\n    @classmethod\n    def make_logp_jax_funcs(\n        cls,\n        model = None,\n        n_params: int | None = None,\n        kind: str = 'lan',\n    ) -> Tuple[LogLikeFunc, LogLikeGrad, LogLikeFunc,]:\n        \"\"\"Makes a jax log likelihood function from flax network forward pass.\n        Args:\n            model: A path or url to the ONNX model, or an ONNX Model object\n            already loaded.\n            compile: Whether to use jit in jax to compile the model.\n        Returns: A triple of jax or Python functions. The first calculates the\n            forward pass, the second calculates the gradient, and the third is\n            the forward-pass that's not jitted.\n        \"\"\"\n        if kind == 'lan':\n            def logp_lan(data: np.ndarray, *dist_params) -> ArrayLike:\n                \"\"\"\n                Computes the sum of the log-likelihoods given data and arbitrary\n                numbers of parameters assuming the trial by trial likelihoods\n                are derived from a LAN.\n                Args:\n                    data: response time with sign indicating direction.\n                    dist_params: a list of parameters used in the likelihood computation.\n                Returns:\n                    The sum of log-likelihoods.\n                \"\"\"\n\n                # Makes a matrix to feed to the LAN model\n                params_matrix = jnp.repeat(\n                    jnp.stack(dist_params).reshape(1, -1), axis=0, repeats=data.shape[0]\n                )\n\n                # Set 'v' parameters depending on condition\n                params_matrix = params_matrix.at[:, 0].set(params_matrix[:, 0] * data[:, 2])\n\n                # Stack parameters and data to have full input\n                input_matrix = jnp.hstack([params_matrix, data[:, :2]])\n\n                # Network forward and sum\n                return jnp.sum(\n                    jnp.squeeze(model(input_matrix))\n                )\n            \n            logp_grad_lan = grad(logp_lan, argnums=range(1, 1 + n_params))\n            return jit(logp_lan), jit(logp_grad_lan), logp_lan\n            \n        elif kind == 'cpn':\n            def logp_cpn(data: np.ndarray, *dist_params) -> ArrayLike:\n                \"\"\"\n                Computes the sum of the log-likelihoods given data and arbitrary\n                numbers of parameters assuming the trial-by-trial likelihood derive for a CPN.\n                Args:\n                    data: response time with sign indicating direction.\n                    dist_params: a list of parameters used in the likelihood computation.\n                Returns:\n                    The sum of log-likelihoods.\n                \"\"\"\n\n                # Makes a matrix to feed to the LAN model\n                n_nogo_go_condition = jnp.sum(data > 0)\n                n_nogo_nogo_condition = jnp.sum(data < 0)\n                dist_params_go  = jnp.stack(dist_params).reshape(1, -1)\n\n                # AF-TODO Bugfix here !\n                dist_params_nogo = jnp.stack(dist_params).reshape(1, -1)\n                dist_params_nogo = dist_params_nogo.at[0].set((-1) * dist_params_nogo[0])\n\n                net_in = jnp.vstack([dist_params_go, dist_params_nogo])\n\n                net_out = jnp.squeeze(model(net_in))\n\n                out = (jnp.log(1 - jnp.exp(net_out[0])) * n_nogo_go_condition) + \\\n                        (jnp.log(1 - jnp.exp(net_out[1])) * n_nogo_nogo_condition)\n\n                return out\n\n            logp_grad_cpn = grad(logp_cpn, argnums=range(1, 1 + n_params))\n            return jit(logp_cpn), jit(logp_grad_cpn), logp_cpn\n    \n    @staticmethod\n    def make_jax_logp_ops(\n        logp: LogLikeFunc,\n        logp_grad: LogLikeGrad,\n        logp_nojit: LogLikeFunc,\n    ) -> LogLikeFunc:\n        \"\"\"Wraps the JAX functions and its gradient in Pytensor Ops.\n        Args:\n            logp: A JAX function that represents the feed-forward operation of the\n                LAN network.\n            logp_grad: The derivative of the above function.\n            logp_nojit: A Jax function\n        Returns:\n            An pytensor op that wraps the feed-forward operation and can be used with\n            pytensor.grad.\n        \"\"\"\n\n        class LogpOp(Op):\n            \"\"\"Wraps a JAX function in an pytensor Op.\"\"\"\n\n            def make_node(self, data, *dist_params):\n                inputs = [\n                    pt.as_tensor_variable(data),\n                ] + [pt.as_tensor_variable(dist_param) for dist_param in dist_params]\n\n                outputs = [pt.scalar()]\n\n                return Apply(self, inputs, outputs)\n\n            def perform(self, node, inputs, output_storage):\n                \"\"\"Performs the Apply node.\n                Args:\n                    inputs: This is a list of data from which the values stored in\n                        output_storage are to be computed using non-symbolic language.\n                    output_storage: This is a list of storage cells where the output\n                        is to be stored. A storage cell is a one-element list. It is\n                        forbidden to change the length of the list(s) contained in\n                        output_storage. There is one storage cell for each output of\n                        the Op.\n                \"\"\"\n                result = logp(*inputs)\n                output_storage[0][0] = np.asarray(result, dtype=node.outputs[0].dtype)\n\n            def grad(self, inputs, output_grads):\n                results = lan_logp_grad_op(*inputs)\n                output_gradient = output_grads[0]\n                return [\n                    pytensor.gradient.grad_not_implemented(self, 0, inputs[0]),\n                ] + [output_gradient * result for result in results]\n\n        class LogpGradOp(Op):\n            \"\"\"Wraps the gradient opearation of a jax function in an pytensor op.\"\"\"\n\n            def make_node(self, data, *dist_params):\n                inputs = [\n                    pt.as_tensor_variable(data),\n                ] + [pt.as_tensor_variable(dist_param) for dist_param in dist_params]\n                outputs = [inp.type() for inp in inputs[1:]]\n\n                return Apply(self, inputs, outputs)\n\n            def perform(self, node, inputs, outputs):\n                results = logp_grad(inputs[0], *inputs[1:])\n\n                for i, result in enumerate(results):\n                    outputs[i][0] = np.asarray(result, dtype=node.outputs[i].dtype)\n\n        lan_logp_op = LogpOp()\n        lan_logp_grad_op = LogpGradOp()\n\n        # Unwraps the JAX function for sampling with JAX backend.\n        @jax_funcify.register(LogpOp) # Can fail in notebooks\n        def logp_op_dispatch(op, **kwargs):  # pylint: disable=W0612,W0613\n            return logp_nojit\n\n        return lan_logp_op\n\n\nThe likelihood class will come in handy when defining our PyMC model below .\n\nWe now construct simple forward functions for our networks (lan_forward(), cpn_forward()). We use the make_forward_partial() method of our previously defined MLPJax class.\n\nFirst we instantiate the networks in evaluation mode. The make_forward_partial() function then attaches our trained parameters to the usual Flax forward call (which takes in two arguments, the parameters and the model input) so that we can call lan_forward() and cpn_forward() with a single argument, the input data to be pushed through the respective network.\n\nAs you can check above, the work is done by the partial() function.\n\n# Initialize LAN in evaluation mode\nnetwork_lan_eval = MLPJax(train = False,\n                          train_output_type = 'logprob')\n\n# Make jitted forward passes (with fixed weights)\nlan_forward, _ = network_lan_eval.make_forward_partial(state = ModelTrainerLAN.state.params)\n\n\n# Initialize CPN in evaluation mode\nnetwork_cpn_eval = MLPJax(train = False,\n                          train_output_type = 'logits')\n\n\n# Make jitted forward passes (with fixed weights)\ncpn_forward, _ = network_cpn_eval.make_forward_partial(state = ModelTrainerCPN.state.params)                     \n\n\nAs a quick aside, to illustrate the performance of the Networks, we plot their behavior below.\n\nFirst, consider the LAN, which gives us choice / reaction time distributions directly. We will vary the  parameter to illustrate how the likelihood produced by the network varies in response.\n\n+ Show Code\n# Loop over parameter configurations to plot\n# multiple LAN outputs\nfor i in np.linspace(1., 2., 20):\n    inp_ = np.zeros((2000, 6))\n    inp_[:, 0] = i # v parameter --> varies\n    inp_[:, 1] = 1.0 # a parameter\n    inp_[:, 2] = 0.5 # z parameter\n    inp_[:, 3] = 0.5 # ndt parameter\n    inp_[:, 4] = np.concatenate([np.linspace(5, 0, 1000),\n                                 np.linspace(0, 5, 1000)]) # rt\n    inp_[:, 5] = np.concatenate([(-1)*np.ones(1000), np.ones(1000)]) # choices\n\n\n    plt.plot(inp_[:, 4] * inp_[:, 5], jnp.exp(lan_forward(inp_)), color = 'black', alpha = 0.2)\n    plt.title('LAN likelihood for varying v parameter')\n    plt.xlabel('Reaction Time')\n    plt.ylabel('Density')\n\n\nNext we consider the performance of the CPN which, remember, spits out choice probabilities only. In this plot we vary the  parameter on the x-axis, and show how the choice probabilities produced by the network vary in reponse. This is repeated for multiple levels of the  (or bias) parameter.\n\n+ Show Code\n# Vary z in outer loop\nfor i in np.linspace(0.1, 0.9, 10):\n    dat_tmp = np.zeros((1000, 4))\n    dat_tmp[:, 0] = np.linspace(-2, 2, 1000) # vary v parameter\n    dat_tmp[:, 1] = 2.0 # a\n    dat_tmp[:, 2] = i # z\n    dat_tmp[:, 3] = 1. # ndt / t\n    plt.plot(dat_tmp[:, 0], jnp.exp(cpn_forward(dat_tmp)), color = 'black')\n    \nplt.ylim(0, 1)\nplt.title('CPN choice probabilities for varying z')\nplt.xlabel('v parameter value')\nplt.ylabel('P(choice = 1)')\n\n\nThe outputs of the networks behave very regularly which is reassuring. To see if they match the real data generation process well, let's consider our previous figure on simulator behavior.\n\n+ Show Code\nparameter_matrix = np.zeros((3, 4))\n\n# vary the first parameter across rows (the 'v' parameter in our case')\nparameter_matrix[:, 0] = np.linspace(-0.5, 0.5, 3)\n\n# set the rest to the values used above\nfor i in range(1, 4, 1):\n    parameter_matrix[:, i] = parameter_vector[i]\n    \n# Make Figure\nfig, axs = plt.subplots(3,2, figsize = (6, 8))\nfig.suptitle('DDM Simulations: vary v')\nfor i in range(3):\n    simulation_data_tmp = simulator(model = 'ddm',\n                                theta = parameter_matrix[i, :],\n                                n_samples = n_trajectories)\n    \n    # LAN\n    inp_ = np.zeros((2000, 6))\n    inp_[:, 0] = parameter_matrix[i, 0]  # v parameter --> varies\n    inp_[:, 1] = parameter_matrix[i, 1], # a parameter\n    inp_[:, 2] = parameter_matrix[i, 2], # z parameter\n    inp_[:, 3] = parameter_matrix[i, 3], # ndt parameter\n    inp_[:, 4] = np.concatenate([np.linspace(5, 0, 1000),\n                                 np.linspace(0, 5, 1000)]) # rt\n    inp_[:, 5] = np.concatenate([(-1)*np.ones(1000), np.ones(1000)]) # choices\n    \n    for j in range(2):\n        if j == 0:\n            # Reaction Times + Choices\n            # Simulator\n            axs[i, j].hist(np.squeeze(simulation_data_tmp['rts']) * np.squeeze(simulation_data_tmp['choices']),\n                       histtype = 'step',\n                       color = 'black',\n                       bins = 40,\n                       density = True,\n                       )\n            \n            # LAN\n            axs[i, j].plot(inp_[:, 4] * inp_[:, 5], jnp.exp(lan_forward(inp_)), \n                           color = 'blue', alpha = 0.5)\n\n            axs[i, j].set_title('v = ' + str(round(parameter_matrix[i, 0], 2)))\n            axs[i, j].set_xlim(-10, 10)\n            axs[i, j].set_xlabel('Reaction Times')\n            axs[i, j].set_ylabel('Freq')\n            \n\n        else: \n            # Choice probabilities\n            p_up = np.sum(simulation_data_tmp['choices'] == 1.) / n_trajectories\n            choice_ps = [1 - p_up, p_up]\n            \n            # Simulator\n            axs[i, j].bar(['choice = -1', 'choice = 1'], choice_ps, fill = None)\n            \n            # CPN\n            axs[i, j].scatter(['choice = -1', 'choice = 1'], \n                              [(1 - jnp.exp(cpn_forward(parameter_matrix[i, :]))), \n                               jnp.exp(cpn_forward(parameter_matrix[i, :]))],\n                              color = 'blue', alpha = 0.5)\n            \n            axs[i, j].set_ylabel('Probability')\n            axs[i, j].set_ylim(0, 1)\n            \nfig.tight_layout()\nplt.show()\n\n\nAs we can see, the network outputs (shown in blue ) follow the simulation data very well.\n\nNOTE:\n\nWe emphasize that for serious applications we are better served using a much larger training data set. The scale of the simulation run here was chosen to make running the code in this blog-post feasible on local machines in a reasonable amount of time.\n\nPlug the custom likelihoods into a PyMC model #\n\nNow the hard work in the previous section culminates into actual results. We are able to construct our PyMC model by assembling the pieces we built in the previous sections. We instantiate our LAN and CPN based likelihood ops using the methods defined in our NetworkLike class. First, we define simple like likelihood functions via the make_logp_jax_funcs() method, then we construct the actual PyTensor LogOp's, which will be used directly in the PyMC model below.\n\n# Instantiate LAN logp functions\nlan_logp_jitted, lan_logp_grad_jitted, lan_logp = NetworkLike.make_logp_jax_funcs(\n                                                                                                 model = lan_forward,\n                                                                                                 n_params = 4,\n                                                                                                 kind = \"lan\")\n\n# Turn into logp op\nlan_logp_op = NetworkLike.make_jax_logp_ops(\n                                logp = lan_logp_jitted,\n                                logp_grad = lan_logp_grad_jitted,\n                                logp_nojit = lan_logp)\n\n# Instantiate CPN logp functions\ncpn_logp_jitted, cpn_logp_grad_jitted, cpn_logp = NetworkLike.make_logp_jax_funcs(\n                                                                                                    model = cpn_forward,\n                                                                                                    n_params = 4,\n                                                                                                    kind = \"cpn\")\n\n# Turn into logp op\ncpn_logp_op = NetworkLike.make_jax_logp_ops(\n                                logp = cpn_logp_jitted,\n                                logp_grad = cpn_logp_grad_jitted,\n                                logp_nojit = cpn_logp)\n\n\nFinally, let's define a function that constructs our PyMC model for us. Note how we use our likelihood ops, the lan_logp_op() and the cpn_logp_op() respectively to define two pm.Potential() functions. You can learn more about pm.Potential() in the docs, and more connected to blackbox likelihoods, in this helpful basic tutorial.\n\ndef construct_pymc_model(data: Type[pd.DataFrame] | None = None):\n    \"\"\"\n    Construct our PyMC model given a dataset.\n    \"\"\"\n    \n    # Data preprocessing:\n    # We expect three columns [rt, choice, condition(go or nogo)]\n    # We split the data according to whether the choice is go or nogo\n    data_nogo = data.loc[data.choice < 0, :]['is_go_trial'].values\n    data_go = data.loc[data.choice > 0, :].values\n    \n    with pm.Model() as ddm:\n        # Define simple Uniform priors\n        v = pm.Uniform(\"v\", -3.0, 3.0)\n        a = pm.Uniform(\"a\", 0.3, 2.5)\n        z = pt.constant(0.5)\n        t = pm.Uniform(\"t\", 0.0, 2.0)\n        \n        pm.Potential(\"choice_rt\", lan_logp_op(data_go, v, a, z, t))\n        pm.Potential(\"choice_only\", cpn_logp_op(data_nogo, v, a, z, t))\n        \n    return ddm\n\nInference example #\n\nWe are nearing the end of this blog-post (promised). All that remains is to simply try it out. At this point we can simulate some synthetic Neuroracer experiment data, fire up our newly designed PyMC model and run our MCMC sampler for parameter inference.\n\nWe pick a set of parameters, and following our modeling assumptions, we apply  for the trials we assign to the NoGo condition.\n\n# Let's make some data \nfrom ssms.basic_simulators import simulator\nparameters = {'v': 1.0,\n              'a': 1.5,\n              'z': 0.5,\n              't': 0.5}\n\nparameters_go = [parameters[key_] for key_ in parameters.keys()]\nparameters_nogo = [parameters[key_] if key_ != 'v' else ((-1)*parameters[key_]) for key_ in parameters.keys()]\n\n# Run simulations for each condition (go, nogo)\nsim_go = simulator(theta = parameters_go, model = 'ddm', n_samples = 500) \nsim_nogo = simulator(theta = parameters_nogo, model = 'ddm', n_samples = 500)\n\n# Process data and add a column that signifies whether the trial,\n# belongs to a go (1) or nogo (-1) condition\ndata_go_condition = np.hstack([sim_go['rts'], sim_go['choices'], np.ones((500, 1))])\ndata_nogo_condition = np.hstack([sim_nogo['rts'], sim_nogo['choices'], (-1)*np.ones((500, 1))])\n\n# Stack the two datasets and turn into DataFrame\ndata = np.vstack([data_go_condition, data_nogo_condition]).astype(np.float32)\ndata_pd = pd.DataFrame(data, columns = ['rt', 'choice', 'is_go_trial'])\n\n\nOur dataset at hand, we can now intiate the PyMC model.\n\nddm_blog = construct_pymc_model(data_pd)\n\n\nLet's visualize the model structure.\n\npm.model_to_graphviz(ddm_blog)\n\n\nThe graphical model nicely illustrates how we handle the Go choices and NoGo choice via separate likelihod objects, while our basic parameters feed into both of these.\n\nNote that we don't fit the  parameter here, which is to avoid known issues with parameter identifiability in case it was included.\n\nWe are now ready to sample...\n\nfrom pymc.sampling import jax as pmj\n\n# Just to keep the blog-post pretty automatically\nimport warnings \nwarnings.filterwarnings('ignore')\n\nwith ddm_blog:\n    ddm_blog_traces_numpyro = pmj.sample_numpyro_nuts(\n            chains=2, draws=2000, tune=500, chain_method=\"vectorized\"\n            )\n\nCompiling...\nCompilation time =  0:00:08.502226\nSampling...\n\n\nsample: 100%|██████████| 2500/2500 [00:17<00:00, 140.40it/s]\n\n\nSampling time =  0:01:03.744460\nTransforming variables...\nTransformation time =  0:00:01.292058\n\n\nAs a last step we can check our posterior distributions. Did all of this actually work out?\n\nNOTE:\n\nThe posterior mass here may be somewhat off the mark when comparing to the ground truth parameters. While this hints at a calibration issue, it was conscious approach to trade-off on precision to avoid potentially very long runtimes for this tutorial. We can in general improve the performance of our neural network by training on much more synthetic data (which in real applications is advisable). This would however make running this notebook very cumbersome, which we in turn encourage you to try!\n\nimport arviz as az\naz.plot_posterior(ddm_blog_traces_numpyro,\n                  kind = 'hist',\n                  **{'color': 'black', \n                     'histtype': 'step'},\n                  ref_val = {'v': [{'ref_val': parameters['v']}],\n                             'a': [{'ref_val': parameters['a']}],\n                             't': [{'ref_val': parameters['z']}]\n                            },\n                  ref_val_color = 'green')\n\narray([<Axes: title={'center': 'v'}>, <Axes: title={'center': 'a'}>,\n       <Axes: title={'center': 't'}>], dtype=object)\n\n\nA somewhat long but hopefully rewarding tutorial is hereby finished. We hope you see some potential in this approach. Many extensions are possible, from the choice of neural network architectures to the structure of the PyMC model a plethora of options arise. As a lowest bar, we hope that this may serve you as another take on a tutorial concerning custom distributions in PyMC.\n\nFor related tutorials check out:\n\nBuilding blackbox likelihood functions\n\nWorking with custom distributions\n\nWrapping jax functions into PyTensor Ops\n\nODEs, Bayesian Neural Nets with Flax and PyMC\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/how-realistic-are-synthetic-consumers": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nEvaluating LLMs on Political and Lifestyle Choices\nParty Identification\nPreliminary Results\nTelevision Hours\nConclusions\nHow Realistic Are Synthetic Consumers?\n\nJune 03, 2025\n\nBy Allen Downey\n\nEvaluating LLMs on Political and Lifestyle Choices#\n\nSynthetic consumers based on large language models (LLMs) have potential to make market research and marketing faster and less expensive, but only if their responses are consistent with real consumers in the target market.\n\nTo see whether they are, the ideal experiment is to compare synthetic consumers with a panel of human beings – and at PyMC Labs, we are working with clients to do just that. But those experiments target specific markets that might not generalize, and the datasets are proprietary. So, in order to generate results we can share more widely, we are undertaking a series of experiments to test synthetic consumers using public datasets and more general questions.\n\nWe’ll start with the General Social Survey, which samples adult residents of the United States and asks questions about their attitudes and beliefs on a variety of topics. The responses are categorical, so they lend themselves to quantitative evaluation. In a future experiment we’ll look at another survey that includes open-ended text responses.\n\nParty Identification#\n\nThe first question we’ll consider asks about political party identification: “Generally speaking, do you think of yourself as a Republican, Democrat, Independent, or what?” The responses are on a seven point scale from “Strong Democrat” to “Strong Republican”. We selected this question in part because choosing a political party is arguably analogous to a consumer choice, and because we expect the responses to be moderately predictable.\n\nTo see whether the responses we get from synthetic consumers are consistent with real people, we randomly selected from the 2022 GSS a test set of 100 respondents. For each respondent, we collected demographic information including age, sex, race, education income, occupation, and religious preference, as well as responses to the following question about political alignment, “Where would you place yourself on a seven-point scale on which the political views that people might hold are arranged from extremely liberal--point 1--to extremely conservative--point 7”\n\nTo test synthetic consumers, we composed a prompt with three elements:\n\nInstructions for the LLM to adopt the role of a person with the given characteristics,\nThe text of the GSS question about party identification (quoted above), and\nInstructions to respond with one of the labels on the seven-point scale.\n\nThen we parse the responses to identify the label predicted by the LLM. To evaluate the responses, we treat the seven-point scale as numerical and compute the mean absolute error (MAE), averaged across the respondents in the test set.\n\nWe compare the results from the synthetic consumers to two alternatives:\n\nA random forest regressor trained with data from 3000 respondents, and\nA baseline classifier that always guesses the median value.\n\nIf synthetic consumers accurately reflect real-world demographic relationships between demographics and party identification, their responses should outperform the naive baseline. However, because they rely only on implicit statistical associations on an external dataset (the internet) we expect their accuracy to fall short of supervised models like random forests trained specifically on this data. Although this comparison is somewhat unfair—akin to evaluating an unsupervised against a supervised method— it is helpful to contextualize the accuracy of this approach. Ultimately, the key advantage of synthetic consumers is their ability to answer novel questions immediately, without additional training.\n\nPreliminary Results#\n\nIn our first experiment, we tested synthetic consumers based on five readily available LLMs: GPT-4o, GPT-o3-mini, Claude 3.7 Sonnet, DeepSeek R1 Distill, and Gemini 2.0 Flash. The following figure shows the results.\n\nEach line represents a run with a different random seed, which controls the selection of the training and test sets. The gray lines show the range between the performance of the baseline classifier (on the right) and the random forest (on the left). The markers show the performance of synthetic consumers with the same prompt submitted to different LLMs.\n\nThe performance of most models is good, often comparable to the random forest model. GPT-o3-mini and Gemini 2.0 Flash are the most consistent, sometimes performing better than the random forest. DeepSeek Distill is consistently the worst of these models and sometimes worse than the baseline. It is a smaller model than the others, which might account for the difference.\n\nTo see whether smaller models generally perform worse, we also tested GPT-4o mini, Claude 3 Opus, Mixtral 8x7b, and Meta Llama 3 8b Instruct. The following figure shows the results. Gemini 2.0 Flash is included again for comparison. Note that not all LLMs are included on the bottom three lines.\n\nSome of these models perform better than others – Claude generally does well, GPT and Llama are not as good. However, even with smaller models, in many cases, the performance of the best LLM is comparable to that of the random forest.\n\nFinally, to confirm that the demographic information included in the prompt informs the responses, we ran the same test with a generic prompt that did not include demographic information. The results were consistently bad and often worse than the baseline. We conclude that LLMs represent statistically valid information about the relationships between demographic variables, political alignment, and choice of political parties. To the degree that party affiliation is analogous to a consumer choice, these results suggest that LLMs can be accurate models of consumers.\n\nTelevision Hours#\n\nAs a second example, we tested a question we expect to be less predictable: “On the average day, about how many hours do you personally watch television?\" We grouped responses into five categories: \"one or less\", \"two hours\", \"three hours\", \"four or five hours\", \"six or more hours\".\n\nThe prompt we constructed contains the same demographic information as in the previous experiment, the question text from the GSS, and the five categorical responses. Again, we parsed the responses, compared to the real respondents, and computed the mean absolute error (MAE). The following figure shows the results from the larger LLMS.\n\nAgain, the gray lines show the range between the performance of the baseline classifier and the random forest. The MAE for the baseline is smaller than in the previous experiment because there are only five categories. The range between the baseline and the random forest is smaller because the prediction task is harder and the data available for training is smaller (2000 respondents).\n\nThe performance of the LLMs is usually in the range between the baseline and the random forest, but occasionally better than the random forest or worse than the baseline. Notably, none of the models are consistently better or worse than the others. But in most cases at least one of the models is comparable to the random forest.\n\nHere are the results with the smaller models. This experiment includes three runs with each of three random seeds.\n\n\nIn some cases the smaller models perform well, with MAE somewhere between the baseline and the random forest, but in several cases the results are worse than the baseline. Mixtral does particularly badly – we have not yet dug into the results to see why. GPT-4o-mini does relatively well at this task, in contrast to the previous experiment. So it’s noteworthy that the relative performance of different LLMs varies from one task to another.\n\nConclusions#\n\nBased on the experiments so far, it looks like synthetic consumers based on LLMs have a lot of potential. On some tasks, their performance is comparable to a machine learning algorithm with a large training set. Some LLMs perform better than others – but not always the same ones – which suggests that an ensemble algorithm that combines responses from multiple LLMs might perform better than any of them alone. We will investigate this in future experiments – so stay tuned for more blogs about this topic!\n\nHowever, on some tasks, LLMs can perform worse than a naive baseline. As a next step, we plan to look more closely at these cases to see if they can be mitigated. Also, in a future post, we will present experiments with a dataset that includes open-ended text responses – a task that makes better use of the unique capabilities of LLMs.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/causal-analysis-with-pymc-answering-what-if-with-the-new-do-operator": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nWhy Causal Analysis?\nWhy Bayesian Causal Analysis?\nWhy Bayesian Causal Analysis in PyMC?\nSetting up\nExample: Estimating impact of Google Ads with TV as a confounder\nFormalizing the underlying Bayesian Causal Network\nDefine the full data generating process in a PyMC model\nSimulating data\nInferring model parameters\n🔥 Bayesian Causal Inference - simulating interventions with the new do operator. 🔥\nSummary\nInterpretation\nCausal Inference in Marketing Analytics```\nFrom Predictions to Decision Intelligence\nTechnical details\nFurther reading\nCausal analysis with PyMC: Answering 'What If?' with the new do operator\n\nAugust 01, 2023\n\nBy Benjamin Vincent, Thomas Wiecki\n\nWhy Causal Analysis?#\n\nCausal analysis is rapidly gaining popularity, but why?\n\nWithout a causal understanding of the world, it's often impossible to identify which actions lead to a desired outcome. For example, if you wanted to cool down on a hot summer day, you would not put your thermometer into an ice bath to make the temperature go down. The obvious reason is that you know that the temperature affects the thermometer, but not the other way around.\n\nIn business, we are constantly taking actions to achieve a certain outcome (e.g. increase sales). So in order not to waste our time heating our proverbial thermometer, we need a solid understanding of the causal relationships underlying our business processes. This is the premise of Decision Intelligence.\n\nMachine learning methods might help us predict what's going to happen with great accuracy, but what's the value of that if it doesn't tell us what to do to achieve a desirable outcome.\n\nWhy Bayesian Causal Analysis?#\n\nCausal analysis is often embedded in a frequentist framework, which comes with some well-documented baggage (see e.g. Scientists rise up against statistical significance).\n\nIt is sometimes claimed that Bayesian statistics does not allow for causal analysis. However, as we will demonstrate in this blog post, this is wrong. Combining these two fields provides many benefits over traditional causal analysis.\n\nWhy Bayesian Causal Analysis in PyMC?#\n\nPyMC is a mature and highly scalable Python package for building Bayesian models using an approachable syntax. Rather than invent new frameworks for performing structural causal analysis, we can super-charge PyMC for Bayesian Causal Analysis with a powerful feature: the new do operator.\n\nNote that we are specifically focusing on Structural Causal Modeling here, rather than e.g. quasi-experimentation which is the focus of packages like CausalPy.\n\nIn sum, there are several advantages compared to more traditional frequentist causal inference approaches:\n\nFrequentist statistics has well-known pratical issues that make it prone to misuse (p-hacking etc).\nPyMC is a well-established tool that allows building and inference of highly sophisticated models.\nIf you already have a PyMC model, now you can do scenario anaysis and ask \"What If\" questions natively.\nSetting up#\nimport arviz as az\nimport daft\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pymc as pm\nimport pytensor as pt\nimport seaborn as sns\nfrom packaging import version\n\n# check we have the required versions\nassert version.parse(pm.__version__) >= version.parse(\"5.8.0\")\n\n# import the new functionality\nfrom pymc import do, observe\n\nSEED = 42\n\nExample: Estimating impact of Google Ads with TV as a confounder#\n\nThe marketing team of our e-commerce company comes to us with a simple request: Estimate the effectiveness of the paid Google search ads in driving sales.\n\nPretty simple, right? We just correlate how much we're spending on search ads with our sales volume. If we wanted to get fancy, we could use a Bayesian Marketing-Mix Model or train a machine-learning model for this.\n\nThere is just one complication: unbeknownst to us, the marketing team is also running TV ad campaigns. In order to maximize the impact of the TV ads, the marketing team is turning Google search ads on when TV ads are active, but when there are no TV campaigns, Google Ads are turned off to save on marketing budget.\n\nThus our situation looks actually like this:\n\nIn causal inference terms, we're dealing with a confounder -- TV influences both: sales and Google ad spend. This is potentially highly problematic, because it's totally possible that it only looks like Google Ads increase sales, but that this effect is completely driven by the TV ads increasing search volume and that there is in fact no causal effect of Google Ads on sales.\n\nThis example illustrates well why we really care about making causal claims about our data. Who cares if Google ad spend is correlated with sales or is very predictive, what we really want to know is what actions we should take to increase sales.\n\nFormalizing the underlying Bayesian Causal Network#\n\nWe can represent the above example with an idiomatic causal directed acyclic graph (DAG) where we have a binary treatment variable  (Google Ads on/off) which may or may not causally influence an outcome  (sales). However, this relationship suffers from confounding by  (TV) which causally influences both treatment and outcome. Further, we turn this into a Bayesian causal DAG by specifying probabilistic causal relationships between the nodes. A prior is placed on  as it has no parent nodes.\n\nAs you can see, we are essentially setting up two regressions: a logistic regression of  on , and a linear regression of  and  on , all with the appropriate intercepts and regression coefficients.\n\nBecause our main relationship of interest here is between the treatment  and the outcome , we can see that this example corresponds to the idiomatic confounded relationship.\n\nOur goal in this example is to establish the strength of the  causal relationship, expressed as parameter . Assuming this is an accurate and complete causal story of our data (a big assumption!) we can quantify the uplift on sales that is caused by paid search.\n\nNote that in this blog post we assume we already know the causal graph. Where that is not the case, we need to look towards methods of causal discovery.\n\nBefore we dive in to the code, let's specify some notation to make life a bit simpler:\n\nWe have random variables , , and \nThese are different from observations, specific values, , , and \nWe have a set of latent parameters, \nDefine the full data generating process in a PyMC model#\n\nThis next step may seem slightly alien to many existing PyMC users. We are going to define an 'empty' model, not conditioned on any data at all. This can be thought of as a 'pure' description of our data generating process, totally divorced from any actual data.\n\nwith pm.Model(coords_mutable={\"i\": [0]}) as model_generative:\n    # priors on Y <- C -> Z\n    beta_y0 = pm.Normal(\"beta_y0\")\n    beta_cy = pm.Normal(\"beta_cy\")\n    beta_cz = pm.Normal(\"beta_cz\")\n    # priors on Z -> Y causal path\n    beta_z0 = pm.Normal(\"beta_z0\")\n    beta_zy = pm.Normal(\"beta_zy\")\n    # observation noise on Y\n    sigma_y = pm.HalfNormal(\"sigma_y\")\n    # core nodes and causal relationships\n    c = pm.Normal(\"c\", mu=0, sigma=1, dims=\"i\")\n    z = pm.Bernoulli(\"z\", p=pm.invlogit(beta_z0 + beta_cz * c), dims=\"i\")\n    y_mu = pm.Deterministic(\"y_mu\", beta_y0 + (beta_zy * z) + (beta_cy * c), dims=\"i\")\n    y = pm.Normal(\"y\", mu=y_mu, sigma=sigma_y, dims=\"i\")\n\npm.model_to_graphviz(model_generative)\n\n\nSimulating data#\n\nHaving defined the full joint distribution, we are first going to use it to generate simulated data with known causal effect sizes, to then test if we can recover them.\n\nIn order to do that, we are going to specify some true parameter values that govern the causal relationships between nodes. Importantly, we will set the true causal influence of  upon  to be equal to 0, that is . This is known as the true Average Treatment Effect (ATE). If you recall our real-world example, this would correspond to Google Ads having no causal effect on sales.\n\nOf course, in real-world situations we will not know what the true ATE is. We only know it here because we are simulating the data and know the ground truth. Our goal is to estimate what the ATE is, and testing how well we can infer known parameters from the data alone (an excercise called Parameter Recovery) is a very useful excercise. If we can't recover the parameters in a simple toy simulated world, then we shouldn't have much faith that we can estimate the true effects in more complex real-world dataset.\n\nWhile there are other ways to do this, here we will use the new do operator to generate a new model with the parameters set to certain values.\n\ntrue_ATE = 0.0\n\ntrue_values = {\n    \"beta_z0\": 0.0,\n    \"beta_y0\": 0.0,\n    \"beta_cz\": 1.5,\n    \"beta_zy\": true_ATE,\n    \"beta_cy\": 1.0,\n    \"sigma_y\": 0.2,\n}\n\nmodel_simulate = do(model_generative, true_values)\n\n\nLet's unpack this a little bit. The do-function takes a pymc.Model object and a dict of parameter values. It then returns a new model where the original random variables (RVs) have been converted to constant nodes taking on the specified values.\n\nBut we're just warming up here, a bit further below, we'll see a much cooler application of the do-operator.\n\nNext, we'll sample from this new model to obtain samples from distibution . Note that we'll use pm.sample_prior_predictive (not pm.sample because we are not doing any inference).\n\nN = 100\n\nwith model_simulate:\n    simulate = pm.sample_prior_predictive(samples=N, random_seed=SEED)\n\nobserved = {\n    \"c\": simulate.prior[\"c\"].values.flatten(),\n    \"y\": simulate.prior[\"y\"].values.flatten(),\n    \"z\": simulate.prior[\"z\"].values.flatten(),\n}\n\ndf = pd.DataFrame(observed).sort_values(\"c\", ascending=False)\n\n\nIf we just did the basic analysis of what our sales () look like based on Google Ads being turned on () or off (), then it looks like there's a big difference:\n\n+ Show Code\nax = sns.displot(data=df, x=\"y\", hue=\"z\", kde=True)\nax.set(xlabel=\"Sales, $y$\");\n\n\nWe could further confirm a significant effect of Google Ads on sales using a t-test or Bayesian significance test but will skip that for now as it's fairly obvious that there is a large difference between the two groups.\n\nThe key point is that in our setup, while it looks like Google Ads has a big (positive) influence on sales, there is actually no underlying causal effect between them. The difference we see is entirely driven by the TV confounder.\n\nInferring model parameters#\n\nLet's next turn to inference. For that, we could just redefine the above model and use the observed kwarg as is common in PyMC.\n\nHowever, we can do something a bit cooler than that and use another new function called observe(). This function takes a model and data, and returns a new model with the data set as observed on our target RV, similar to the do() function.\n\nImportantly, we want to derive this observed model from our original \"empty\" model_generative from above, so that no parameters are fixed in place.\n\nmodel_inference = observe(model_generative, {\"c\": df[\"c\"], \"y\": df[\"y\"], \"z\": df[\"z\"]})\nmodel_inference.set_dim(\"i\", N, coord_values=np.arange(N))\n\n\nNow we can press the Inference Button(TM) and sample from our posterior as if we had defined our model in the classic way.\n\nwith model_inference:\n    idata = pm.sample(random_seed=SEED)\n\n\nWe can inspect our posterior distributions using arviz. By plotting the known parameter values we used to generate the simulated data, we can confirm that the inference step has done a good job of estimating these values.\n\n# !collapse\naz.plot_posterior(\n    idata,\n    var_names=list(true_values.keys()),\n    ref_val=list(true_values.values()),\n    figsize=(12, 6),\n)\nplt.tight_layout();\n\n\nSo far we've simply demonstrated that the generative model we wrote can do a good job of recovering the parameters based on data which was generated directly from that data generating process. This can be a very valuable excercise and represents a new Bayesian workflow for those interested in parameter recovery studies. But let's not get distracted by this and return to our causal focus.\n\n🔥 Bayesian Causal Inference - simulating interventions with the new do operator. 🔥#\n\nNow we are ready for the coolest use case for the do operator -- doing counterfactual reasoning! This is where we're asking \"What if?\": What if we stopped Google Ads? What if we increased spending on Google Ads? What would we expect our data to look like in these scenarios?\n\nCritically, these hypothetical interventions remove any influence of TV on Google Ads, because the assumption is that we turn Google Ads on and off irrespective of what TV is doing. This is the logic behind the do operator.\n\nOnce we have these two hypotheticals, we estimate the thing we're really interested in: How strong is the causal influence of Google Ads on sales, independent of the TV confounder?\n\nFor those familiar with causal inference, you'll know this as the Average Treatment Effect:\n\nTo achieve this in PyMC, we can use the do() function again to generate the counterfactual scenarios as explained above. However, before diving in with something like the following:\n\nmodel_z0 = do(model_inference, {\"z\": np.zeros(N, dtype=\"int32\")})\n\n\nFirst, we have to replace the TV RV c with its observed data df[\"C\"] so that it doesn't get resampled later.\n\n# Replace c with its observed values\nmodel_counterfactual = do(model_inference, {\"c\": df[\"c\"]})\n\n\nNow we are able to use the do() operator to set all the values of  to either 0 or 1 to calculate  and , respectively.\n\n# Generate models with Z=0 and Z=1\nmodel_z0 = do(model_counterfactual, {\"z\": np.zeros(N, dtype=\"int32\")}, prune_vars=True)\nmodel_z1 = do(model_counterfactual, {\"z\": np.ones(N, dtype=\"int32\")}, prune_vars=True)\n\n\nThis gives us our two What-If models. As we are interested in what data we would expect to observe under these hypothetical scenarios, we next need to simulate new data for both models.\n\nThis is done using our trusted pm.sample_posterior_predictive() function which generates new data for given a model and a posterior inference trace (i.e. the idata from above when we fit the model to data containing the posteriors of our estimated regression coefficients).\n\n# Sample new sales data assuming Google Ads off: P(Y | c, do(z=0))\nidata_z0 = pm.sample_posterior_predictive(\n    idata,\n    model=model_z0,\n    predictions=True,\n    var_names=[\"y_mu\"],\n    random_seed=SEED,\n)\n# Sample new sales data assuming Google Ads on: P(Y | c, do(z=1))\nidata_z1 = pm.sample_posterior_predictive(\n    idata,\n    model=model_z1,\n    predictions=True,\n    var_names=[\"y_mu\"],\n    random_seed=SEED,\n)\n\n\nGiven this hypothetical sales data from the two simulated interventions, we can compute the difference in sales between Google Ads on and off to give us the estimated Average Treatment Effect:\n\n# calculate estimated ATE\nATE_est = idata_z1.predictions - idata_z0.predictions\nprint(f\"Estimated ATE = {ATE_est.y_mu.mean().values:.2f}\")\n\nEstimated ATE = 0.06\n\n\nWe get a small but positive ATE. If we didn't know any better, we might be tempted to say that there is indeed a small causal effect of Google Ads on sales.\n\nFortunately, as the good Bayesians that we are, we know to be weary of point-estimates and always look at the full distribution of outcomes.\n\nResults of our Bayesian causal inference. The left panel shows our individual level estimates of the outcome $y$ under the counterfactual situations of Google Ads off ($\\operatorname{do}(z=0)$) or on ($\\operatorname{do}(z=1)$). Each day (y-axis) has a different outcome due to the influence of the confounding variable, $C$. The right panel shows our posterior estimate of the Average Treatment Effect of $Z \\rightarrow Y$. While the ATE distribution is shifted to the right, we see that our uncertainty is nonetheless very high and that 0 is inside the Bayesian credible interval (the black line at the bottom of the right plot).\nSo is the causal impact of Google Ads on sales large or small? There are many ways we could go about answering this question but here we'll calculate the size of the causal effect of Google Ads on sales ($Z \\rightarrow Y$) as a percentage of the causal effect of TV advertising on sales ($C \\rightarrow Y$).\npercent = (idata.posterior[\"beta_zy\"] / idata.posterior[\"beta_cy\"]) * 100\nprint(\n    f\"Causal effect of Google Ads on sales is {percent.mean():.1f}% [{az.hdi(percent).x.data[0]:.1f}%,{az.hdi(percent).x.data[1]:.1f}%] of the effect of TV on sales\"\n)\n\nCausal effect of Google Ads on sales is 6.2% [-4.1%, 16.5%] of the effect of TV on sales\n\n\nDespite a positive point-estimate of the ATE, we maybe don't conclude that the effect of Google Ads on sales is that high, if there is one at all.\n\nAs this example has hopefully demonstrated, Bayesian statistics is the perfect framework for doing causal analysis, and the do operator is an enormously helpful tool.\n\nSummary#\nInterpretation#\n\nIn this blog post we have used an example from marketing analytics to show how easy it is to get fooled by correlation. While it clearly looked like Google Ads were increasing sales, this difference was entirely driven by the TV confounder. This insight became quite obvious when we simulated data under two hypothetical interventions: manually turning Google Ads on and off (which removed the influence of TV on Google Ads).\n\nIf we hadn't taken a causal approach, we might have well told the marketing team that increasing Google Ad spend is a good way to boost sales.\n\nIn addition, we have shown how even with a causal approach, it is easy to get fooled by randomness. As we're always estimating causal effects from data, point estimates give us no sense of uncertainty. By embedding our causal analysis in a Bayesian framework we are able to use probabilities to quantify how certain we are of there being a causal effect.\n\nCausal models can be easily expressed in PyMC. We simply implement the Data Generating Process and hit the Inference Button. Combined with the newly added do() operator, we are able to simulate interventions that take causality as well as uncertainty into account.\n\nCausal Inference in Marketing Analytics```#\n\nConfounders are a big concern, especially in marketing analytics. While the example used here is realistic, it's definitely also quite simplistic. Usually we would consider not just two marketing channels, but many. In that case, the story becomes more complex, but we still can leverage our understanding of underlying causal structure: marketing channels higher up the funnel are more for brand-building and exhibit longer-term effects, while lower-funnel channels have more direct short-term effects.\n\nHere is a diagram that illustrates this idea:\nAs you can see, higher-funnel brand marketing spend (e.g. TV) is building awareness which drives interest which ultimately creates sales. Lower-funnel performance marketing spend (e.g. paid search ads) has a more direct influence on sales. Notably, we have all kinds of other factors and confounders as well here we can include as well. For example, we know that price plays a big role on sales, so this allows us to bridge MMMs with price elasticity modeling.\n\nOther real-world effects can be included as well, like channel saturation and delay (adstock) effects. Including these effects into our model turns it into the well-known Media Mix Model (MMM) which we've done a lot of work on at PyMC Labs, such as adding time-varying parameters. This work culminated into releasing an open-source package to put Bayesian MMMs and customer lifetime value models in PyMC at your fingertips: https://www.pymc-marketing.io.\n\nIf this of interest to you, we have built a causal full-funnel Bayesian MMM that includes price-elasticity and are excited to show it around. Get in touch, we are looking forward to hearing from you.\n\nFrom Predictions to Decision Intelligence#\n\nIf we want to make better decisions, we need a solid understanding of how our actions affect outcomes. This is the domain of causal analysis.\n\nIf we don't want to be fooled by the randomness in our data, we need to know how confident we are in our results. This is the domain of Bayesian statistics.\n\nCombining these two gives us a very powerful framework for doing data science that provides actual business value. As we leave the domain of pure associations or predictions, we stop merely providing insight into what has happened or will happen, but also what to do to maximize chances of a favorable outcome. We enter the domain of Decision Intelligence.\n\nWith the new functionality presented here, PyMC emerges as a powerful tool to aid in this endeavour.\n\nTechnical details#\n\nIf you are hungry for even more detail, we've got you covered!\n\nYou can find the notebook version of this blogpost here.\nAnd you can find an in-depth yet accessible example of using the do operator to calculate interventional distributions here.\nFurther reading#\nGlymour, M., Pearl, J., & Jewell, N. P. (2016). Causal inference in statistics: A primer. John Wiley & Sons.\nPearl, J. & Mackenzie, D. (2018). The book of why: the new science of cause and effect. Basic books.\nPearl, J. (2009). Causality. Cambridge University Press\nTalebi, S. (2022) Causal Effects via the Do-operator.\nCausal inference in statistics: A primer.\nBlog post on Out of model predictions with PyMC\nFor the related approach of quasi-experimentation, check out CausalPy, a package we put together for causal inference in situations, built on top of PyMC (also see our video from PyData Global 2022 about this topic).\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/2023-10-27-Latent-calendar-Will": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nIntroduction\nAbout Speaker\nTimestamps\nImportant Links\nLatent Calendar: Modeling Weekly Behavior with Latent Components\n\nOctober 27, 2023\n\nBy William Dean\n\nIntroduction#\n\nIn this webinar, we will explore the use of a traditional Natural Language Processing technique for modeling weekly calendar data. We will delve into how Latent Dirichlet Allocation can be applied to discretized calendar events, allowing us to tap into the model's probabilistic origins and its connection to Bayesian principles, offering a wide array of potential applications and insights.\n\nAbout Speaker#\n\nWill Dean is a Statistician and Data Scientist with experience in geospatial and user analytics. He is passionate about Bayesian methods and using data visualization to tell a story. He is interested in software design and how it can make data problems easier and more enjoyable to solve.\n\nLinkedIn: https://www.linkedin.com/in/williambdean/\nTimestamps#\n\n00:00 Webinar begins\n\n04:00 Presentation begins\n\n04:29 Will's background\n\n05:11 About the talk\n\n05:57 Case study dataset\n\n06:57 Data in mind\n\n07:33 Timestamps provide more info\n\n07:52 Calendar Visualization\n\n09:50 Data Generation Process (First Attempt)\n\n11:26 Data Generation Process (Second Attempt)\n\n12:32 Discrete Approximation\n\n13:19 Data Generation Process (Third Attempt)\n\n13:57 How to people get around?\n\n13:58 Data Generation Process (Fourth Attempt)\n\n17:41 Latent Dirichlet Allocation\n\n18:26 Use what is available\n\n19:51 Define \"Vocab\"from timestamps\n\n20:23 Aggregate to \"documents\"\n\n21:42 Learn from \"topics\"\n\n23:43 \"Topic\" insights\n\n24:12 Predict and Transform\n\n26:01 Low data support\n\n27:24 Prior Impact\n\n29:27 Next steps for latent-calendar project\n\n30:44 Where it fits into marketing?\n\n33:57 (Q/A) How was the project perceived by stakeholders?\n\n38:10 (Q/A) Is this being used primarily as an insight generating tool?\n\n39:48 (Q/A) Can you explain the connection to CLV modelling ...\n\n44:14 (Q/A) In general more about properties and exploiting them when modelling ...\n\n53:34 (Q/A) Whenever working with Timestamp data is it cyclical ....?\n\n55:40 Webinar ends\n\nImportant Links#\n\nLatent-Calendar Docs\n\nPyMC Labs\n\nIntuitive bayes course\n\nRepository\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/AI-based-Customer-Research": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nThe Problem with Asking AI for Numbers\nThe Breakthrough: Semantic Similarity Rating (SSR)\nWhat This Means for Business\nThe Technical Foundation That Makes It Work\nAbout the Research Team\nWhat's Next\nRead the Full Preprint\nWant to learn more about synthetic consumers?\nAI-based Customer Research: Faster & Cheaper Surveys with Synthetic Consumers\n\nOctober 09, 2025\n\nBy Benjamin F. Maier\n\nConsumer research costs corporations billions annually, yet it still struggles with panel bias, limited scale, and noisy results. What if synthetic consumers powered by large language models (LLMs) could replicate human survey responses with high accuracy while providing richer qualitative feedback?\n\nThat's exactly what PyMC Labs researchers just proved in a new preprint that's changing how we think about AI-powered market research.\n\nThe Problem with Asking AI for Numbers#\n\nWhen companies first tried using LLMs as synthetic consumers, they hit a wall. Ask LLMs directly for a 1-5 rating about purchase intent given a product concept, and you get unrealistic distributions — too many \"3s,\" hardly any extreme responses, and patterns that don't match real human behavior. The conventional wisdom? LLMs just aren't reliable survey takers.\n\nOur PyMC Labs team showed that's wrong. The problem wasn't the models, it was how we were asking them questions.\n\nThe Breakthrough: Semantic Similarity Rating (SSR)#\n\nInstead of forcing LLMs to pick a number, the research team developed a two-step approach:\n\nStep One: Let AI respond naturally in text (like humans actually think about purchase intent)\nStep Two: Map that response to a rating distribution on the 1-5 scale using semantic similarity, comparing the AI's statement to reference anchors for each point\n\nThe results? Using 57 real consumer surveys from a leading consumer products company (9,300 human responses), the SSR method achieved:\n\n90% correlation attainment with product ranking in human surveys\nMore than 85% distributional similarity to actual survey results\nRealistic response patterns that mirror how humans actually rate products\n\nThis isn't just incrementally better, it's the first approach that produces synthetic consumer data reliable enough to guide real product development decisions.\n\nWhat This Means for Business#\n\nFor Product Development Teams: You can now screen dozens of concepts with synthetic panels before committing budget to human surveys. Test ideas faster, iterate more, and reserve expensive panel studies for only the most promising candidates.\n\nFor Consumer Insights Leaders: Synthetic consumers don't just replicate ratings, they provide detailed explanations for their scores.\n\nFor Research Innovation: The method works without any training data or fine-tuning. It's plug-and-play, preserving compatibility with traditional survey metrics while unlocking qualitative depth that was previously impossible at scale.\n\nPerhaps most importantly: synthetic consumers showed less positivity bias than human panels, producing wider, more discriminative signals between good and mediocre concepts.\n\nThe Technical Foundation That Makes It Work#\n\nThe paper demonstrates something fundamental about LLMs: they've absorbed vast amounts of human consumer discourse from their training data. When properly prompted with demographic personas and asked to respond naturally, they can simulate realistic purchase intent patterns - not because they're copying training examples, but because they've learned the underlying patterns of how different people evaluate products. The team tested this rigorously:\n\nCompared LLM architectures (GPT-4o, Gemini 2.0 Flash)\nValidated demographic conditioning (age, income, product category all influenced responses realistically)\nBenchmarked against supervised ML approaches (which couldn't match LLM performance even with training data)\n\nThis isn't prompt engineering wizardry, it's a fundamental shift in how we should think about eliciting structured information from language models.\n\nAbout the Research Team#\n\nThis work comes from PyMC Labs, led by corresponding authors Benjamin F. Maier and Kli Pappas (Colgate-Palmolive), alongside the broader PyMC Labs and Colgate-Palmolive research team including Ulf Aslak, Luca Fiaschi, Nina Rismal, Kemble Fletcher, Christian Luhmann, Robbie Dow and Thomas Wiecki.\n\nWhat's Next#\n\nThis research opens doors well beyond purchase intent:\n\nExtending SSR to other survey items (satisfaction, trust, relevance)\nOptimizing reference statement sets for different domains\nCombining synthetic and human panels in hybrid research designs\nExploring multi-stage pipelines where one LLM generates responses and another calibrates them\n\nThe fundamental insight — that textual elicitation plus semantic mapping outperforms direct numerical elicitation — likely applies far beyond consumer research.\n\nRead the Full Preprint#\n\nThis blog post covers the highlights, but the complete paper includes:\n\nDetailed methodology and mathematical framework\nFull experimental results across all 57 surveys\nComparative analysis of different LLM architectures\nReference statement design principles\nDemographic conditioning experiments\nOpen-source Python implementation of the SSR methodology\n\nDownload the full preprint here to see the methods, experiments, and detailed results.\n\nWant to learn more about synthetic consumers?#\n\nCheck out PyMC Labs’ previous work:\n\nCan LLMs play The Price is Right?Can LLMs play The Price is Right?\nCan Synthetic Consumers Answer Open-Ended Questions?\nHow Realistic Are Synthetic Consumers?\n\nDiscover how PyMC Labs is helping organisations harness the power of synthetic consumers to transform research and decision-making. See what we’re building in our Innovation Lab — and connect with us to learn more.\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/2023-09-15-Hierarchical-models-Chris-Fonnesbeck": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nIntroduction\nAbout Speaker\nTimestamps\nImportant Links\nDeveloping Hierarchical Models for Sports Analytics\n\nSeptember 26, 2023\n\nBy Chris Fonnesbeck\n\nIntroduction#\n\nDecision-making in sports has become increasingly data-driven with GPS, cameras, and other sensors providing streams of information at high spatial and temporal resolution. While machine learning is a popular approach for turning these data streams into actionable information, Bayesian statistical methods offer a robust alternative. They allow for the combining of multiple data sources, a natural means for imputing missing data, as well as full accounting for various system uncertainties. In particular, hierarchical models provide a means for integrating information at multiple scales and adjusting for biases associated with small sample sizes. I will demonstrate a Bayesian workflow for model development using PyMC version 5, from data preparation through to the summarization of estimates and predictions, using baseball data.\n\nAbout Speaker#\n\nChris is the Principal Quantitative Analyst in Baseball Research & Development for the Philadelphia Phillies. He is interested in computational statistics, machine learning, Bayesian methods, and applied decision analysis. He hails from Vancouver, Canada and received his Ph.D. from the University of Georgia.​\n\nLinkedIn: https://www.linkedin.com/in/christopher-fonnesbeck-374a492a/\nTimestamps#\n\n00:00:00 Welcome\n\n00:07:24 Presentation begins\n\n00:09:11 Data Science in Baseball\n\n00:09:36 Sabermetrics\n\n00:10:33 Canoncial Baseball statistcs\n\n00:12:02 Advanced metrics\n\n00:13:03 Ball Tracking technology\n\n00:13:44 Trackman\n\n00:14:08 Hawkeye\n\n00:17:36 Bayesian inference\n\n00:18:58 PyMC\n\n00:19:59 Home run rate estimation\n\n00:23:37 Prior predictive checks\n\n00:25:00 Nuts about MCMC\n\n00:28:14 Posterior predictive sampling\n\n00:28:48 Informative priors\n\n00:31:18 Unpooled Model\n\n00:31:40 Hierarchical Model\n\n00:32:16 Partial pooling\n\n00:32:40 HyperPriors\n\n00:32:56 Partial Pooling Model\n\n00:34:06 Group Covariate Model\n\n00:36:12 Park Effects\n\n00:38:24 Model Comparison with Expected Log Predictive Density\n\n00:39:08 Leave One Out Cross Validation\n\n00:40:18 Individual covariates\n\n00:42:03 Variable interactions\n\n00:42:27 Gaussian processes\n\n00:43:55 Accelerated Sampling\n\n00:45:13 Out-Of-Sample Prediction\n\n00:47:05 Prediction Model\n\n00:48:38 Workflow steps\n\n00:50:51 Q/A Could you explain the kernel function ...?\n\n00:52:30 Q/A What is the advantage of ...?\n\n00:54:23 Q/A How would you handle categorical variables in the individual ...?\n\n00:56:37 Q/A How Bayesian analytics is bringing value to ...?\n\n01:00:26 Q/A Can you give insights into how you interact ...?\n\n01:01:40 Q/A Do you have recommended ...?\n\n01:03:32 Q/A Any advice if I'm new and want to improve?\n\n01:04:28 Q/A Does it happen that a selected model is not good at ...?\n\n01:06:13 Q/A Could you comment on the usage of Bayesian decision-making...?\n\n01:08:10 Webinar Ends\n\nImportant Links#\n\nSlides\n\nModeling spatial data with Gaussian processes in PyMC\n\nUsing Bayesian decision making\n\nPyMC Labs\n\nIntuitive bayes course\n\nRepository\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved.",
  "https://www.pymc-labs.com/blog-posts/jax-functions-in-pymc-3-quick-examples": "Home\nAbout\nBlog\nWorkshops\nResources\nLLM Price Is Right Benchmark\nContact us\nTable of Contents\nWrapping a pure JAX function\nWrapping a Diffrax ODE Solver\nWrapping a Flax neural network\nSummary\nHow to use JAX ODEs and Neural Networks in PyMC\n\nJanuary 03, 2023\n\nBy Ricardo Vieira And Adrian Seyboldt\n\nPyMC strength comes from its expressiveness. If you have a data-generating process and want to infer parameters of interest, all you need to do is write it down, choose some priors and let it sample.\n\nSometimes this is easier said than done, especially the \"write it down\" part. With Python's rich ecosystem, it's often the case that you already have a generative function, but it's written in another framework, and you would like to use it in PyMC. Thanks to the highly composable nature of the PyMC backend, this is simple. Even simpler if that framework can also provide you gradients for free!\n\nIn this blog post, we show how you can reuse code from another popular auto-diff framework, JAX, directly in PyMC.\n\nWe will start with a dummy example by simply wrapping a pure function that already exists under pymc.math, and then show two real examples: reusing an ODE Solver from the Diffrax library and a CNN from the Flax library.\n\nThis blog post won't explain in detail why we do things the way they are shown, but will only show you how to do it. If you want to have a better understanding, you should check the PyMC example How to wrap a JAX function for use in PyMC and the relevant PyTensor documentation of Op.\n\nWithout further ado, let's import some stuff.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport pytensor\nimport pytensor.tensor as pt\nfrom pytensor.graph import Apply, Op\nfrom pytensor.link.jax.dispatch import jax_funcify\n\nimport jax\nimport jax.numpy as jnp\n\nimport pymc as pm\nimport pymc.sampling.jax\n\nWrapping a pure JAX function#\n\nIn this first example, we will wrap the jax.numpy.exp function so you can use it in PyMC models. This is purely demonstrative, as you could use pymc.math.exp.\n\nWe first create a function that encapsulates the operation (or series of operations) that we care about. We also save the jitted function into a variable.\n\ndef custom_op_jax(x):\n    return jnp.exp(x)\n\njitted_custom_op_jax = jax.jit(custom_op_jax)\n\n\nJAX's jit function accepts a function and returns a function, meaning that we can call on jitted_custom_op_jax:\n\njitted_custom_op_jax(np.arange(3))\n\n\nWe then create the function that computes the vector-jacobian product (vjp) needed for PyTensor gradients. JAX vjp takes as inputs a computational graph, expressed as a function, and its inputs. It returns the evaluated graph, which we don't need, and a partial function that computes the vjp, given the output gradients, which is what we need for PyTensor.\n\ndef vjp_custom_op_jax(x, gz):\n    _, vjp_fn = jax.vjp(custom_op_jax, x)\n    return vjp_fn(gz)[0]\n\njitted_vjp_custom_op_jax = jax.jit(vjp_custom_op_jax)\n\n\nNow for the meaty part! We create two PyTensor Ops, one for the operation we care about and another for the vjp of that operation. This is how we can glue external code into PyMC's backend. (Note here: It's a bit verbose, but nothing too complicated.)\n\nHere's what's happening below. We subclass from the Op class and implement 3 methods: make_node, perform and grad. For the vjp we need to implement only the first two.\n\n# The CustomOp needs `make_node`, `perform` and `grad`.\nclass CustomOp(Op):\n    def make_node(self, x):\n        # Create a PyTensor node specifying the number and type of inputs and outputs\n\n        # We convert the input into a PyTensor tensor variable\n        inputs = [pt.as_tensor_variable(x)]\n        # Output has the same type and shape as `x`\n        outputs = [inputs[0].type()]\n        return Apply(self, inputs, outputs)\n\n    def perform(self, node, inputs, outputs):\n        # Evaluate the Op result for a specific numerical input\n\n        # The inputs are always wrapped in a list\n        (x,) = inputs\n        result = jitted_custom_op_jax(x)\n        # The results should be assigned inplace to the nested list\n        # of outputs provided by PyTensor. If you have multiple\n        # outputs and results, you should assign each at outputs[i][0]\n        outputs[0][0] = np.asarray(result, dtype=\"float64\")\n\n    def grad(self, inputs, output_gradients):\n        # Create a PyTensor expression of the gradient\n        (x,) = inputs\n        (gz,) = output_gradients\n        # We reference the VJP Op created below, which encapsulates\n        # the gradient operation\n        return [vjp_custom_op(x, gz)]\n\n\nclass VJPCustomOp(Op):\n    def make_node(self, x, gz):\n        # Make sure the two inputs are tensor variables\n        inputs = [pt.as_tensor_variable(x), pt.as_tensor_variable(gz)]\n        # Output has the shape type and shape as the first input\n        outputs = [inputs[0].type()]\n        return Apply(self, inputs, outputs)\n\n    def perform(self, node, inputs, outputs):\n        (x, gz) = inputs\n        result = jitted_vjp_custom_op_jax(x, gz)\n        outputs[0][0] = np.asarray(result, dtype=\"float64\")\n\n# Instantiate the Ops\ncustom_op = CustomOp()\nvjp_custom_op = VJPCustomOp()\n\n\nHow do we know that we've implemented the Ops correctly? To do that, we can use the pytensor verify_grad utility:\n\npytensor.gradient.verify_grad(custom_op, (np.arange(5, dtype=\"float64\"),), rng=np.random.default_rng())\n\n\nIt didn't raise an Error, so we're clear! Now we can use our wrapped Op directly with PyMC models:\n\nwith pm.Model() as model:\n    x = pm.Normal(\"x\", shape=(3,))\n    y = pm.Deterministic(\"y\", custom_op(x))  # HERE IS WHERE WE USE THE CUSTOM OP!\n    z = pm.Normal(\"z\", y, observed=[1, 2, 0])\n\n\nPyMC provides model_to_graphviz to visualize the model graph:\n\npm.model_to_graphviz(model)\n\n\nPart of verifying that the Op's API works with PyMC is in evaluating the model's logp and dlogp. First, the logp function:\n\n# Compute the logp at the initial point\nip = model.initial_point()\nlogp_fn = model.compile_fn(model.logp(sum=False))\nlogp_fn(ip)\n\n[array([-0.91893853, -0.91893853, -0.91893853]),\n array([-0.91893853, -1.41893853, -1.41893853])]\n\n\nAnd now the dlogp function:\n\ndlogp_fn = model.compile_fn(model.dlogp())\ndlogp_fn(ip)\n\narray([ 0.,  1., -1.])\n\n\nThe numerical values of the evaluated logp and dlogp are correct in both cases.\n\nIf we want to use PyTensor's JAX backend, we have to, somewhat paradoxically, tell PyTensor how to convert our Ops to JAX code. PyTensor does not know it was JAX code to begin with! Fortunately, this is pretty simple by simply returning the original functions.\n\nNote that we don't return the jitted functions, because we want PyTensor to use JAX to jit the whole JAX graph together.\n\n@jax_funcify.register(CustomOp)\ndef custom_op_jax_funcify(op, **kwargs):\n    return custom_op_jax\n\n@jax_funcify.register(VJPCustomOp)\ndef vjp_custom_op_jax_funcify(op, **kwargs):\n    return vjp_custom_op_jax\n\n\nNow we can compile to the JAX backend and get the same results! First with the logp:\n\n# Now using the JAX backend\nip = model.initial_point()\nlogp_fn = model.compile_fn(model.logp(sum=False), mode=\"JAX\")\nlogp_fn(ip)\n\n[DeviceArray([-0.91893853, -0.91893853, -0.91893853], dtype=float64),\n DeviceArray([-0.91893853, -1.41893853, -1.41893853], dtype=float64)]\n\n\nAnd now with the dlogp:\n\ndlogp_fn = model.compile_fn(model.dlogp(), mode=\"JAX\")\ndlogp_fn(ip)\n\nDeviceArray([[ 0.,  1., -1.]], dtype=float64)\n\nWrapping a Diffrax ODE Solver#\n\nLet's move on to a more complicated situation. We will wrap an ODE solver from the Diffrax library in this second example. It will be a straightforward example with a single variable parameter: The initial point y0. First, we import diffrax:\n\nimport diffrax\n\n\nThen we set up a simple ODE.\n\nvector_field = lambda t, y, args: -y\nterm = diffrax.ODETerm(vector_field)\nsolver = diffrax.Dopri5()\nsaveat = diffrax.SaveAt(ts=[0., 1., 2., 3.])\nstepsize_controller = diffrax.PIDController(rtol=1e-5, atol=1e-5)\n\nsol = diffrax.diffeqsolve(term, solver, t0=0, t1=3, dt0=0.1, y0=1, saveat=saveat,\n                  stepsize_controller=stepsize_controller)\n\nprint(sol.ts)  # DeviceArray([0.   , 1.   , 2.   , 3.    ])\nprint(sol.ys)  # DeviceArray([1.   , 0.368, 0.135, 0.0498])\n\nDeviceArray([0.   , 1.   , 2.   , 3.    ])\nDeviceArray([1.   , 0.368, 0.135, 0.0498])\n\n\nFor those who are not familiar with ODEs, the vector_field is the derivative of the function y with respect to t, and the term is the ODE itself. The solver is the method used to solve the ODE; the saveat is the collection of times at which we want to save the solution; and the stepsize_controller is used to control the step size of the solver. Finally, sol is the solution to the ODE, evaluated at the saveat points.\n\nFrom this point onward, the rest of the code should look very similar to what we did above.\n\nFirstly, we need a JAX function that we will wrap. Our function will return the solutions for ys, given a starting point y0. The other parameters will be constant for this example, but they could also be variables in a more complex Op.\n\ndef sol_op_jax(y0):\n    sol = diffrax.diffeqsolve(\n        term,\n        solver,\n        t0=0,\n        t1=3,\n        dt0=0.1,\n        y0=y0,\n        saveat=saveat,\n        stepsize_controller=stepsize_controller\n    )\n\n    return sol.ys\n\njitted_sol_op_jax = jax.jit(sol_op_jax)\n\n\nThen, we define the vjp function.\n\ndef vjp_sol_op_jax(y0, gz):\n    _, vjp_fn = jax.vjp(sol_op_jax, y0)\n    return vjp_fn(gz)[0]\n\njitted_vjp_sol_op_jax = jax.jit(vjp_sol_op_jax)\n\n\nAfter that, we define the Op and VJPOp classes for the ODE problem:\n\nclass SolOp(Op):\n    def make_node(self, y0):\n        inputs = [pt.as_tensor_variable(y0)]\n        # Assume the output to always be a float64 vector\n        outputs = [pt.vector(dtype=\"float64\")]\n        return Apply(self, inputs, outputs)\n\n    def perform(self, node, inputs, outputs):\n        (y0,) = inputs\n        result = jitted_sol_op_jax(y0)\n        outputs[0][0] = np.asarray(result, dtype=\"float64\")\n\n    def grad(self, inputs, output_gradients):\n        (y0,) = inputs\n        (gz,) = output_gradients\n        return [vjp_sol_op(y0, gz)]\n\n\nclass VJPSolOp(Op):\n    def make_node(self, y0, gz):\n        inputs = [pt.as_tensor_variable(y0), pt.as_tensor_variable(gz)]\n        outputs = [inputs[0].type()]\n        return Apply(self, inputs, outputs)\n\n    def perform(self, node, inputs, outputs):\n        (y0, gz) = inputs\n        result = jitted_vjp_sol_op_jax(y0, gz)\n        outputs[0][0] = np.asarray(result, dtype=\"float64\")\n\nsol_op = SolOp()\nvjp_sol_op = VJPSolOp()\n\npytensor.gradient.verify_grad(sol_op, (np.array(3.0),), rng=np.random.default_rng())\n\n\nAnd with no errors, we go on to register the JAX-ified versions of the Op and VJPOp:\n\n@jax_funcify.register(SolOp)\ndef sol_op_jax_funcify(op, **kwargs):\n    return sol_op_jax\n\n@jax_funcify.register(VJPSolOp)\ndef vjp_sol_op_jax_funcify(op, **kwargs):\n    return vjp_sol_op_jax\n\n\nFinally, we can use the Op in a model, this time to infer what the initial value of the ODE was from observed data:\n\nwith pm.Model() as model:\n    y0 = pm.Normal(\"y0\")\n    ys = sol_op(y0)\n    noise = pm.HalfNormal(\"noise\")\n    llike = pm.Normal(\"llike\", ys, noise, observed=[1, 0.367, 0.135, 0.049])\n\n\nAs always, we can inspect the model's structure to make sure it is correct:\n\npm.model_to_graphviz(model)\n\n\nAnd finally, we can verify that the model's logp and dlogp functions execute. Firstly, without JAX mode:\n\nip = model.initial_point()\nlogp_fn = model.compile_fn(model.logp(sum=False))\nlogp_fn(ip)\n\n[array(-0.91893853),\n array(-0.72579135),\n array([-1.41893853, -0.98628303, -0.92805103, -0.92013903])]\n\n\nAnd then with JAX mode:\n\nlogp_fn = model.compile_fn(model.logp(sum=False), mode=\"JAX\")\nlogp_fn(ip)\n\n[DeviceArray(-0.91893853, dtype=float64),\n DeviceArray(-0.72579135, dtype=float64),\n DeviceArray([-1.41893853, -0.98628303, -0.92805103, -0.92013903], dtype=float64)]\n\n\nAnd then the dlogp functions in both non-JAX and JAX mode:\n\ndlogp_fn = model.compile_fn(model.dlogp())\ndlogp_fn(ip)\n\narray([ 1.15494948, -2.844685  ])\n\ndlogp_fn = model.compile_fn(model.dlogp(), mode=\"JAX\")\ndlogp_fn(ip)\n\nDeviceArray([ 1.15494948, -2.844685  ], dtype=float64)\n\nWrapping a Flax neural network#\n\nOur final example will be encapsulating a Neural Network built with the Flax library. In this example, we will skip the gradient implementation. As discussed below, you don't need to implement it if you defer the gradient transformation to JAX, as PyMC does when using sampling.jax.\n\nIn this problem setup, we will be training a CNN to predict digit identity in a given MNIST dataset image. We will make use of tensorflow_datasets to get access to the MNIST dataset:\n\nimport tensorflow_datasets as tfds\n\nfrom flax import linen as nn\nfrom flax.core import freeze\n\ndef get_datasets():\n    \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n\n    ds_builder = tfds.builder('mnist')\n    ds_builder.download_and_prepare()\n    train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n    test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n    train_ds['image'] = np.float32(train_ds['image']) / 255.\n    test_ds['image'] = np.float32(test_ds['image']) / 255.\n    return train_ds, test_ds\n\ntrain, _ = get_datasets()\ntrain[\"image\"] = train[\"image\"][:1_000]\ntrain[\"label\"] = train[\"label\"][:1_000]\n\n\nWe can inspect the dataset to figure out its dimensions:\n\ntrain[\"image\"].shape\n\n(1000, 28, 28, 1)  # (batch_size, height, width, channels)\n\n\nHere, we selected 1,000 images, each of which is 28x28 pixels, with 1 channel\n\nLet's see what one of those images looks like:\n\nplt.imshow(train[\"image\"][0])\nplt.title(train[\"label\"][0]);\n\n\nNow, we will implement a simple Convolution Neural Network (CNN) using the very user-friendly Flax library. (It has an API that is very, very close in spirit to PyTorch.)\n\nclass CNN(nn.Module):\n    \"\"\"A simple CNN model.\"\"\"\n\n    @nn.compact\n    def __call__(self, x):\n        # Convolution layer\n        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n\n        # Convolution layer\n        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n\n        # Dense layer\n        x = x.reshape((x.shape[0], -1))  # flatten\n        x = nn.Dense(features=256)(x)\n        x = nn.relu(x)\n\n        # Output layer\n        x = nn.Dense(features=10)(x)\n\n        return x\n\n\nThe exact structure of the model is unimportant here; what is important, though, is that the model is a callable.\n\nLet's initialize the CNN and iterate over the layers to get an idea of the number of parameters\n\ncnn = CNN()\n\nrng = jax.random.PRNGKey(0)\nrng, init_rng = jax.random.split(rng)\nparams = cnn.init(rng, jnp.ones([1, 28, 28, 1]))\n\nn_params = 0\nfor layer in params[\"params\"].values():\n    n_params += layer[\"kernel\"].size + layer.get(\"bias\", np.array(())).size\nn_params\n\n824458\n\n\nThis model has a lot of parameters, many more than most of the classical statistical estimation models will have.\n\nWe can evaluate the forward pass of the network by calling cnn.apply. This is the function we want to wrap for use in PyMC.\n\ncnn.apply(params, train[\"image\"][0:1])\n\nDeviceArray([[ 0.11332617, -0.06580747, -0.06869425, -0.02406035,\n              -0.05488511, -0.00442111,  0.05316056,  0.1178513 ,\n               0.10901824,  0.09090584]], dtype=float32)\n\n\nWe want to pass the weights of each kernel as vanilla arrays, but FLAX requires them to be in a tree structure for evaluation. This requires using some utilities, but it's otherwise straightforward. Note that this is specific to Flax, of course.\n\ntreedef = jax.tree_util.tree_structure(params)\n\ndef cnn_op_jax(flat_params, images):\n    unflat_params = jax.tree_util.tree_unflatten(treedef, flat_params)\n    return cnn.apply(unflat_params, images)\n\njitted_cnn_op_jax = jax.jit(cnn_op_jax)\n\n\nIf you are feeling a bit confused because of the presence of \"unflattened\" and \"flattened\" parameters, don't worry: it's just a technicality we need to deal with now. What's worth noting here is that the CNN's forward pass is wrapped in a JAX function that will be wrapped in a PyTensor Op, just as we had done before.\n\nNow, let's create the CNN Op. Note that we don't implement the gradient method!\n\nclass CNNOp(Op):\n    def make_node(self, *inputs):\n        # Convert our inputs to symbolic variables\n        inputs = [pt.as_tensor_variable(inp) for inp in inputs]\n        # Assume the output to always be a float64 matrix\n        outputs = [pt.matrix(dtype=\"float64\")]\n        return Apply(self, inputs, outputs)\n\n    def perform(self, node, inputs, outputs):\n        *flat_params, images = inputs\n        result = jitted_cnn_op_jax(flat_params, images)\n        outputs[0][0] = np.asarray(result, dtype=\"float64\")\n\n    def grad(self, inputs, output_gradients):\n        raise NotImplementedError(\"PyTensor gradient of CNNOp not implemented\")\n\n\n@jax_funcify.register(CNNOp)\ndef cnn_op_jax_funcify(op, **kwargs):\n    def perform(*inputs):\n        *flat_params, images = inputs\n        return cnn_op_jax(flat_params, images)\n    return perform\n\ncnn_op = CNNOp()\n\n\nWe can now create a Bayesian Neural Network model, giving a Normal prior for all the parameters in the CNN.\n\nwith pm.Model() as model:\n    weights_prior = []\n    for layer_name, layer in params[\"params\"].items():\n        for layer_weights_name, layer_weights in sorted(layer.items()):\n            prior_name = f\"{layer_name}_{layer_weights_name}\"\n            layer_weights_prior = pm.Normal(prior_name, 0, 1, shape=layer_weights.shape)\n            weights_prior.append(layer_weights_prior)\n\n    logitp_classes = cnn_op(*weights_prior, train[\"image\"])\n    logitp_classes = pt.specify_shape(logitp_classes, (len(train[\"label\"]), 10))\n    label = pm.Categorical(\"label\", logit_p=logitp_classes, observed=train[\"label\"])\n\npm.model_to_graphviz(model)\n\n\nAs before, we can compute the logp at the models' initial point, which lets us figure out whether there are any issues with the model or not.\n\n# Compute the logp at the initial point\nip = model.initial_point()\nlogp_fn = model.compile_fn(model.logp(sum=True))\nlogp_fn(ip)\n\narray(-759928.81029946)\n\n\nWe can do the same with the JAX backend.\n\n# Same in JAX backend\nlogp_fn = model.compile_fn(model.logp(sum=True), mode=\"JAX\")\nlogp_fn(ip)\n\nDeviceArray(-759928.81030185, dtype=float64)\n\n\nAs we mentioned, we don't always need to define the gradient method. For instance, when using JAX samplers such as sample_numpyro_nuts, the gradients will be directly obtained from the jax compiled function.\n\nLet's confirm this is the case, by using the PyMC helper get_jaxified_logp that returns the JAX function that computes the model joint logp, and then taking the gradient with respect to the first set of parameters. Firstly, we use the get_jaxified_logp helper to get the JAX function (and we evaluate it below):\n\nfrom pymc.sampling.jax import get_jaxified_logp\n\nlogp_fn = get_jaxified_logp(model)\nlogp_fn(list(ip.values()))\n\nDeviceArray(-759928.81030185, dtype=float64)\n\n\nAnd then, we take the gradient with respect to the first set of parameters and evaluate it below as well:\n\ndlogp_fn = jax.grad(logp_fn)\ndlogp_fn(list(ip.values()))[0]\n\nDeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n             0., 0.], dtype=float64)\n\nSummary#\n\nWe hope you found this introduction to using PyMC with JAX helpful. JAX is a powerful automatic differentiation library, and a growing ecosystem is forming around it. PyTensor is a flexible library for the compilation and manipulation of symbolic expressions, for which JAX is one supported backend. We hope that this introduction will help you to use JAX with PyMC, and that you will find it helpful in your work!\n\nThe Probabilistic AI\nConsultancy\n\nHome\nAbout\nBlog\n\ninfo@pymc-labs.com\n\nSubscribe to our newsletter\n\nStay connected with the latest developments in Probabilistic AI Statistics and AI.\n\nSubscribe\n\nYou can unsubscribe at any time. For more details, review ourPrivacy Policypage.\n\nContact\n\n© 2025 PyMC Labs. All Rights Reserved."
}